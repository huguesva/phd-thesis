\chapter{Introduction}
\label{c-intro} % a label for the chapter, to refer to it later

\minitoc

\section{Building dense representations of the data}

Exploring and analyzing high-dimensional data is a core problem of data science. As presented in \Cref{c-intro}, it often requires constructing dense low-dimensional and interpretable representations of the data. 


Typically, this data is redundant and noisy, making it difficult to directly extract useful information from the raw data. Therefore, the challenge of finding a compact and meaningful representation of high-dimensional data becomes paramount.


One major objective of unsupervised learning~\citep{Hastie2009} is to provide interpretable and meaningful approximate representations of the data that best preserve its structure \ie the underlying geometric relationships between the data samples.
Similar in essence to Occam's principle frequently employed in supervised learning, the preference for unsupervised data representation often aligns with the pursuit of simplicity, interpretability or visualizability in the associated model.
These aspects are determinant in many real-world applications, such as cell biology
\citep{cantini2021benchmarking, ventre2023one}, where the interaction with domain experts is paramount for interpreting the results and extracting meaningful insights from the model. 

Ideally, the reduced representation should align with the intrinsic dimensionality of the data, which broadly refers to the minimum number of parameters needed to capture its essential characteristics.


\paragraph{Dimensionality reduction and clustering.}
When faced with the question of extracting
interpretable representations, from a dataset $\mX = (\vx_1, ..., \vx_N) ^\top
\in \R^{N \times p}$ of $N$ samples in $\R^p$, the machine learning community
has proposed a variety of methods. Among them, dimensionality reduction (DR) algorithms have been widely used to summarize data in a low-dimensional space
$\mZ = (\vz_1, ..., \vz_N) ^\top \in \R^{N \times d}$ with $d \ll p$, allowing
for visualization of every individual points for small enough $d$ \citep{agrawal2021minimum,van2009dimensionality}.
Another major approach is to cluster the data into $n$ groups, with $n$
typically much smaller than $N$, and to summarize these groups through their centroids \citep{saxena2017review,ezugwu2022comprehensive}.
%by clusteres defined as  centroids or groups of samples. 
Clustering is particularly interpretable since it provides a
smaller number of points that can be easily inspected. %visualized or interpreted. 
The cluster assignments can also be analyzed. %but also assignment of each original point to a cluster. 
Both DR and clustering follow a
similar philosophy of summarization and reduction of the dataset using a smaller size representation.
% matrix. 



\paragraph{The AI revolution is happening in latent space.}


\section{What is referred to as dimensionality reduction?}


When a ground distance (or similarity) metric is available on $\mathcal{X}$, learning data representations translates into solving a dimensionality reduction (DR) problem.


Dimensionality reduction (DR) is of central importance when dealing with high-dimensional data \citep{donoho2000high}. It mitigates the curse of dimensionality, allowing for greater statistical flexibility and less computational complexity. DR also enables visualization that can be of great practical interest for understanding and interpreting the structure of large datasets.
Most seminal approaches include Principal Component Analysis (PCA) \citep{pearson1901liii},  multidimensional scaling \citep{kruskal1978multidimensional} and more broadly kernel eigenmaps methods such as Isomap \citep{balasubramanian2002isomap}, Laplacian eigenmaps \citep{belkin2003laplacian} and diffusion maps \citep{coifman2006diffusion}. These methods share the definition of a pairwise similarity kernel that assigns a high value to close neighbors and the resolution of a spectral problem. They are well understood and unified in the kernel PCA framework \citep{ham2004kernel}.

In the past decade, the field has witnessed a major shift with the emergence of a new class of methods. They are also based on pairwise similarities but these are not converted into inner products. Instead, they define pairwise similarity functions in both input and latent spaces and optimize a cost between the two. Among such methods, the Stochastic Neighbor Embedding (SNE) algorithm \citep{NIPS2002SNE}, its heavy-tailed symmetrized version t-SNE \citep{maaten2008tSNE} or more recent approaches like LargeVis \citep{tang2016visualizing} and UMAP \citep{mcinnes2018umap} are arguably the most used in practice. These will be referred to as \textit{SNE-like} or \textit{neighbor embedding} methods in what follows. They are increasingly popular and now considered state-of-art techniques in many fields \citep{li2017application,kobak2019art,anders2018dissecting}. Their popularity is mainly due to their exceptional ability to preserve the local structure, \textit{i.e.}\ close points in the input space have close embeddings, as shown empirically \citep{wang2021understanding}. They also demonstrate impressive performances in identifying clusters \citep{arora2018analysis, linderman2019clustering}. However this is done at the expense of global structure, that these methods struggle in preserving \citep{wattenberg2016use, coenen2019understanding} \textit{i.e.}\ the relative large-scale distances between embedded points do not necessarily correspond to the original ones.

Given a dataset $\mathbf{X} \in \mathbb{R}^{n \times p}$ of $n$ samples in dimension $p$, most DR algorithms compute a representation of $\mathbf{X}$ in a lower-dimensional latent space $\mathbf{Z} \in \mathbb{R}^{n \times q}$ with $q \ll p$ that faithfully captures and represents pairwise dependencies between the samples (or rows) in $\X$. This is generally achieved by optimizing $\mathbf{Z}$ such that the corresponding affinity matrix matches another affinity matrix defined from $\X$. These affinities are constructed from a matrix $\mathbf{C} \in \R^{n \times n}$ that encodes a notion of ‘‘distance'' between the samples, \eg the squared Euclidean distance $C_{ij} = \|\mathbf{X}_{i:}-\mathbf{X}_{j:}\|_2^2$ or more generally any \emph{cost matrix} $\mathbf{C} \in \mathcal{D} := \{\Cb \in \mathbb{R}_+^{n \times n} : \Cb = \Cb^\top \text{and } C_{ij}=0 \iff i=j\}$. A commonly used option is the Gaussian affinity that is obtained by performing row-wise normalization of the kernel $\exp(-\Cb / \varepsilon)$, where $\varepsilon >0$ is the bandwidth parameter.




\paragraph{Two sides of the same coin.} As a matter of fact, methods from both families share many similitudes, %among which is
including the construction of a similarity graph between input samples. In clustering, many popular approaches design a reduced or coarsened version of the initial similarity graph while preserving some of its spectral properties~\citep{von2007tutorial, schaeffer2007graph}. 
%attributes related to the graph spectrum 
In DR, the goal is to solve the inverse problem of finding low-dimensional embeddings that generate a similarity graph close to the %initial
one computed from input data points \citep{ham2004kernel,hinton2002stochastic}.
%With so many similarities
Our work builds on these converging viewpoints and addresses the following question: \emph{can DR and clustering  be expressed in a common and unified framework ?}


\section*{Outline of the thesis}

\paragraph{Chapter 2.}

\paragraph{Chatper 3.} We introduce a new probabilistic model for dimensionality reduction.

\paragraph{Chapter 4.} We propose a new algorithm for clustering.


\paragraph{Chapter 5.} 

\text


\begin{rem2}
	This is a test aa.
\end{rem2}


% The shortest path principle guides most decisions in life and sciences: When a commodity, a person or a single bit of information is available at a given point and needs to be sent at a target point, one should favor using the least possible effort. This is typically reached by moving an item along a straight line when in the plane or along geodesic curves in more involved metric spaces. The theory of optimal transport generalizes that intuition in the case where, instead of moving only one item at a time, one is concerned with the problem of moving simultaneously several items (or a continuous distribution thereof) from one configuration onto another. As schoolteachers might attest, planning the transportation of a group of individuals, with the constraint that they reach a given target configuration upon arrival, is substantially more involved than carrying it out for a single individual. Indeed, thinking in terms of groups or distributions requires a more advanced mathematical formalism which was first hinted at in the seminal work of~\citet{Monge1781}. Yet, no matter how complicated that formalism might look at first sight, that problem has deep and concrete connections with our daily life. Transportation, be it of people, commodities or information, very rarely involves moving only one item. All major economic problems, in logistics, production planning or network routing, involve moving distributions, and that thread appears in all of the seminal references on optimal transport. Indeed~\citet{tolstoi1930methods},~\citet{Hitchcock41} and~\citet{Kantorovich42} were all guided by practical concerns. It was only a few years later, mostly after the 1980s, that mathematicians discovered, thanks to the works of~\citet{Brenier91} and others, that this theory provided a fertile ground for research, with deep connections to convexity, partial differential equations and  statistics. At the turn of the millenium, researchers in computer, imaging and more generally data sciences understood that optimal transport theory provided very powerful tools to study distributions in a different and more abstract context, that of comparing distributions readily available to them under the form of bags-of-features or descriptors.
% %

% Several reference books have been written on optimal transport, including the two recent monographs by~\citeauthor{Villani03} (\citeyear{Villani03,Villani09}), those by~\citeauthor{rachev1998mass} (\citeyear{rachev1998mass,rachev1998mass2}) and more recently that by~\citet{SantambrogioBook}. As exemplified by these books, the more formal and abstract concepts in that theory deserve in and by themselves several hundred pages. Now that optimal transport has gradually established itself as an applied tool (for instance, in economics, as put forward recently by~\citet{galichon2016optimal}), we have tried to balance that rich literature with a computational viewpoint, centered on applications to data science, notably imaging sciences and machine learning. We follow in that sense the motivation of the recent review by~\citet{kolouri2017optimal} but try to cover more ground.
% %
% Ultimately, our goal is to present an overview of the main theoretical insights that support the practical effectiveness of OT and spend more time explaining how to turn these insights into fast computational schemes. 
% %
% The main body of Chapters \ref{c-continuous}, \ref{c-algo-basics}, \ref{c-entropic}, \ref{c-variational}, and \ref{c-extensions} is devoted solely to the study of the geometry induced by optimal transport in the space of probability vectors or discrete histograms. 
% %
% Targeting more advanced readers, we also give in the same chapters,  in light gray boxes, a more general mathematical exposition of optimal transport tailored for discrete measures. Discrete measures are defined by their probability weights, but also by the location at which these weights are defined. These locations are usually taken in a continuous metric space, giving a second important degree of freedom to model random phenomena.
% %This corresponds to the case where the support of the considered measures can take arbitrary values in a continuous space, giving a second important degree of freedom, beyond just weights, when comparing probability measures.
% %
% Lastly, the third and most technical layer of exposition is indicated in dark gray boxes and deals with arbitrary measures that need not be discrete, and which can have in particular a density w.r.t. a base measure. This is traditionally the default setting for most classic textbooks on OT theory, but one that plays a less important role in general for practical applications.
% %
% Chapters~\ref{c-algo-semidiscr} to~\ref{c-statistical} deal with the interplay between continuous and discrete measures and are thus targeting a more mathematically inclined audience. 

% The field of computational optimal transport is at the time of this writing still an extremely active one. There are therefore a wide variety of topics that we have not touched upon in this survey. Let us cite in no particular order the subjects of distributionally robust optimization \citep{NIPS2015_5745,esfahani2018data,NIPS2018_7534,NIPS2018_8015}, in which parameter estimation is carried out by minimizing the worst posssible empirical risk of any data measure taken within a certain Wasserstein distance of the input data; convergence of the Langevin Monte Carlo sampling algorithm in the Wasserstein geometry \citep{dalalyan2017user,pmlr-v65-dalalyan17a,pmlr-v75-bernton18a}; other numerical methods to solve OT with a squared Euclidian cost in low-dimensional settings using the Monge-Amp\`ere equation~\citep{froese2011convergent,benamou2014numerical,sulman2011efficient} which are only briefly mentioned in Remark~\ref{rem:MA}.


%\begin{figure}[h!]
%\centering
%\begin{tabular}{@{}c@{\hspace{5mm}}c@{}}
%\includegraphics[width=.45\linewidth]{barycenters-1d/bary-eucl}  &
%\includegraphics[width=.45\linewidth]{barycenters-1d/bary-ot} 
%\end{tabular}
%\caption{\label{fig-w-vs-eucl}
%Comparison of Wasserstein vs. Euclidean average. \todo{Blabla}
%}
%\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Notation}

\begin{itemize}
	\item $\integ{n}$: set of integers $\{1,\dots,n\}$.
	\item $\one_{n}$: vector of $\R^{n}$ with all entries identically set to $1$.
	\item $\mI_n$: identity matrix of size $n \times n$.
    \item $\exp$ and $\log$ applied to vectors/matrices are taken element-wise.
    \item $\mathcal{S}^n$: space of $n \times n$ symmetric matrices.
    \item The $i^{th}$ entry of a vector $\vv$ is denoted as either $v_i$ or $[\vv]_i$.
	Similarly, for a matrix $\mM$, $M_{ij}$ and $[\mM]_{ij}$ both denote its entry $(i,j)$.
    \item $\mathbf{P}_{i:}$ or $[\mathbf{P}]_{i:}$: $i$-th row of a matrix $\Pb$.
    \item $\odot$ (\textit{resp.} $\oslash$): element-wise multiplication (\textit{resp.} division) between vectors/matrices.
    \item $\langle \cdot, \cdot \rangle$ is the standard Euclidean inner product for matrices/vectors.
    \item For $\bm{\alpha}, \bm{\beta} \in \R^n, \bm{\alpha} \oplus \bm{\beta} \in \R^{n \times n}$ is $(\alpha_i + \beta_j)_{ij}$.
    \item The entropy of $\p \in \mathbb{R}^{n}_+$ is\footnote{With the convention $0 \log 0 = 0$.} $\operatorname{H}(\p) = -\sum_{i} p_i(\log(p_i)-1) = -\langle \pb, \log \pb - \bm{1} \rangle$.
    \item The Kullback-Leibler divergence between two matrices $\Pb, \Qb$ with nonnegative entries such that $Q_{ij} = 0 \implies P_{ij}=0$ is $\KL(\Pb | \Qb) = \sum_{ij} P_{ij}\left(\log(\frac{P_{ij}}{Q_{ij}})-1\right) = \langle \Pb, \log \left(\Pb \oslash \Qb \right) - \bm{1}\bm{1}^\top \rangle$.
    \item $S_N$ is the set of permutations of $\integ{N}$.
    \item $P_N(\R^d)$ refers to the set of discrete probability measures composed of N points of $\R^d$.
    \item $\Sigma_N$ stands for the probability simplex of size $N$ that is $\Sigma_N := \{\vh \in \R^N_+ \: \text{s.t.} \: \sum_i h_i = 1 \}$.
    \item For $\vx \in \R^{N}, \diag(\vx)$ denotes the diagonal matrix whose elements are the $x_i$.
    \item We denote by $\gU(\vh) = \left\{ \mT \in \R_+^{N \times n} | \mT \bm{1}_n = \vh \right\}$ and $\gU(\vh, \overline{\vh}) = \left\{ \mT \in \R_+^{N \times n} | \mT \bm{1}_n = \vh, \mT^\top \bm{1}_N = \overline{\vh} \right\}$ the set of discrete couplings with respectively one and two marginals.
    \item $L_2(x,y) \coloneqq \frac{1}{2} |x - y|^2$ is the quadratic loss,  $L_{\mathrm{KL}}(x,y) \coloneqq L_{\mathrm{KL}}(x,y) - x + y $ the Kullback-Leibler divergence loss.
    \item For a matrix $\mZ \in \R^{N \times d}$, we denote by $\mC_{\mZ} =  \simi(\mZ)$ its output through a similarity function $\simi: \R^{N \times d} \to \R^{N \times N}$.
    % \item $\operatorname{DS} = \mathcal{U}(\bm{1}, \bm{1})$ is the space of $N \times N$ doubly stochastic matrices.
\end{itemize}

\subsection*{Acronyms.}
\begin{itemize}
	\item DR: Dimensionality Reduction.
	\item OT: Optimal Transport.
	\item GW: Gromov-Wasserstein.
	\item DS: Doubly Stochastic.
\end{itemize}