\chapter{Introduction}\label{chap:intro}

\minitoc

\section{Representation Learning at the Core of Data Science}

One major objective of unsupervised learning \citep{Hastie2009} is to provide interpretable and meaningful approximate representations of the data that best reveal its structure \ie the underlying geometric relationships between the data samples.

Similar in essence to Occam's principle frequently employed in supervised learning, the preference for unsupervised data representation often aligns with the pursuit of simplicity, interpretability or visualizability in the associated model.

These aspects are determinant in many real-world applications, such as cell biology
\citep{regev2017human,kobak2019art,becht2019dimensionality} where the interaction with domain experts is paramount for interpreting the results and extracting meaningful insights from the model. 



Exploring and analyzing high-dimensional data is a core problem of data science. As presented in, it often requires constructing dense low-dimensional and interpretable representations of the data. 


Typically, this data is redundant and noisy, making it difficult to directly extract useful information from the raw data. Therefore, the challenge of finding a compact and meaningful representation of high-dimensional data becomes paramount.




Ideally, the reduced representation should align with the intrinsic dimensionality of the data, which broadly refers to the minimum number of parameters needed to capture its essential characteristics.



\paragraph{The AI revolution is happening in latent space.} aaa


\section{What is Dimensionality reduction in this Landscape}

When a ground distance (or similarity) metric is available on $\mathcal{X}$, learning data representations translates into solving a dimensionality reduction (DR) problem.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/single_cell_readme.png}
    \caption{TSNE and LargeVis embeddings of single cell RNA-seq data from \citep{macosko2015highly} and \citep{zheng2017massively}.
    }
    \label{fig:intro_fig}
\end{figure}

\paragraph{Classical approaches to dimensionality reduction.}
Dimensionality reduction (DR) is of central importance when dealing with high-dimensional data \citep{donoho2000high}. It mitigates the curse of dimensionality, allowing for greater statistical flexibility and less computational complexity. DR also enables visualization that can be of great practical interest for understanding and interpreting the structure of large datasets.
Most seminal approaches include Principal Component Analysis (PCA) \citep{pearson1901liii},  multidimensional scaling \citep{kruskal1978multidimensional} and more broadly kernel eigenmaps methods such as Isomap \citep{balasubramanian2002isomap}, Laplacian eigenmaps \citep{belkin2003laplacian} and diffusion maps \citep{coifman2006diffusion}. These methods share the definition of a pairwise similarity kernel that assigns a high value to close neighbors and the resolution of a spectral problem. They are well understood and unified in the kernel PCA framework \citep{ham2004kernel}.

\paragraph{The revolution of neighbor embedding methods.}
In the past decade, the field has witnessed a major shift with the emergence of a new class of methods. They are also based on pairwise similarities but these are not converted into inner products. Instead, they define pairwise similarity functions in both input and latent spaces and optimize a cost between the two. Among such methods, the Stochastic Neighbor Embedding (SNE) algorithm \citep{NIPS2002SNE}, its heavy-tailed symmetrized version t-SNE \citep{maaten2008tSNE} or more recent approaches like LargeVis \citep{tang2016visualizing} and UMAP \citep{mcinnes2018umap} are arguably the most used in practice. These will be referred to as \textit{SNE-like} or \textit{neighbor embedding} methods in what follows. They are increasingly popular and now considered state-of-art techniques in many fields \citep{li2017application,kobak2019art,anders2018dissecting}. Their popularity is mainly due to their exceptional ability to preserve the local structure, \textit{i.e.}\ close points in the input space have close embeddings, as shown empirically \citep{wang2021understanding}. They also demonstrate impressive performances in identifying clusters \citep{arora2018analysis, linderman2019clustering}. However this is done at the expense of global structure, that these methods struggle in preserving \citep{wattenberg2016use, coenen2019understanding} \textit{i.e.}\ the relative large-scale distances between embedded points do not necessarily correspond to the original ones.


\section{Outline of the thesis}

This thesis is organized as follows.

\paragraph{Chapter 2.}
In this chapter, we review all the necessary background material for the subsequent chapters. We begin with an overview of various dimensionality reduction (DR) methods through the lens of the affinity matching problem, where affinities are defined in both input and latent spaces, and the goal is to match them. We then provide two perspectives on DR methods: one based on latent variable models and the other from a distributional viewpoint using the framework of optimal transport. Extending these perspectives to neighbor embedding methods will be the primary focus of \Cref{chapter:GraphCoupling} and \Cref{chapter:DistR}. Finally, we raise the question of designing new affinity matrices that are more robust to noise heterogeneities in order to address real-world scenarios. This motivates the work presented in \Cref{chapter:SNEkhorn}.

\paragraph{Chatper 3.} 
In this chapter, we introduce a new latent variable model that broadly encompasses both linear and non-linear dimensionality reduction (DR) methods. This framework allows us to conceptualize these methods in a probabilistic manner, thus facilitating the incorporation of priors and providing a clearer understanding of the underlying modeling assumptions when applying these methods in practice.

\paragraph{Chapter 4.} 
In this chapter, we adopt a distributional perspective and frame affinity-based dimensionality reduction (DR) approaches as optimal transport problems. Thinking in terms of distributions provides the flexibility to control the granularity of the resulting low-dimensional representation, allowing us to retrieve points that serve as prototypes for multiple input data points. This perspective unifies DR and clustering within a common framework, and we introduce a novel method that jointly learns both a reduced representation and a clustering of the data. We demonstrate that, in practice, this approach delivers strong performance in both clustering and dimensionality reduction, allowing for a more effective exploration of the trade-offs between these two aspects.

\paragraph{Chapter 5.} 
In this final chapter, we introduce a new family of affinity matrices that effectively account for varying noise levels in the data. This is achieved by adapting the geometry under which the symmetric version is constructed, leading to a more coherent framework where normalization and symmetrization are performed simultaneously.
