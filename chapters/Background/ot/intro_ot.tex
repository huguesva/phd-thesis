\section{The Distributional Perspective}\label{sec:dist_perspective_dr}

In this section, we propose to treat both data samples and their embeddings as probability distributions. We introduce the theory of Optimal Transport (OT) \citep{villani2009optimal,peyre2019computational}, which plays a central role in this thesis. OT offers an elegant approach for quantifying the distance between two distributions, specifically a source and a target, while leveraging the geometry of the underlying spaces. It can be seen as a canonical way to lift a ground distance between points to a distance between measures supported on these points. We begin by outlining the classical formulation of OT, where a well-defined cost is assigned between individual source and target samples, and then expand the presentation to cases where the source and target distributions lie in incomparable spaces. While the classical setting is useful for defining meaningful affinities within a domain, the latter approach becomes particularly relevant when modeling the embedding process from high-dimensional to low-dimensional spaces as an OT problem.


\subsection{Comparing Distributions with Optimal Transport}\label{sec:background_ot}

The first mathematical formulation of the optimal transport (OT) problem was introduced by Gaspard Monge in his 1781 \textit{Mémoire} \citep{monge1781memoire}. In this work, Monge sought to determine the most efficient allocation strategy for transporting a pile of sand, treated as a collection of granular particles, from one location (\emph{déblais}) to another (\emph{remblais}). This problem naturally provides a framework for comparing distributions, which can be viewed as collections of potentially infinitely many particles.

\paragraph{Lagrangian viewpoint.} We start by considering discrete probability measures which can be expressed as finite sums of Dirac masses. Specifically, a discrete measure with weights $\vh \in \Sigma_N$ where $\Sigma_N := \{\vh \in \R_+^N \mid \sum_i h_i = 1\}$, along with the corresponding support $\{\vx_1, \ldots, \vx_N\}$, can be written as
\[
\nu = \sum_i h_i \delta_{\vx_i}
\]
where $\delta_{\vx_i}$ denotes the Dirac measure centered at $\vx_i$ for each $i \in \integ{N}$. Here, \(h_i\) represents the weight associated with the support point \(\vx_i\) in the distribution.

\paragraph{Comparing histograms with OT.}
Let $X_S = \{ \bm{x}_i^S \in \R^d \}_{i=1}^{N_S}$ and $X_T = \{ \bm{x}_i^T \in \R^d \}_{i=1}^{N_T}$ denote respectively the source (\emph{déblais}) and target (\emph{remblais}) point locations. Let  \(\mu_S = \sum_{i \in \integ{N_S}} a_i \delta_{\vx^S_i}\) and \(\mu_T = \sum_{j \in \integ{N_T}} b_j \delta_{\vx^T_j}\), with $\va \in \Sigma_{N_S}$ and $\vb \in \Sigma_{N_T}$, denote respectively the source and target discrete probability measures or \emph{histograms} that we wish to compare. The original optimal transport problem can be formulated as follows: \emph{how can we transfer the total mass of \(\mu_S\) onto \(\mu_T\) in such a way that the overall transfer cost is minimized?}

In what follows, we first introduce the discrete OT problem before presenting regularized formulations and associated algorithms.

\subsubsection{Unregularized Optimal Transport}

\paragraph{Monge formulation.}
In the original Monge formulation, the transfer of mass is modeled by a mapping, known as the Monge map, which assigns each point \(\vx^S_i\) in the support of \(\mu_S\) to a point \(\vx^T_j\) in the support of \(\mu_T\), thereby transporting the mass from \(\mu_S\) to \(\mu_T\). To define the overall transport cost, we introduce a ground cost function \(c: (\vx, \vy) \in \text{supp}(\mu_S) \times \text{supp}(\mu_T) \to \mathbb{R}_+\). One can typically consider the squared Euclidean distance $(\vx, \vy) \mapsto \| \vx - \vy \|_2^2$ or any Riemannian distance over a manifold \citep{villani2009optimal} depending on the underlying spaces of $\text{supp}(\mu_S) \times \text{supp}(\mu_T)$. The total transport cost is then the sum of the ground costs for each unit of mass moved according to the map \(T\). Thus, the Monge problem is formulated as
\begin{align}
    \label{eq:monge_pb}
    \tag{Monge}
    M(\mu_S, \mu_T) \coloneqq \inf_{T_{\#}\mu_S = \mu_T} \sum_{\bm{x} \in \text{supp}(\mu_S)} c(\bm{x}, T(\bm{x}))
\end{align}
where \(T_{\#}\mu_S = \sum_{i=1}^{N_S} a_i \delta_{T(\vx_i^S)}\) represents the push-forward of \(\mu_S\) under the map \(T\).

First, note that there may not always exist a map \(T\) that pushes \(\mu_S\) forward to \(\mu_T\), i.e., satisfying \(T_{\#}\mu_S = \mu_T\). Moreover, the Monge problem \eqref{eq:monge_pb} is highly non-linear in \(T\), and the set of admissible maps is not convex, which makes it challenging to analyze. To gain intuition about the complexity of the problem, consider the case where \(N_S = N_T = N\) for some \(N \in \mathbb{N}\), and \(\va = \vb = \frac{1}{N} \mathbf{1}_N\). Here, mass conservation implies that the map \(T\) defines a bijection on \(\{1, \ldots, N\}\). Solving the OT problem in this case involves finding the optimal permutation in the enormous space of \(N!\) possible outcomes. This combinatorial complexity renders the Monge problem intractable in practice. These difficulties have contributed to the transportation problem remaining an open mathematical challenge for nearly two centuries.


\paragraph{Kantorovich relaxation.}
We have just seen that defining transport through a map considerably limits the scope of problems that can be efficiently addressed. To overcome this, a well-known relaxation proposed by \citep{kantorovich1942translocation} involves optimizing over the space of probabilistic couplings with marginals \(\mu_S\) and \(\mu_T\). This approach seeks to compute a \emph{coupling} \(\mT \in \mathbb{R}_{+}^{N_S \times N_T}\), which represents a joint probability measure over \(\text{supp}(\mu_S) \times \text{supp}(\mu_T)\). Intuitively, this relaxation allows for mass splitting, meaning that a unit of mass from \(\mu_S\) can be distributed across multiple points in \(\mu_T\), and vice versa.
The Kantorovich formulation of OT is then given by the following linear program
\begin{equation}
    \tag{OT}
    \label{eq:OT}
    W_c(\mu_S, \mu_T) \coloneqq \min_{\mT \in \gU(\va, \vb)} \: \langle \mT, \mC \rangle
\end{equation}
where $\gU(\va, \vb)$ is the transport polytope with marginals $(\bm{a}, \bm{b})$ that is $\gU(\va, \vb) \coloneqq \left\{ \mT \in \R_+^{N_S \times N_T} \mid \mT \mathbf{1}_n = \va, \mT^\top \mathbf{1}_N = \vb \right\}$ and the \emph{cost matrix} $\mathbf{C} \in \R_+^{N_S \times N_T}$ such that $C_{ij} = c(\vx_i^S, \vx_j^T)$ encodes the pairwise distances between the source and target samples. 

Note that for discrete measures, the Monge and Kantorovich formulations are equivalent when $\mu_S = \frac{1}{N} \sum_{i \in \integ{N}} \delta_{\vx^S_i}$ and $\nu = \frac{1}{N} \sum_{i \in \integ{N}} \delta_{\vx^T_i}$. In this case, the solution of \eqref{eq:Wasserstein} corresponds to an extremal point of the polytope of doubly stochastic matrices \citep{bertsimas1997introduction}. This implies that there is no splitting of mass, meaning that each row of the optimal solution $\mT^\star$ contains exactly one non-zero coefficient. These solutions are also referred to as \emph{assignation matrices} in \Cref{chap:DistR}.

In the Kantorovich formulation, the set $\gU(\va, \vb)$ forms a convex polytope, making the optimal transport (OT) problem in \eqref{eq:OT} a linear program. As a result, it can be efficiently solved using linear programming solvers \citep{dantzig2016linear}. For a comprehensive review of network flow algorithms employed to solve \Cref{eq:OT}, we refer the reader to \citep{peyre2019computational}.

\begin{mem1}{Primer on OT for continuous distributions}
    We consider a Polish space $(\mathcal{X}, d_{\gX})$ and a cost function $c : \mathcal{X} \times \mathcal{X} \to \R_+$. Let $\mu_S \in \mathcal{P}(\mathcal{X})$ and $\mu_T \in \mathcal{P}(\mathcal{X})$ be two probability measures that we aim to compare.

    The original Monge formulation \citep{monge1781memoire} of OT seeks the map $T$ satisfying $T_{\#}\mu_S = \mu_T$ that minimizes the transportation cost given by
    \begin{align}\label{eq:monge_pb_continuous}
        M(\mu_S, \mu_T) \coloneqq \inf_{T_{\#}\mu_S = \mu_T} \int_{\mathcal{X}} c(\bm{x}, T(\bm{x})) \mathrm{d}\mu_S(\bm{x}) \:.
    \end{align}
    Similarly to the case of discrete measures, the set of admissible maps is challenging to work with due to its non-convexity. Moreover, the existence of an optimal map \(T\) is not guaranteed in general \citep{santambrogio2015optimal}.

    The relaxation by \cite{kantorovich1942translocation} consists of optimizing instead over the space of probabilistic couplings with marginals $\mu_S$ and $\mu_T$ denoted by $\gU(\mu_S, \mu_T)$. The latter is defined as the probability distributions on $(\mathcal{X} \times \mathcal{X})$ whose marginals are $\mu$ and $\nu$ that is $\gU(\mu_S, \mu_T) = \left\{ \gamma \in \mathcal{P}(\mathcal{X} \times \mathcal{X}) : (P_{1})_\# \gamma = \mu_S, (P_{2})_\# \gamma = \mu_T \right\}$ where $P_{1}$ and $P_{2}$ are the projections such that $P_{1}(\bm{x}, \bm{y}) = \bm{x}$ and $P_{2}(\bm{x}, \bm{y}) = \bm{y}$.
    The Kantorovich formulation of OT is then given by
    \begin{align}\label{eq:Wasserstein}
        W(\mu_S, \mu_T) \coloneqq \inf_{\gU \in \gU(\mu_S, \mu_T)} \int_{\mathcal{X} \times \mathcal{X}} c(\bm{x}, \bm{y}) \mathrm{d}\pi(\bm{x}, \bm{y}) \:.
    \end{align}
    This formulation is a convex optimization problem and the infimum is well
    defined under mild assumptions \citep{santambrogio2015optimal}. If the optimal
    coupling $\gU^\star$ is supported on a \emph{deterministic} function, \ie
    $\gU^\star$ is of the form $(\mathrm{id} \times T^\star)\# \mu_S$, then
    $T^\star$ solves \eqref{eq:monge_pb}. This holds under the assumption that one
    of the inputs is absolutely continuous with respect to the Lebesgue measure
    for $c(\bm{x}, \bm{y}) = h(\bm{x} - \bm{y})$ with
    $h$ strictly convex \citep{gangbo1996geometry}.
    
    \end{mem1}

\begin{mem1}{Metric properties of the Wasserstein distance}
    When \( c = d_{\gX}^p \), the Wasserstein distance \( W \) defined in \Cref{eq:Wasserstein} forms a proper metric between probability measures on \( \mathcal{X} \), known as the \( p \)-Wasserstein distance \citep{villani2009optimal}. A key advantage of Wasserstein distances is their capacity to compare probability measures while taking into account the geometry of the underlying space, as dictated by the ground cost \( c \). This contrasts with commonly used f-divergences \citep{csiszar1967information}, such as the Total Variation (TV) norm or Kullback-Leibler (KL) divergence, which only perform pointwise comparisons of the two density functions. Another crucial aspect of the Wasserstein distance is its ability to compare distributions with non-overlapping supports and quantify the dissimilarity between these supports. A consequence is that the Wasserstein distance is closely related to weak convergence. Indeed, convergence in Wasserstein distance \( W(\mu_n,\mu) \xrightarrow{n \to +\infty} 0 \) is equivalent to the weak convergence of measures, meaning that for any continuous and bounded function $f$: $\int_{\gX} f \mathrm{d}\mu_n \xrightarrow{n \to +\infty} \int_{\gX} f \mathrm{d}\mu$ \citep{villani2009optimal}.
\end{mem1}


\subsubsection{Regularized Optimal Transport}

% \subsection{Strictly Convex Optimal Transport}
To enable faster algorithmic resolution as well as smoother solutions, one can rely on a strictly convex regularizer $\psi : \R^{N_s} \to \R$. It amounts to solving $\min_{\mT \in \gU(\bm{a}, \bm{b})} \: \langle \mT, \C \rangle + \varepsilon^\star \sum_i \psi(\mT_{i:})$ where $\varepsilon^\star > 0$. The regularizer $\Psi$ is typically related to the notion of \emph{generalized entropy} \citep{blondel2020learning} and encourages the mass transported from each source sample to be more dispersed compared to the non regularized OT formulation of \Cref{eq:OT}.

We now introduce a slightly unconventional perspective on regularized optimal transport (OT), which is equivalent to the penalized objective and will serve as the foundation for \Cref{chap:snekhorn}.
Interestingly, regularized OT can also be framed using a convex constraint as follows
\begin{align}\label{eq:cot}
    \min_{\mT \in \gU(\bm{a}, \bm{b})} \: \langle \mT, \mC \rangle \quad \text{s.t.} \quad  \mT \in \overline{\mathcal{B}}(\eta)
    \tag{ROT}
\end{align}
where $\overline{\mathcal{B}}(\eta) \coloneqq \{ \mT \: \text{s.t.} \: \sum_i \psi(\mT_{i:}) \leq \eta \}$. Note that the previously introduced $\varepsilon^\star$ is the optimal dual variable associated with the constraint $\overline{\mathcal{B}}(\eta)$ in the above equivalent formulation. Formulation \Cref{eq:cot} allows for direct control over the value of the regularizer \( \psi \) in the solution, whereas in the more commonly used penalized formulation, this control is typically indirect and governed by the regularization parameter \( \varepsilon^\star \).

Throughout, we make the following assumption on $\psi$.
\begin{assumption}\label{assumption_psi}
    Let $\psi: \mathrm{dom}(\psi) \to \R \cup \{\infty\}$ be strictly convex and differentiable on the interior of its domain $\mathrm{dom}(\psi) \subset \R^{N_S}_+$.
\end{assumption}

In what follows, we denote by $\psi(\mT) = (\psi(\mT_{1:}), ..., \psi(\mT_{N_S:}))^\top$.
We introduce $\psi^*:= \mathbf{p} \to \sup_{\mathbf{q} \in \mathrm{dom}(\psi)} \langle \mathbf{p}, \mathbf{q} \rangle - \psi(\mathbf{q})$ the convex conjugate of $\psi$ \citep{rockafellar1997convex}.
Note that when $\psi$ is strictly convex, this supremum is uniquely achieved and from Danskin's theorem \citep{danskin1966theory}: $\nabla \psi^*(\bm{p}) = \argmax_{\mathbf{q} \in \mathrm{dom}(\psi)} \langle \mathbf{p}, \mathbf{q} \rangle - \psi(\mathbf{q})$. We also introduce the $\psi$-Bregman divergence as 
\begin{align}
    D_\psi(\mT, \mQ) := \psi(\mT) - \psi(\mQ) - \langle \mT - \mQ, \nabla \psi(\mQ) \rangle \:.
\end{align} 

The following proposition offers valuable insights into solving the regularized optimal transport (OT) problem using two key strategies: optimization via the primal variables using a projection-based method, or through the dual variables using standard convex optimization techniques.

\begin{proposition}\label{prop:cot}
    Let $\psi : \R^{N_S} \to \R$ satisfy \cref{assumption_psi}. Let $(\bm{a}, \bm{b}, \eta)$ be such that $\gU(\bm{a}, \bm{b}) \cap \overline{\mathcal{B}}(\eta)$ has an interior point and let $\mT^\star$ be a solution of
\begin{align}
    \min_{\mT \in \gU(\bm{a}, \bm{b})} \: \langle \mT, \mC \rangle \quad \text{s.t.} \quad  \mT \in \overline{\mathcal{B}}(\eta) \:.
    \tag{ROT}
\end{align}
    Let $\varepsilon^\star$ be an optimal dual variable associated with $\mT \in \overline{\mathcal{B}}(\eta)$.
    If $\varepsilon^\star > 0$, then
    \begin{itemize}[leftmargin=*, labelsep=0.1em]
        \item for any $0 < \sigma \leq \varepsilon^\star$, it holds, where $\mK_\sigma \coloneqq \nabla \psi^*(-\mC/ \sigma)$, 
        \begin{align}\label{eq:proj-rot}\tag{Proj-ROT}
            \mT^\star = \operatorname{Proj}^{D_\psi}_{\gU(\bm{a}, \bm{b}) \cap \overline{\mathcal{B}}(\eta)}(\mK_\sigma) \:.
        \end{align}
        \item One also has $\mT^\star = \nabla \psi^*((\mC - \bm{\lambda}^\star \oplus \bm{\mu}^\star) / \varepsilon^\star)$
        where $(\bm{\lambda}^\star, \bm{\mu}^\star, \varepsilon^\star)$ solve
        \begin{align}\label{eq:dual-rot}
            \max_{\bm{\lambda}, \bm{\mu}, \varepsilon>0}  \: \langle \bm{\lambda}, \bm{a} \rangle + \langle \bm{\mu}, \bm{b} \rangle + \varepsilon \Big(\sum_i \psi^*((\C_{i:} - \lambda_i \bm{1} - \bm{\mu}) / \varepsilon) - \eta \Big) \:.
        \tag{Dual-ROT}
        \end{align}
    \end{itemize}
\end{proposition}

We present a detailed proof for pedagogical purposes, as it employs tools that will be essential throughout this thesis. While the result has not been explicitly stated in this exact form elsewhere, it draws upon standard derivations commonly found in the OT literature, such as those in \citep{peyre2019computational}. We start by considering the first point related to the primal based formulation of the problem as a single Bregman projection. Then we consider the derivation of the dual formulation.

\paragraph{$\ast$ Bregman Projection Formulation.}

\begin{proof}    
    % \vspace{0.5cm}
    % \textbf{\underline{$\ast$ Part I : Proof of the Bregman projection formulation.}}
    Simplifying the constant terms $\operatorname{Proj}^{D_\psi}_{\gU(\bm{a}, \bm{b}) \cap \overline{\mathcal{B}}(\eta)}(\mK_\varepsilon)$ boils down to the following problem
    \begin{align}
        \min_{\mT \in \gU(\va, \vb)} \quad &\sum_i \psi(\mT_{i:}) - \langle \mT, \nabla \psi(\mK_\sigma) \rangle \\
        \text{s.t.} \quad &\sum_i \psi(\mT_{i:}) \leq \eta \:.
    \end{align}
    This problem is convex and strictly feasible. Strong duality holds thanks to Slater's constraint qualification. Therefore the KKT conditions \citep{boyd2004convex} are necessary and sufficient conditions for optimality.
    The Lagrangian can be expressed as
    \begin{align}
        \mathcal{L}(\mT, \nu, \bm{\lambda}, \bm{\mu}, \mathbf{\Omega}) &= \sum_i \psi(\mT_{i:}) - \langle \mT, \nabla \psi(\mK_\sigma) \rangle + \nu\Big(\sum_i \psi(\mT_{i:}) - \eta \Big) \\
        &+ \langle \bm{\lambda}, \bm{a} - \mT \bm{1} \rangle + \langle \bm{\mu}, \bm{b} - \mT^\top \bm{1} \rangle - \langle \mathbf{\Omega}, \mT \rangle \:.
    \end{align}
    Any optimal primal-dual variables $(\mT^\star, \nu^\star, \bm{\lambda}^\star, \bm{\mu}^\star, \mathbf{\Omega}^\star)$ satisfies
    \begin{align}
            \nabla_{\mT} \mathcal{L}(\mT^\star,  \nu^\star, \bm{\lambda}^\star, \bm{\mu}^\star) &=  (\nu^\star + 1) \nabla \psi(\mT^\star) - \nabla \psi(\mK_\sigma) - \bm{\lambda}^\star \oplus \bm{\mu}^\star - \mathbf{\Omega}^\star = \bm{0} \:.
    \end{align}
    Note that by definition $\mK_\sigma = \nabla \psi^\star(-\mC/\epsilon)$ and thus $\nabla \psi(\mK_\sigma) = \nabla \psi[\nabla \psi^\star(-\mC/\sigma)] = -\mC / \sigma$ \citep{rockafellar1997convex}. Thus we have 
    \begin{align}
        \mC + \sigma (\nu^\star + 1) \nabla \psi(\mT^\star) -  \sigma \bm{\lambda}^\star \oplus \bm{\mu}^\star - \sigma \mathbf{\Omega}^\star= \bm{0} \:.
    \end{align}
    Similarly, for the optimal transport problem of \eqref{prop:cot}
    \begin{align}
        \min_{\mT} \quad \langle \mT, \mC \rangle \quad \text{s.t.} \quad  \mT \in \gU(\bm{a}, \bm{b}) \cap \overline{\mathcal{B}}(\eta) \:,
    \end{align}
    we get the optimality condition for optimal primal-dual variables $(\mT^\star, \varepsilon^\star, \bm{\rho}^\star, \bm{\kappa}^\star, \mathbf{\Lambda}^\star)$
    \begin{align}
        \C + \varepsilon^\star \nabla \psi(\mT^\star) -  \bm{\rho}^\star \oplus \bm{\kappa}^{\star} - \mathbf{\Lambda}^\star = \bm{0} \:.
    \end{align}
    Focusing on the original Bregman projection problem, we can then consider
    \[
    \bm{\lambda} = \bm{\rho}^\star / \sigma, \quad \bm{\mu} = \bm{\kappa}^\star / \sigma, \quad \nu = \varepsilon^\star / \sigma - 1, \quad \mathbf{\Omega} = \mathbf{\Lambda}^\star / \sigma, \quad \mT = \mT^\star.
    \]
    With the above choice, 
    \begin{itemize}
        \item  $(\mT, \nu, \bm{\lambda}, \bm{\mu}, \mathbf{\Omega})$ satisfies the first order optimality condition.
        \item $\mT = \mT^\star \in \gU(\bm{a}, \bm{b}) \cap \mathcal{E}(\eta)$ thus the primal constraint is satisfied. 
        \item $\sigma \leq \varepsilon^\star$ implies that $\nu \geq 0$ and $\mathbf{\Omega}$ has positive entries thereby dual constraints are satisfied. 
        \item $\varepsilon^\star \neq 0$ thus by complementary slackeness $\psi(\mT^\star) = \eta$ hence complementary slackness is also verified for $\mT$ since $\mT = \mT^\star$.
    \end{itemize}
    Therefore the KKT conditions are met hence $(\mT, \nu, \bm{\lambda}, \bm{\mu}, \mathbf{\Omega}) = (\mT^\star, \nu^\star, \bm{\lambda}^\star, \bm{\mu}^\star, \mathbf{\Omega}^\star)$ and $\mT^\star = \mT^\star$.
\end{proof}

Interpreting the problem as a $\psi$-Bregman projection over a convex set as done in \Cref{eq:proj-rot} gives the advantage of employing alternating Bregman projection schemes \citep{benamou2015iterative}. This approach involves decomposing the convex set into an intersection of convex subsets for which projection is straightforward, and then alternating these projections cyclically until convergence. Convergence to the projection onto the intersection is guaranteed if the sets are affine. If the sets are not affine, such as $\overline{\mathcal{B}}(\eta)$, we can introduce auxiliary variables to slightly modify the updates while still ensuring convergence \citep{censor1998dykstra}. This approach is known as \emph{Dykstra's algorithm} \citep{dykstra1983algorithm}. Overall, this framework of Bregman projections is highly flexible and allows for the easy incorporation of various convex constraints by simply applying the corresponding projections \citep{benamou2015iterative}.

\begin{remark}
    The aforementioned characterization as a projection holds under the condition $0 < \sigma \leq \varepsilon^\star$. Intuitively, this condition ensures that the intiial point $\mK_\sigma$ lies outside the set $\overline{\mathcal{B}}(\eta)$ so that the constraint is active when projecting.
    It is important to note that there is a more general method that does not require a well-chosen initial point. This method is based on mirror descent under the geometry induced by the Bregman divergence $D_\psi$. At each iteration, it involves solving a projection problem, relying on Bregman or Dykstra depending on whether all the convex sets are affine. We detail the update of the primal variable $\mT$ and show how it can be expressed as a projection under the Bregman divergence $D_\psi$.
    \begin{align}
    \mT^{(t+1)} &= \argmin_{\mT \in \gC} \: \langle \mT, \mC \rangle + \frac{1}{\gamma^{(t)}} D_{\psi}(\mT, \mT^{(t)}), \\
    &= \argmin_{\mT \in \gC} \: \psi(\mT) - \langle \mT, \nabla \psi(\mT^{(t)}) - \gamma^{(t)} \mC \rangle, \\
    &= \operatorname{Proj}^{D_\psi}_{\gC}(\nabla \psi^*(\nabla \psi(\mT^{(t)}) - \gamma^{(t)} \mC)) \:.
    \end{align}
    In the above derivation, $\gamma^{(t)}$ is the step size at time $t$ and $\gC$ is the convex set of interest.
\end{remark}    


\paragraph{$\ast$ Dual Problem.}

\begin{proof}
% \vspace{0.5cm}
% \textbf{\underline{$\ast$ Part II : Dual problem.}}

The optimal dual variables $(\bm{\lambda}^\star, \bm{\mu}^\star, \varepsilon^\star)$ solve the following problem 
\begin{align}
    &\max_{\bm{\lambda}, \bm{\mu}, \varepsilon > 0} \: \min_{\mT \bm{\geq} \bm{0}} \: \langle \mT, \mC \rangle + \langle \bm{\lambda}, \bm{a} - \mT \bm{1} \rangle + \langle \bm{\mu}, \bm{b} - \mT^\top \bm{1} \rangle + \varepsilon \Big(\sum_i \psi(\mT_{i:}) - \eta \Big) \\
    = &\max_{\bm{\lambda}, \bm{\mu}, \varepsilon> 0} \: \langle \bm{\lambda}, \bm{a} \rangle + \langle \bm{\mu}, \bm{b} \rangle - \varepsilon \eta + \min_{\mT \bm{\geq} \bm{0}} \: \langle \mT, \mC - \bm{\lambda}\bm{1}^\top - \bm{1}\bm{\mu}^\top \rangle + \varepsilon \sum_i \psi(\mT_{i:}) \\
    = &\max_{\bm{\lambda}, \bm{\mu}, \varepsilon>0} \: \langle \bm{\lambda}, \bm{a} \rangle + \langle \bm{\mu}, \bm{b} \rangle - \varepsilon \eta + \min_{\mT \bm{\geq} \bm{0}} \: \sum_i \langle \mT_{i:}, \mC_{i:} - \lambda_i \bm{1} - \bm{\mu} \rangle + \varepsilon \psi(\mT_{i:}) \\
    \stackrel{(\star)}{=} &\max_{\bm{\lambda}, \bm{\mu}, \varepsilon>0}  \: \langle \bm{\lambda}, \bm{a} \rangle + \langle \bm{\mu}, \bm{b} \rangle + \varepsilon \Big(\sum_i \psi^*((\mC_{i:} - \lambda_i \bm{1} - \bm{\mu}) / \varepsilon) - \eta \Big) \:.
    \tag{Dual-ROT}
    \label{eq:dual-ROT}
\end{align}
In ($\star$) we have used that $\psi^*(\bm{x}) = \sup_{\bm{y} \bm{\geq} \bm{0}} \langle \bm{x}, \bm{y} \rangle - \psi(\bm{y})$.
From Danskin's theorem \citep{danskin1966theory}, one can recover the solution of the primal
\begin{align}
    \forall i, \: \mT^\star_{i:} = \nabla \psi^*((\mC_{i:} - \lambda^\star_i \bm{1} - \bm{\mu}^\star) / \varepsilon^\star) \:.
\end{align}
Using matrix notations yields $\mT^\star = \nabla \psi^*((\mC - \bm{\lambda}^\star \oplus \bm{\mu}^\star) / \varepsilon^\star)$
where $(\bm{\lambda}^\star, \bm{\mu}^\star, \varepsilon^\star)$ are the solution of the dual problem \eqref{eq:dual-ROT}.

\end{proof}

Using the dual formulation of \Cref{eq:dual-ROT}, which is convex like any dual problem, as it involves maximizing a concave objective, one can leverage any convex optimization solver, such as stochastic gradient descent (SGD), BFGS \citep{liu1989limited}, or ADAM \citep{kingma2014adam}, to solve the problem exactly.

\begin{mem1}{Relation with the Sinkhorn Algorithm}\label{mem:sinkhorn}
    As highlighted earlier, in the optimal transport (OT) literature, the spread constraint governed by the function $\psi$ is often handled via a fixed dual variable, $\varepsilon^\star$. To establish connections with existing approaches, we now assume $\varepsilon^\star$ is fixed. The most commonly used Bregman divergence for optimal transport is the Kullback-Leibler (KL) divergence, defined as
    \[
    D_{\KL}(\mT \| \mQ) = \langle \mT, \log \left( \mT \oslash \mQ \right) - \bm{1} \bm{1}^\top \rangle,
    \]
    with the associated negative entropy $\psi_{\KL}(\mathbf{p}) = \operatorname{H}(\vp) \coloneqq \langle \mathbf{p}, \log \mathbf{p} - 1 \rangle$.
    In this case, \eqref{eq:cot} simplifies to the widely popular entropic OT \citep{cuturi2013sinkhorn}. Its popularity arises from the significant computational gains it enables, with a runtime complexity of $\gO(N_S \times N_T)$, compared to at least $\gO(N^3 \log N)$, when $N_S \sim N_T \sim N$, required by the network flow algorithm \citep{peyre2019computational}. Moreover, it is highly compatible with modern hardware (see \Cref{remark:ot_gpu}).
    
    \paragraph{Primal formulation.}
    In this case, the primal-based formulation of the problem can be expressed as the Bregman projection $\operatorname{Proj}^{\KL}_{\gU(\bm{a}, \bm{b})}(\mK_{\varepsilon^\star})$, where $\mK_{\varepsilon^\star} = \nabla \psi_{\KL}^{*}(-\mC / \varepsilon^\star) = \exp(-\mC / \varepsilon^\star)$ is a Gibbs kernel. This projection is well-known as the \emph{static Schrödinger bridge} \citep{leonard2013survey}, originally appearing in statistical physics \citep{schrodinger1931umkehrung}.  As we generally do not have access to a closed form for the projection onto the transport polytope $\gU(\bm{a}, \bm{b})$, one can simply alternate the projections onto $\{ \mT \in \gU(\bm{a}) \}$ and $\{ \mT^\top \in \gU(\bm{b}) \}$ separately where $\gU(\vh)$ is the set of discrete couplings with one marginal, defined as $\gU(\vh) = \left\{\mT \mid T_{ij} \geq 0, \: \mT \mathbf{1}_n = \vh \right\}$. It leads to the seminal Sinkhorn algorithm introduced by \cite{cuturi2013sinkhorn}.
    The $\KL$ projection on $\gU(\vh)$ simply reads
    \begin{align}
        \operatorname{Proj}^{\KL}_{\gU(\vh)}(\mT) = \left(\left(\vh \oslash \mT \one  \right) \one^\top \right) \odot \mT
    \end{align}
    and the Sinkhorn algorithm in the primal simply alternates projections onto $\{ \mT \in \gU(\bm{a}) \}$ and $\{ \mT^\top \in \gU(\bm{b}) \}$ until convergence.

    \paragraph{Dual formulation.} Again considering a fixed dual variable $\varepsilon^\star$, the dual problem of \Cref{eq:dual-ROT} can be adressed using a block coordinate strategy on the dual variables $(\bm{\lambda}, \bm{\mu})$. Indeed for a fixed $\bm{\mu}$ one has a closed form solution for $\bm{\lambda}$ and vice versa. This dual ascent strategy can be understood as a Sinkhorn algorithm in log domain:
    \begin{align}
        \forall i \in \integ{N_S}, \quad \lambda^{(t+1)}_i &= \varepsilon^\star \log a_i - \varepsilon^\star  \operatorname{LSE} \big((\bm{\mu}^{(t)} - \C_{i:}) / \varepsilon^\star \big), \\
        \forall j \in \integ{N_T}, \quad \mu^{(t+1)}_j &= \varepsilon^\star \log b_j - \varepsilon^\star  \operatorname{LSE} \big((\bm{\lambda}^{(t+1)} - \C_{:j}) / \varepsilon^\star \big).
    \end{align}
    where for a vector $\bm{\alpha}$, we use the notation
    $\operatorname{LSE}(\bm{\alpha}) = \log \sum_{k} \exp (\alpha_k)$. These updates are numerically stable, as they rely on the \emph{logsumexp} operator $\operatorname{LSE}$.
\end{mem1}

\begin{remark}{(OT on the GPU)}\label{remark:ot_gpu}
    For common choices of regularizers $\Psi$ (e.g., Shannon entropy or $\ell_\alpha$-norms), both formulations \Cref{eq:proj-rot} and \Cref{eq:dual-rot} rely heavily on matrix reductions, which are easily parallelizable and can be significantly accelerated when computed on a GPU.
    
    As highlighted previously, the primal formulation of \Cref{eq:proj-rot} is appealing because it allows for easily adapting the OT problem by incorporating new constraints in the alternating projection scheme. However, working in the primal space can be limiting in terms of memory when using a GPU. Indeed, when $N_S \times N_T$ exceeds a dimensionality on the order of $10^8$, it is likely that the full transport plan $\mT$ will not fit entirely in the GPU memory.
    
    The dual formulation of \Cref{eq:dual-ROT} is particularly useful in this regard, as it allows for symbolic encoding. This means only the dual variables and the formula to reconstruct the OT plan from the input data and these dual variables need to be stored, thus leading to a memory complexity which is linear with respect to the sample size. The OT plan is then accessed only through reductions when necessary, without ever being fully reconstructed in GPU memory. This can be efficiently implemented using the \emph{KeOps} package \citep{charlier2021kernel}.
\end{remark}
    
\begin{remark}
    Another widely used convex regularizer is the squared Euclidean distance, defined as $\psi_{2}(\mathbf{p}) = \frac{1}{2} \| \mathbf{p} \|^2_2$. The corresponding problem \eqref{prop:cot} is commonly referred to as quadratic OT \citep{lorenz2021quadratically} and, unlike the entropic regularization, can result in sparse OT plans. However, this sparsity often results in a slight reduction in computational efficiency, particularly when executed on a GPU.
\end{remark}

% In this work, we focus on a specific class of Bregman divergences called \emph{$\alpha$-divergences} or \emph{Rényi divergences}.
% The \emph{Rényi divergence} \citep{renyi1961measures} for $\mT, \mQ \in \R_+^{N_S \times N_T}$ reads
% \begin{align}
%     D_{\alpha} (\mathbf{P} | \mathbf{Q}) = \frac{1}{\alpha-1} \log \left\langle \mathbf{P}^{\odot \alpha}, \mathbf{Q}^{\odot(1-\alpha)}\right\rangle
%     \tag{$D_\alpha$}
% \end{align}
% where throughout we adopt the conventions that $0/0 = 0$, $0 \log(0) = 0$ and $x/0 = \infty$ for $x > 0$. In the context of Bregman divergence, it corresponds to choosing $\psi = \operatorname{H}_\alpha$, the related \emph{Rényi entropy} (or \emph{$\alpha$-entropy})\citep{ben1978renyi}, where for a vector $\p \in \mathbb{R}^{N_T}_+$, $\operatorname{H}_{\alpha}(\mathbf{p}) = \frac{1}{1-\alpha} \log \left\langle \mathbf{p}^{\odot \alpha}, \mathbf{1} \right\rangle$ with $\bm{1}$ being the all-one vector. Note that these definitions can be extended to $\alpha = 0$, $1$, and $+\infty$ by continuity. Of special note is the limit $\alpha \to 1$
