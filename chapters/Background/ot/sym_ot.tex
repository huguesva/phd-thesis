\section{Enhancing the Ground Metric with Affinities}

We call \emph{affinity} the weight matrix of a graph that encodes this similarity. It has positive entries and the higher the weight in position $(i,j)$, the
higher the similarity or proximity between samples $i$ and $j$.
Seminal approaches relying on affinities include Laplacian
eigenmaps \citep{belkin2003laplacian}, spectral clustering
\citep{von2007tutorial} and semi-supervised learning \citep{zhou2003learning}. Numerous methods can be employed to construct such affinities. A common choice is to use a kernel (\eg Gaussian) derived from a distance matrix normalized by a bandwidth parameter that usually has a large influence on the outcome of the algorithm. 
Indeed, excessively small kernel bandwidth can result in %points \nc{points ?} 
solely capturing the positions of closest neighbors, at the expense of large-scale dependencies. Inversely, setting too large a bandwidth blurs information about close-range pairwise relations. 

Ideally, one should select a different bandwidth for each point to accommodate varying sampling densities and noise levels. One approach is to compute the bandwidth of a point based on the distance from its $k$-th nearest neighbor \citep{zelnik2004self}. However, this method fails to consider the entire distribution of distances.
In general, selecting appropriate kernel bandwidths can be a laborious task, and many practitioners resort to greedy search methods. This can be limiting in some settings, particularly when dealing with large sample sizes.

\subsection{Affinity via Symmetric OT}\label{sec:affinity_sym_ot}

\paragraph{Doubly Stochastic Affinities.}
Doubly stochastic (DS) affinities are non-negative matrices whose rows and columns have unit $\ell_1$ norm. 
In many applications, it has been demonstrated that DS affinity normalization (\ie determining the nearest DS matrix to a given affinity matrix) offers numerous benefits. First, it can be seen as a relaxation of k-means \citep{zass2005unifying} and it is well-established that it enhances spectral clustering performances \citep{Ding_understand,Zass,beauchemin2015affinity}. Additionally, DS matrices present the benefit of being invariant to the various Laplacian normalizations \citep{von2007tutorial}. Recent observations indicate that the DS projection of the Gaussian kernel under the $\KL$ geometry is more resilient to heteroscedastic noise compared to its stochastic counterpart \citep{landa2021doubly}. It also offers a more natural analog to the heat kernel \citep{marshall2019manifold}.
These properties have led to a growing interest in DS affinities, with their use expanding to various applications such as smoothing filters \citep{Milanfar}, subspace clustering \citep{lim2020doubly} and transformers \citep{sander2022sinkformers}.
%attention mechanisms 

\subsection{From Symmetric Entropy-Constrained OT to Sinkhorn Iterations}\label{sec:proof_sinkhorn}

\paragraph{Symmetric Entropy-Constrained Optimal Transport.} Entropy-regularized
OT \citep{peyre2019computational} and its connection to affinity
matrices are crucial components in our solution. In the special case of uniform
marginals, and for $\nu > 0$, entropic OT computes the minimum of $\Pb
\mapsto \langle \Pb, \C \rangle -\nu \sum_{i} \operatorname{H}(\Pb_{i:})$
over the space of doubly stochastic matrices $\{\Pb \in \R_{+}^{n \times n} :
\Pb \bm{1} = \Pb^\top \bm{1} =\bm{1}\}$. The optimal solution is the
\emph{unique} doubly stochastic matrix $\Pb^{\mathrm{ds}}$ of the form $\Pb^{\mathrm{ds}}=\diag(\mathbf{u})\K
\diag(\mathbf{v})$ where $\K = \exp(-\C/\nu)$ is the Gibbs energy derived from
$\C$ and $\mathbf{u}, \mathbf{v}$ are positive vectors that can be found with
the celebrated Sinkhorn-Knopp’s algorithm \citep{cuturi2013sinkhorn,
sinkhorn1964relationship}. Interestingly, when the cost $\C$ is \emph{symmetric}
(\eg $\C \in \mathcal{D}$) we can take $\mathbf{u} = \mathbf{v}$ \cite[Section
5.2]{idel2016review} so that the unique optimal solution is itself symmetric and writes  
\begin{align}\label{eq:plan_sym_sinkhorn}
    \Pb^{\mathrm{ds}} = \exp \left((\mathbf{f} \oplus \mathbf{f} - \Cb) / \nu \right) \text{ where } \mathbf{f} \in \R^n \,.
\tag{DS}
\end{align}
In this case, by relying on convex duality as detailed in Appendix \ref{sec:proof_sinkhorn}, an equivalent formulation for the symmetric entropic OT problem is
% To define a DS affinity from $\X$ using OT, the somewhat counterintuitive idea is to transport the unit vector $\bm{1}$ to itself with transportation cost $\C_{\X}$. Note that, since $\C_{\X} \in \mathcal{D}$ is null on the diagonal, the unconstrained OT plan is trivially the identity matrix $\mathbf{I}_n$. To retrieve information about the samples' neighborhoods, the idea is to force the mass to spread outside the diagonal typically using constraints on the entropy of the OT plan.
% \begin{equation}
% \begin{split}
%     &\min_{\begin{smallmatrix} \Pb \in \R_{+}^{n \times n}: \ \Pb \bm{1} = \bm{1},  \Pb^\top \bm{1} = \bm{1}\end{smallmatrix}} \quad \langle \Pb, \C \rangle \quad -\nu \sum_{i} \operatorname{H}(\Pb_{i:}) 
%     %\text{s.t.} \quad \Pb \bm{1} = \bm{1}, \: \Pb = \Pb^\top \text{ and } \sum_i \operatorname{H}(\Pb_{i:}) \geq \eta \:,
% \end{split}
% \end{equation}
\begin{equation}
\tag{SEOT}
\label{eq:entropy_constrained_OT}
\min_{\Pb \in \R_{+}^{n \times n}} \quad \langle \Pb, \Cb \rangle \quad \text{s.t.} \quad \Pb \bm{1} = \bm{1}, \: \Pb = \Pb^\top \text{ and } \sum_i \operatorname{H}(\Pb_{i:}) \geq \eta \:,
\end{equation}
where $0 \leq \eta \leq n (\log n + 1)$ is a constraint on the global entropy $\sum_i \operatorname{H}(\Pb_{i:})$ of the OT plan $\Pb$ which happens to be saturated at optimum (Appendix \ref{sec:proof_sinkhorn}). This constrained formulation of symmetric entropic OT will provide new insights into entropic affinities, as detailed in the next sections.

% \begin{remark}\label{rem:sink_proj}
%     For a set $\mathcal{E}$, we denote $\operatorname{Proj}^{\operatorname{\KL}}_{\mathcal{E}}(\K) = \argmin_{\Pb \in \mathcal{E}}\KL(\Pb | \K)$ the projection on $\mathcal{E}$ under the $\KL$ geometry. Then \citep{benamou2015iterative} shows that $\Pb^s = \operatorname{Proj}^{\operatorname{\KL}}_{\mathcal{P} \cap \mathcal{S}}(\K)$ is another way of characterizing the solution of problem (\ref{eq:entropy_constrained_OT}), which thus consists in projecting a Gaussian kernel onto the set of doubly stochastic matrices under the $\KL$ geometry. As shown in \citep{Zass}, finding this closest doubly stochastic matrix can be linked to solving a relaxed cluster assignment problem. \titouan{vraiment pas sûr de 'intéret de cette remarque ici, + c'est pas vraiment proj sur P et S, c'est proj sur P, S et entropie globale geq than $\eta$}
%     %In this framework, the KL is linked to normalized cut clustering \citep{shi2000normalized}.
% \end{remark}

%In practice, we fix $\nu$ which is equivalent to setting the entropy lower bound $\eta$ while $\f$ can be computed by solving the dual of (\ref{eq:entropy_constrained_OT}).

%and $\f \in \R^n$ and $\nu \in \R^*_+$ are the optimal Lagrange dual variables associated respectively with the stochasticity and entropy constraints in \eqref{eq:entropy_constrained_OT}. 

%The problem \eqref{eq:entropy_constrained_OT} is convex since $\bm{p} \in \R_+^n \mapsto \operatorname{H}(\bm{p})$ is concave.

%. where 

% and in this paper $\mathcal{P} = \{\Pb \in \mathbb{R}_+^{n \times n} \:
% \text{s.t.} \: \Pb \bm{1} = \bm{1} \}$ is the set of stochastic matrices (each
% sample has a mass of $1$). 
%  matrix, the
% We consider stochastic (\ie row-normalized) similarities without
% self-loop that is the set $\mathcal{P} = \{\Pb \in \mathbb{R}_+^{n \times n} \:
% \text{s.t.} \: \Pb \bm{1} = \bm{1} \}$. 
% % We frame the problem with an entropy constraint instead of the usual entropic penalty. 
% Let us define $\Pb^{\mathrm{s}}$ as the solution of the following strictly convex optimization problem: 
% \begin{align}\label{eq:entropy_constrained_OT}
%     \min_{\Pb \in \mathcal{P} \cap \mathcal{S}} \quad \langle \Pb, \C \rangle \quad
%     \text{s.t.} \quad \sum_i \operatorname{H}(\Pb_i) \geq \eta
% \end{align}
% where $\eta \leq n (\log n + 1)$, this maximum of entropy being reached when
% $\Pb$ is uniform. $\Pb^{\mathrm{s}}$ is required to be DS (\ie normalized in
% both row and column axes) given the domain $\mathcal{P} \cap \mathcal{S}$. 
%It means that, for every data point, it transports a unit mass to all other points ($\Pb \bm{1} = \bm{1}$) while ensuring the reception of a unit mass ($\Pb^\top \bm{1} = \bm{1}$). %Therefore, the above \eqref{eq:entropy_constrained_OT} boils down to finding an OT plan $\Pb$ with unit marginals, where $\C$ plays the role of the transportation cost between the samples.
% which can be linked to the Schrödinger problem \citep{leonard2013survey} \nc{Quel intérêt de faire le lien ici ?}, 

% Relying on strong duality, one can show  that the solution, \ie the OT plan, has the form: 
% \begin{align}\label{eq:plan_sym_sinkhorn}
%     \Pb^{\mathrm{s}} = \exp \left((\f \oplus \f - \C) / \nu \right)
% \tag{DS}
% \end{align}
% where $\f \in \R^n$ and $\nu \in \R^*_+$ are the optimal Lagrange dual variables associated respectively with the stochasticity and entropy constraints. . 

%As derived in appendix \ref{sec:proof_sinkhorn}, $\f$ must obey the following fixed point relation:$f_k
% = - \nu \operatorname{LSE}\big((\f - \C_{:k}) / \nu \big)$  where $\operatorname{LSE}$ stands for \texttt{logsumexp}. Iterating this
% fixed point relation to compute $\f$ yields the celebrated Sinkhorn algorithm (in log domain) \citep{sinkhorn1967concerning}. The latter is very popular in OT, especially thanks to its quadratic complexity \citep{cuturi2013sinkhorn}.

%\begin{remark}\label{rem:sink_proj}
%    For a set $\mathcal{E}$, we denote $\operatorname{Proj}^{\operatorname{\KL}}_{\mathcal{E}}(\K) =    \argmin_{\Pb \in \mathcal{E}}\KL(\Pb | \K)$. Denoting $\K = \exp(-\C/\nu)$, another characterization of this solution can be obtained with a Bregman KL projection: $\Pb^s = \operatorname{Proj}^{\operatorname{\KL}}_{\mathcal{P} \cap \mathcal{S}}(\K)$ \citep{darroch1972generalized}. Hence solving (\ref{eq:entropy_constrained_OT}) is equivalent to projecting a Gaussian kernel $\K$ onto the set of doubly stochastic matrices under the $\KL$ geometry. As shown in \citep{Zass}, finding the closest doubly stochastic matrix to  $\K$ can be linked to solving a relaxed cluster assignment problem. In this framework, the KL is linked to normalized cut clustering \citep{shi2000normalized}.
%\end{remark}



% Using Lagrangian duality, the above problem can be seen as a regularized problem of the form $\langle \Pb, \C \rangle + \varepsilon^\star \langle \operatorname{H}(\Pb), \bm{1} \rangle$, where $\varepsilon^\star$ is the optimal dual variable associated with the entropy constraint.


In this section, we derive Sinkhorn iterations from the problem (\ref{eq:entropy_constrained_OT}). Let $\C \in \mathcal{D}$. 

\begin{proof}
We start by making the constraints explicit.
\begin{align}
    \min_{\Pb \in \R_+^{n \times n}} \quad &\langle \Pb, \C \rangle \\
    \text{s.t.} \quad &\sum_{i \in \integ{n}} \operatorname{H}(\Pb_{i:}) \geq \eta \\
    & \Pb \bm{1} = \bm{1}, \quad \Pb = \Pb^\top \:.
\end{align}
For the above convex problem the Lagrangian writes, where $\nu \in \mathbb{R}_+$, $\mathbf{f} \in \mathbb{R}^n$ and $\bm{\Gamma} \in \mathbb{R}^{n \times n}$:
\begin{align}
    \mathcal{L}(\Pb, \mathbf{f}, \nu, \bm{\Gamma}) &= \langle \Pb, \C \rangle + \Big\langle \nu, \eta - \sum_{i \in \integ{n}} \operatorname{H}(\Pb_i) \Big\rangle + 2\langle \mathbf{f}, \bm{1} - \Pb \bm{1} \rangle + \big\langle \bm{\Gamma}, \Pb - \Pb^\top \big\rangle \:.
\end{align}
Strong duality holds and the first order KKT condition gives for the optimal primal $\Pb^\star$ and dual $(\nu^\star, \mathbf{f}^\star, \bm{\Gamma}^\star)$ variables: 
\begin{align}
    \nabla_{\Pb} \mathcal{L}(\Pb^\star, \mathbf{f}^\star, \nu^\star, \bm{\Gamma}^\star) &= \C + \nu^\star \log{\Pb^\star} - 2\mathbf{f}^\star \bm{1}^\top + \bm{\Gamma}^\star - \bm{\Gamma}^{\star\top} = \bm{0} \:.
\end{align}
Since $\Pb^\star, \C \in \mathcal{S}$ we have $\bm{\Gamma}^\star - \bm{\Gamma}^{\star\top} = \mathbf{f}^\star \bm{1}^\top - \bm{1}\mathbf{f}^{\star \top}$. Hence $\C + \nu^\star \log{\Pb^\star} - \mathbf{f}^\star \oplus \mathbf{f}^\star = \bm{0}$. Suppose that $\nu^\star = 0$ then the previous reasoning implies that $\forall (i,j), C_{ij} = f_i^\star + f_j^\star$. Using that $\C \in \mathcal{D}$ we have $C_{ii} = C_{jj} = 0$ thus $\forall i,  f^\star_i = 0$ and thus this would imply that $\C = 0$ which is not allowed by hypothesis. Therefore $\nu^\star \neq 0$ and the entropy constraint is saturated at the optimum by complementary slackness. Isolating $\Pb^\star$ then yields:
\begin{align}
    \Pb^{\star} &= \exp{\left( (\mathbf{f}^\star \oplus \mathbf{f}^{\star} - \C) / \nu^\star \right)} \:.
\end{align}
$\Pb^\star$ must be primal feasible in particular $\Pb^\star \bm{1} = \bm{1}$. This constraint gives us the Sinkhorn fixed point relation for $\mathbf{f}^\star$:
\begin{align}
    \forall i \in \integ{n}, \quad [\mathbf{f}^\star]_i = - \nu^\star \operatorname{LSE} \big((\mathbf{f}^\star - \C_{:i}) / \nu^\star \big)\,,
\end{align}
where for a vector $\bm{\alpha}$, we use the notation
$\operatorname{LSE}(\bm{\alpha}) = \log \sum_{k} \exp (\alpha_k)$.
\end{proof}


\subsection{Entropic Affinities}

\paragraph{Entropic Affinities and SNE/t-SNE.}
Entropic affinities (EAs) were first introduced in the
seminal paper \emph{Stochastic Neighbor Embedding} (SNE) 
\citep{hinton2002stochastic}. It consists in normalizing each row $i$ of a distance matrix by a
bandwidth parameter $\varepsilon_i$ such that the distribution associated with each row of the corresponding stochastic (\ie row-normalized) Gaussian affinity has a fixed entropy. The value of this entropy, whose exponential is called
the \emph{perplexity}, is then the only hyperparameter left to tune and has
an intuitive interpretation as the number of effective neighbors of each point \citep{vladymyrov2013entropic}.
EAs are notoriously used to encode pairwise relations in a high-dimensional space for the DR algorithm t-SNE \citep{van2008visualizing}, among other DR methods including \citep{carreira2010elastic}. t-SNE is increasingly popular in many applied fields \citep{kobak2019art,
melit2020unsupervised} mostly due to its ability to represent clusters in the data \citep{linderman2019clustering, JMLR:v23:21-0524}. Nonetheless, one major flaw of EAs is that they are inherently directed and often require post-processing symmetrization.

\paragraph{Entropic Affinities (EAs).} Another frequently used approach to generate affinities from $\mathbf{C} \in \mathcal{D}$ is to employ \emph{entropic affinities}  \citep{hinton2002stochastic}. The main idea is to consider \emph{adaptive} kernel bandwidths $(\varepsilon^\star_i)_{i \in \integ{n}}$ to capture finer structures in the data compared to constant bandwidths \citep{van2018recovering}. Indeed, EAs rescale distances to account for the varying density across regions of the dataset.

Given $\xi \in \integ{n-1}$, the goal of EAs is to build a Gaussian Markov chain transition matrix $\Pb^{\mathrm{e}}$ with prescribed entropy as
\begin{equation}
\begin{split}
    \forall i, \: &\forall j, \: P^{\mathrm{e}}_{ij} = \frac{\exp{(-C_{ij}/\varepsilon^\star_i)}}{\sum_\ell \exp{(-C_{i\ell}/\varepsilon^\star_i)}} \\
    &\text{with} \:\: \varepsilon^\star_i \in \mathbb{R}^*_+ \:\: \text{s.t.} \: \operatorname{H}(\Pb^{\mathrm{e}}_{i:}) = \log{\xi} + 1\,. \label{eq:entropic_affinity_pb}
\end{split}\tag{EA}
\end{equation}
The hyperparameter $\xi$, which is also known as \emph{perplexity}, can be interpreted as the effective number of neighbors for each data point \citep{vladymyrov2013entropic}. Indeed, a perplexity of $\xi$ means that each row of $\Pb^{\mathrm{e}}$ (which is a discrete probability since $\Pb^{\mathrm{e}}$ is row-wise stochastic) has the same entropy as a uniform distribution over $\xi$ neighbors. Therefore, it provides the practitioner with an interpretable parameter specifying which scale of dependencies the affinity matrix should faithfully capture. In practice, a root-finding algorithm is used to find the bandwidth parameters $(\varepsilon_i^\star)_{i \in \integ{n}}$ that satisfy the constraints \citep{vladymyrov2013entropic}. Hereafter, with a slight abuse of language, we call $e^{\operatorname{H}(\Pb_{i:})-1}$ the perplexity of the point $i$.

\begin{prob}{Construction of adaptive affinities}
    aa
\end{prob}