\section{Background DistR}

%\paragraph{Unsupervised learning.}
One major objective of unsupervised learning~\cite{Hastie2009} is to provide interpretable and meaningful approximate representations of the data that best preserve its structure \ie the underlying geometric relationships between the data samples.
Similar in essence to Occam's principle frequently employed in supervised learning, the preference for unsupervised data representation often aligns with the pursuit of simplicity, interpretability or visualizability in the associated model.
These aspects are determinant in many real-world applications, such as cell biology
\cite{cantini2021benchmarking, ventre2023one}, where the interaction with domain experts is paramount for interpreting the results and extracting meaningful insights from the model. 
% \nc{or to assess the separability of data embeddings within deep neural layers.}

\paragraph{Dimensionality reduction and clustering.}
When faced with the question of extracting
interpretable representations, from a dataset $\mX = (\vx_1, ..., \vx_N) ^\top
\in \R^{N \times p}$ of $N$ samples in $\R^p$, the machine learning community
has proposed a variety of methods. Among them, dimensionality reduction (DR) algorithms have been widely used to summarize data in a low-dimensional space
$\mZ = (\vz_1, ..., \vz_N) ^\top \in \R^{N \times d}$ with $d \ll p$, allowing
for visualization of every individual points for small enough $d$ \cite{agrawal2021minimum,van2009dimensionality}.
Another major approach is to cluster the data into $n$ groups, with $n$
typically much smaller than $N$, and to summarize these groups through their centroids \cite{saxena2017review,ezugwu2022comprehensive}.
%by clusteres defined as  centroids or groups of samples. 
Clustering is particularly interpretable since it provides a
smaller number of points that can be easily inspected. %visualized or interpreted. 
The cluster assignments can also be analyzed. %but also assignment of each original point to a cluster. 
Both DR and clustering follow a
similar philosophy of summarization and reduction of the dataset using a smaller size representation.
% matrix. 

\paragraph{Two sides of the same coin.} As a matter of fact, methods from both families share many similitudes, %among which is
including the construction of a similarity graph between input samples. In clustering, many popular approaches design a reduced or coarsened version of the initial similarity graph while preserving some of its spectral properties~\cite{von2007tutorial, schaeffer2007graph}. 
%attributes related to the graph spectrum 
In DR, the goal is to solve the inverse problem of finding low-dimensional embeddings that generate a similarity graph close to the %initial
one computed from input data points \cite{ham2004kernel,hinton2002stochastic}.
%With so many similarities
Our work builds on these converging viewpoints and addresses the following question: \emph{can DR and clustering  be expressed in a common and unified framework ?}




\section{Background on Dimensionality Reduction and Optimal Transport}

We start by reviewing the most popular DR approaches and we introduce the Gromov-Wasserstein problem.

\subsection{Unified View of Dimensionality Reduction \label{sec:dr_methods}}

Let $\mX = (\vx_1, ..., \vx_N) ^\top \in \R^{N \times p}$ be an input dataset. Dimensionality reduction focuses on constructing a low-dimensional representation or \emph{embedding} $\mZ = (\vz_1, ..., \vz_N)^\top \in \R^{N \times d}$, where $d< p$. The latter should preserve a prescribed geometry for the dataset encoded via a symmetric pairwise similarity matrix $\mC_{\mX} \in \R_+^{N \times N}$ obtained from $\mX$.
To this end, most popular DR methods optimize $\mZ$ such that a certain pairwise similarity matrix in the output space matches $\mC_{\mX}$ according to some criteria. We subsequently introduce the functions
\begin{equation}
\label{eq:sim_function}
\simiX: \R^{N \times p} \to \R^{N \times N}, \simi: \R^{N \times d} \to \R^{N \times N}\,,
\end{equation}
which define pairwise similarity matrices in the input and output space from embeddings $\mX$ and $\mZ$.
 The DR problem can be formulated quite generally as the optimization problem
%  minimizing over $\mZ$ the objective function
\begin{equation}
\label{eq:DR_criterion}\tag{DR}
\min_{\mZ \in \R^{N \times d}} \: \sum_{(i,j) \in \integ{N}^2}  L\big([\simiX(\mX)]_{ij}, [\simi(\mZ)]_{ij}\big) \,. %+ R(\mZ)\,.
\end{equation}
where $L:\R \times \R \rightarrow \R$ is a loss that quantifies how similar are two points in the input space $\R^{p}$ compared to two points in the output space $\R^{d}$. Various losses are used, such as the quadratic loss $L_2(x,y) \coloneqq (x - y)^2$
%  the cross-entropy $L_{\CE}(x,y) \coloneqq - x \log(y)$  
or the Kullback-Leibler divergence $L_{\KL}(x,y) \coloneqq x \log (x/y) - x +y$.
Below, we recall several popular methods that can be placed within this framework.
%$R: \R^{N \times d} \to \R$ is a regularizer term on the embeddings $\mZ$.  

\paragraph{Spectral methods.}
%\footnote{\ie\ for any $(\bm{x}_1, ..., \bm{x}_N) \in \mathcal{X}^N$ and $(a_1, ..., a_N) \in \R^N, \sum_{ij} a_i a_j k_{\mathcal{X}}(\bm{x}_i, \bm{x}_j) \geq 0$.} 
When $\simiX(\mX)$ is a positive semi-definite matrix, \cref{eq:DR_criterion}
recovers spectral methods by choosing the quadratic loss $L = L_2$ and $\simi(\mZ) = (\langle \vz_i, \vz_j \rangle)_{(i,j) \in \integ{N}^2}$ the matrix of inner
products in the embedding space. Indeed, in this case, the objective value of \cref{eq:DR_criterion}
reduces to
\begin{equation*}
\label{eq:spectral_method}
	\sum_{(i,j) \in \integ{N}^2} \!\!\!\!\! L_2([\simiX(\mX)]_{ij}, \langle \vz_i, \vz_j \rangle) = \| \simiX(\mX) - \mZ\mZ^\top \|^2_F\,
\end{equation*}
where $\|\cdot\|_F$ is the Frobenius norm. This problem is commonly known as kernel Principal Component Analysis (PCA)~\cite{scholkopf1997kernel}
 and an optimal solution is given by
$\mZ^\star = (\sqrt{\lambda_1} \vv_{1}, ..., \sqrt{\lambda_d} \vv_{d})^\top$ where $\lambda_i$ is the $i$-th
largest eigenvalue of $\simiX(\mX)$ with corresponding eigenvector $\vv_{i}$ 
\cite{eckart1936approximation}.  As shown by \citet{ham2004kernel, ghojogh2021unified}, numerous dimension reduction methods can be categorized in this manner.
This includes PCA when $\simiX(\mX) = \mX\mX^\top$ is the matrix of inner products in the input space; (classical) multidimensional scaling \cite{borg2005modern}, when $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX \mH$ with $\mD_\mX$ the matrix of squared euclidean distance between the points in $\R^{p}$ and $\mH = \mI_N - \frac{1}{N} \one_N \one_N^\top$ is the centering matrix; Isomap \cite{tenenbaum2000global}, with $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX^{(g)} \mH$ with $\mD_\mX^{(g)}$ the geodesic distance matrix; Laplacian Eigenmap
\cite{belkin2003laplacian}, with $\simiX(\mX) = \mL_\mX^{\dagger}$ the pseudo-inverse of the Laplacian associated to some adjacency matrix $\mW_\mX$; but also Locally Linear Embedding \cite{roweis2000nonlinear}, and Diffusion Map \cite{coifman2006diffusion} (for all of these examples we refer to \citealt[Table 1]{ghojogh2021unified}).

\paragraph{Neighbor embedding methods.} An alternative group of methods relies on neighbor embedding techniques which consists in minimizing in $\mZ$ the quantity
% \begin{equation}
% \label{eq:neighbor_techniques}
% 	- \sum_{(i,j) \in \integ{N}^2} [\mC_{\mX}]_{ij} \log [\simi(\mZ)]_{ij}\,.
% \end{equation}
\begin{equation}\tag{NE}
	\label{eq:neighbor_techniques}
	\sum_{(i,j) \in \integ{N}^2} L_{\KL}([\simiX(\mX)]_{ij}, [\simi(\mZ)]_{ij}) \:.
\end{equation}
Within our framework, this corresponds to \cref{eq:DR_criterion} with $L = L_{\KL}$. The objective function of popular methods such as stochastic neighbor embedding (SNE) \cite{hinton2002stochastic} or t-SNE \cite{van2008visualizing} can be derived from \cref{eq:neighbor_techniques} with a particular choice of $\mC_{X}, \mC_{Z}$. For instance SNE and t-SNE both consider in the input space a symmetrized version of the entropic affinity \cite{vladymyrov2013entropic,van2023snekhorn}. In the embedding space, $\mC_Z(\mZ)$ is usually constructed from a ‘‘kernel'' matrix $\mK_\mZ$ which undergoes a scalar \cite{van2008visualizing}, row-stochastic \cite{hinton2002stochastic} or doubly stochastic \cite{lu2019doubly,van2023snekhorn} normalization. Gaussian kernel $[\mK_\mZ]_{ij} = \exp(-\|\vz_i-\vz_j\|_2^2)$, or heavy-tailed Student-t kernel $[\mK_\mZ]_{ij} = (1+\|\vz_i-\vz_j\|_2^2)^{-1}$, are typical choices \cite{van2008visualizing}. We also emphasize that one can retrieve the UMAP objective \cite{mcinnes2018umap} from \cref{eq:DR_criterion} using the binary cross-entropy loss. As described in \cite{van2022probabilistic} from a probabilistic point of view, all these objectives can be derived from a common Markov random field model with various graph priors.
\begin{remark}
The usual formulations of neighbor embedding methods rely on the loss $L(x,y) = x \log(x/y)$. However, due to the normalization, the total mass $\sum_{ij} [\mC_Z(\mZ)]_{ij}$ is constant (often equal to $1$) in all of the cases mentioned above. Thus the minimization in $\mZ$ with the $L_{\KL}$ formulation is equivalent.
\end{remark}

\paragraph{Hyperbolic geometry.} The presented DR methods can also be extended to incorporate non-Euclidean geometries. Hyperbolic spaces \cite{Chami21, Fan_2022_CVPR, Guo22, Lin23} are of particular interest as they can capture hierarchical structures more effectively than Euclidean spaces and mitigate the curse of dimensionality by producing representations with lower distortion rates. 
For instance, \citet{Guo22} adapted t-SNE by using the Poincaré distance and by changing the Student's t-distribution with a more general hyperbolic Cauchy distribution.  Notions of projection subspaces can also be adapted, \eg \citet{Chami21} use horospheres as one-dimensional subspaces. 
% \begin{remark}

% 	% where data projections have maximal variances in PCA-like methods.
% \end{remark}
% }


% and respectively choose $[\simi(\mZ)]_{ij} = \exp(-\|\bm{z}_i - \bm{z}_j\|_2^2)/ \sum_{nm} \exp(-\|\bm{z}_n - \bm{z}_m\|_2^2)$ and $(1+\|\bm{z}_i - \bm{z}_j\|_2^2)^{-1}/ \sum_{nm} (1+\|\bm{z}_n - \bm{z}_m\|_2^2)^{-1}$.

% \tv{petite phrase sur UMAP aussi ? avec une loss cross entropie ?}

%\cvc{+hyperbolic to do : motivation, many SP and NE methods are adapted to this setting, quick exemples }


% Classical OT methods require defining a meaningful transportation cost between the supports of the two distributions. 
% This is however difficult in the context of dimensionality reduction where the two spaces $\R^p$ and $\R^d$ have different dimensions.



% \textbf{Linear optimal transport.}
% We consider two Polish spaces $\mathcal{X}$ and $\mathcal{Y}$ such that we can define a cost function $c_{\mathcal{X} \mathcal{Y}} : \mathcal{X} \times \mathcal{Y} \to \R_+$. Let $\mu \in \mathcal{P}(\mathcal{X})$ and $\nu \in \mathcal{P}(\mathcal{Y})$ be two probability measures that we aim to compare.
% The original formulation \cite{monge1781memoire} of OT seeks the map $T$ satisfying $T_{\#}\mu = \nu$ that minimizes the transportation cost given by
% \begin{align}\label{eq:monge_pb}
% 	M(\mu, \nu) \coloneqq \inf_{T_{\#}\mu = \nu} \int_{\mathcal{X}} c_{\mathcal{X} \mathcal{Y}}(\bm{x}, T(\bm{x})) \mathrm{d}\mu(\bm{x}) \:.
% \end{align}
% The above formulation is highly non-linear in $T$ and the set of admissible maps $T$ is not convex hindering an easy analysis. Moreover the existence of an optimal map $T$ is not guaranteed in general.
% To resolve this, a popular relaxation by \cite{kantorovich1942translocation} consists of optimizing instead over the space of probabilistic couplings with marginals $\mu$ and $\nu$
% \begin{align}\label{eq:Wasserstein}
% 	W(\mu, \nu) \coloneqq \inf_{\pi \in \Pi(\mu, \nu)} \int_{\mathcal{X} \times \mathcal{Y}} c_{\mathcal{X} \mathcal{Y}}(\bm{x}, \bm{y}) \mathrm{d}\pi(\bm{x}, \bm{y}) \:.
% \end{align}
% This formulation is a convex optimization problem and the infimum is well
% defined under mild assumptions \cite{santambrogio2015optimal}. If the optimal
% coupling $\pi^\star$ is supported on a \emph{deterministic} function, \ie
% $\pi^\star$ is of the form $(\mathrm{id} \times T^\star)\# \mu$, then
% $T^\star$ solves \eqref{eq:monge_pb}. This holds under the assumption that one
% of the inputs is absolutely continuous with respect to the Lebesgue measure
% for $c_{\mathcal{X} \mathcal{Y}}(\bm{x}, \bm{y}) = h(\bm{x} - \bm{y})$ with
% $h$ strictly convex \cite{gangbo1996geometry}. In the case of discrete
% measures, the equivalence holds if $\mu = \frac{1}{N} \sum_{i \in \integ{N}}
% \delta_{\bm{x}_i}$ and $\nu = \frac{1}{N} \sum_{i \in \integ{N}}
% \delta_{\bm{y}_i}$ as the solution of \eqref{eq:Wasserstein} is reached at an
% extremal point of the polytope of doubly stochastic matrices
% \cite{bertsimas1997introduction}.
% It is usually impossible to define a meaningful transportation cost from two spaces $\mathcal{X}$ and $\mathcal{Z}$ that are not part of a common ground metric space
% This occurs when considering ambient Euclidean spaces of different dimensions \ie $\mathcal{X} \subseteq \R^p$ and $\mathcal{Z} \subseteq \R^d$ with $p \neq d$ which is precisely the context of dimensionality reduction.


% We first introduce the general GW problem before highlighting its utility to compare distributions in incomparable spaces.
%\tv{Pas d'accord avec les notations: les similarité dans le cas du DR sont pas des "fonctions" de pairwise distance, ce sont des matrices qui prennent tous les points en entrée (normalisation)}

\subsection{Optimal Transport Across Spaces}

Optimal Transport (OT) \cite{villani2009optimal,peyre2019computational} is a popular
framework for comparing probability distributions and is at the core of our contributions. We review in this section the Gromov-Wasserstein formulation of OT aiming at comparing distributions ‘‘across spaces''.

\paragraph{Gromov-Wasserstein (GW).} The GW framework \cite{memoli2011gromov,sturm2012space} comprises a collection of OT methods designed to compare distributions by examining the pairwise relations \emph{within each domain}. For two matrices $\mC \in \R^{N \times N}$, $\overline{\mC} \in \R^{n \times n}$, and weights $\vh \in \Sigma_N, \overline{\vh} \in \Sigma_n$, the GW discrepancy is defined as
\begin{equation}
\label{eq:gw_pb} 
\tag{GW}
\begin{split}
	&\GW_L (\mC, \overline{\mC}, \vh, \overline{\vh}) \coloneqq \min_{\mT \in \gU(\vh, \overline{\vh})} E_L(\mC, \overline{\mC}, \mT)\,, \\
	&\text{where} \quad E_L(\mC, \overline{\mC}, \mT) \coloneqq \sum_{ijkl}  L(C_{ij}, \overline{C}_{kl}) T_{ik} T_{jl}\,,
\end{split}
\end{equation}
% \end{align}
% \begin{align*}
%\sum_{\substack{(i,j) \in \integ{N}^2 \\ (k,l) \in \integ{n}^2}}
%where  $L:\R \times \R \rightarrow \R_+$ is a loss 
% \footnote[1]{our definition of $L_{\mathrm{KL}}$ differs from \cite{peyre2016gromov} where the generalized Kullback-Leibler divergence is considered.}
and $\gU(\vh, \overline{\vh}) = \left\{ \mT \in \R_+^{N \times n} : \mT \bm{1}_n = \vh, \mT^\top \bm{1}_N = \overline{\vh} \right\}$ is the set of couplings between $\vh$ and $\overline{\vh}$.
In this formulation, both pairs $(\mC, \vh)$ and $(\overline{\mC}, \overline{\vh})$ can be interpreted as graphs with corresponding connectivity matrices $\mC, \overline \mC$, and where nodes are weighted by histograms $\vh, \overline \vh$. \Cref{eq:gw_pb} is thus a \emph{quadratic problem} (in $\mT$) which consists in finding a soft-assignment matrix $\mT$ that aligns the nodes of the two graphs in a way that preserves their pairwise connectivities. 

From a distributional perspective, GW can also be viewed as a distance between distributions that do not belong to the same metric space. For two discrete probability distributions $\mu_X = \sum_{i=1}^N [\vh_X]_i \delta_{\vx_i} \in \gP_N(\R^{p}), \mu_Z =\sum_{i=1}^n [\vh_Z]_i \delta_{\vz_i} \in \gP_n(\R^d)$ and pairwise similarity matrices $\simiX(\mX)$ and $\simi(\mZ)$ associated with the supports $\mX = (\vx_1, \cdots, \vx_n)^\top$ and $\mZ = (\vz_1, \cdots, \vz_n)^\top$, the quantity $\GW_L (\simiX(\mX), \simi(\mZ), \vh_X, \vh_Z)$ is a measure of dissimilarity or discrepancy between $\mu_X, \mu_Z$. Specifically, when $L=L_2$, and $\simiX(\mX), \simi(\mZ)$ are pairwise distance matrices, GW defines a proper distance between $\mu_X$ and $\mu_Z$ with respect to measure preserving isometries\footnote{With weaker assumptions on $\simiX, \simi$, GW defines a pseudo-metric \textit{w.r.t.} a different notion of isomorphism \cite{chowdhury2019gromov}. See Appendix \ref{sec:srGW_divergence} for more details.} \cite{memoli2011gromov}. 

Due to its versatile properties, notably in comparing distributions over different domains,  the GW problem has found many applications in machine learning, \textit{e.g.,} for 3D meshes alignment \cite{solomon2016entropic,ezuz2017gwcnn}, NLP \cite{alvarez2018gromov}, (co-)clustering  \cite{peyre2016gromov, redko2020co}, single-cell analysis \cite{demetci2020gromov}, neuroimaging \cite{thual2022aligning}, graph representation learning \cite{xu2020gromov, vincent2021online, liu2022robust, vincent2022template, pmlr-v202-zeng23c} and partitioning \cite{xu2019scalable, chowdhury2021generalized}.

In this work, we leverage the GW discrepancy to extend classical DR approaches, framing them as the projection of a distribution onto a space of lower dimensionality. 