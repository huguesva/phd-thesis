\section{Background DistR}






\section{Background on Dimensionality Reduction and Optimal Transport}

We start by reviewing the most popular DR approaches and we introduce the Gromov-Wasserstein problem.

\subsection{Unified View of Dimensionality Reduction \label{sec:dr_methods}}

Let $\mX = (\vx_1, ..., \vx_N) ^\top \in \R^{N \times p}$ be an input dataset. Dimensionality reduction focuses on constructing a low-dimensional representation or \emph{embedding} $\mZ = (\vz_1, ..., \vz_N)^\top \in \R^{N \times d}$, where $d< p$. The latter should preserve a prescribed geometry for the dataset encoded via a symmetric pairwise similarity matrix $\mC_{\mX} \in \R_+^{N \times N}$ obtained from $\mX$.
To this end, most popular DR methods optimize $\mZ$ such that a certain pairwise similarity matrix in the output space matches $\mC_{\mX}$ according to some criteria. We subsequently introduce the functions
\begin{equation}
\label{eq:sim_function}
\simiX: \R^{N \times p} \to \R^{N \times N}, \simi: \R^{N \times d} \to \R^{N \times N}\,,
\end{equation}
which define pairwise similarity matrices in the input and output space from embeddings $\mX$ and $\mZ$.
 The DR problem can be formulated quite generally as the optimization problem
%  minimizing over $\mZ$ the objective function
\begin{equation}
\label{eq:DR_criterion}\tag{DR}
\min_{\mZ \in \R^{N \times d}} \: \sum_{(i,j) \in \integ{N}^2}  L\big([\simiX(\mX)]_{ij}, [\simi(\mZ)]_{ij}\big) \,. %+ R(\mZ)\,.
\end{equation}
where $L:\R \times \R \rightarrow \R$ is a loss that quantifies how similar are two points in the input space $\R^{p}$ compared to two points in the output space $\R^{d}$. Various losses are used, such as the quadratic loss $L_2(x,y) \coloneqq (x - y)^2$
%  the cross-entropy $L_{\CE}(x,y) \coloneqq - x \log(y)$  
or the Kullback-Leibler divergence $L_{\KL}(x,y) \coloneqq x \log (x/y) - x +y$.
Below, we recall several popular methods that can be placed within this framework.
%$R: \R^{N \times d} \to \R$ is a regularizer term on the embeddings $\mZ$.  

\paragraph{Spectral methods.}
%\footnote{\ie\ for any $(\bm{x}_1, ..., \bm{x}_N) \in \mathcal{X}^N$ and $(a_1, ..., a_N) \in \R^N, \sum_{ij} a_i a_j k_{\mathcal{X}}(\bm{x}_i, \bm{x}_j) \geq 0$.} 
When $\simiX(\mX)$ is a positive semi-definite matrix, \cref{eq:DR_criterion}
recovers spectral methods by choosing the quadratic loss $L = L_2$ and $\simi(\mZ) = (\langle \vz_i, \vz_j \rangle)_{(i,j) \in \integ{N}^2}$ the matrix of inner
products in the embedding space. Indeed, in this case, the objective value of \cref{eq:DR_criterion}
reduces to
\begin{equation*}
\label{eq:spectral_method}
	\sum_{(i,j) \in \integ{N}^2} \!\!\!\!\! L_2([\simiX(\mX)]_{ij}, \langle \vz_i, \vz_j \rangle) = \| \simiX(\mX) - \mZ\mZ^\top \|^2_F\,
\end{equation*}
where $\|\cdot\|_F$ is the Frobenius norm. This problem is commonly known as kernel Principal Component Analysis (PCA)~\cite{scholkopf1997kernel}
 and an optimal solution is given by
$\mZ^\star = (\sqrt{\lambda_1} \vv_{1}, ..., \sqrt{\lambda_d} \vv_{d})^\top$ where $\lambda_i$ is the $i$-th
largest eigenvalue of $\simiX(\mX)$ with corresponding eigenvector $\vv_{i}$ 
\cite{eckart1936approximation}.  As shown by \citet{ham2004kernel, ghojogh2021unified}, numerous dimension reduction methods can be categorized in this manner.
This includes PCA when $\simiX(\mX) = \mX\mX^\top$ is the matrix of inner products in the input space; (classical) multidimensional scaling \cite{borg2005modern}, when $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX \mH$ with $\mD_\mX$ the matrix of squared euclidean distance between the points in $\R^{p}$ and $\mH = \mI_N - \frac{1}{N} \one_N \one_N^\top$ is the centering matrix; Isomap \cite{tenenbaum2000global}, with $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX^{(g)} \mH$ with $\mD_\mX^{(g)}$ the geodesic distance matrix; Laplacian Eigenmap
\cite{belkin2003laplacian}, with $\simiX(\mX) = \mL_\mX^{\dagger}$ the pseudo-inverse of the Laplacian associated to some adjacency matrix $\mW_\mX$; but also Locally Linear Embedding \cite{roweis2000nonlinear}, and Diffusion Map \cite{coifman2006diffusion} (for all of these examples we refer to \citealt[Table 1]{ghojogh2021unified}).

