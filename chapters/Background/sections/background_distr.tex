\newpage

Exploring and analyzing high-dimensional data is a core problem of data science. As presented in \Cref{c-intro}, it often requires constructing dense low-dimensional and interpretable representations of the data. When a ground distance metric is available on $\mathcal{X}$, learning these representations amounts to solving a dimensionality reduction (DR) problem. In this chapter, we start by reviewing the existing litterature on DR methods in \Cref{sec:background_dr}.

\paragraph{Dimensionality reduction context.}
Throughout, we consider an input dataset with $N$ vectorial data points with dimensionality $p$ \ie $\mX = (\vx_1, ..., \vx_N) ^\top$ where for all i, $\vx_i \in \mathcal{X} \subset \R^{p}$. Dimensionality reduction focuses on constructing a low-dimensional representation or \emph{embedding} $\mZ = (\vz_1, ..., \vz_N)^\top \in \R^{N \times d}$, where $d< p$. 

\section{Overview of Dimensionality Reduction Methods}\label{sec:background_dr}

Ideally, these representations should preserve the data structure by mimicking, in the reduced representation space (called \emph{latent space}), the geometric structure of the data samples. 

a notion of similarity between samples.

The latter should preserve a prescribed geometry for the dataset encoded via a symmetric pairwise similarity matrix $\mC_{\mX} \in \R_+^{N \times N}$ obtained from $\mX$.

We start by reviewing the most popular DR approaches and we introduce the Gromov-Wasserstein problem.


To this end, most popular DR methods optimize $\mZ$ such that a certain pairwise similarity matrix in the output space matches $\mC_{\mX}$ according to some criteria. We subsequently introduce the functions
\begin{equation}
\label{eq:sim_function}
\simiX: \R^{N \times p} \to \R^{N \times N}, \simi: \R^{N \times d} \to \R^{N \times N}\,,
\end{equation}
which define pairwise similarity matrices in the input and output space from embeddings $\mX$ and $\mZ$.
 The DR problem can be formulated quite generally as the optimization problem
%  minimizing over $\mZ$ the objective function
\begin{equation}
\label{eq:DR_criterion}\tag{DR}
\min_{\mZ \in \R^{N \times d}} \: \sum_{(i,j) \in \integ{N}^2}  L\big([\simiX(\mX)]_{ij}, [\simi(\mZ)]_{ij}\big) \,. %+ R(\mZ)\,.
\end{equation}
where $L:\R \times \R \rightarrow \R$ is a loss that quantifies how similar are two points in the input space $\R^{p}$ compared to two points in the output space $\R^{d}$. Various losses are used, such as the quadratic loss $L_2(x,y) \coloneqq (x - y)^2$
%  the cross-entropy $L_{\CE}(x,y) \coloneqq - x \log(y)$  
or the Kullback-Leibler divergence $L_{\KL}(x,y) \coloneqq x \log (x/y) - x +y$.
Below, we recall several popular methods that can be placed within this framework.
%$R: \R^{N \times d} \to \R$ is a regularizer term on the embeddings $\mZ$.  

\begin{rem2}{Kernels and Reproducing Kernel Hilbert Spaces}
	A kernel $\kappa(\cdot, \cdot)$ is positive definite if for an arbitrary number of points $(\bm{x}_1, ..., \bm{x}_n) \in \mathcal{X}^n$, the kernel matrix $\bm{K} = (\kappa(\bm{x}_i, \bm{x}_j))_{(i,j) \in [n]^2}$ is positive definite. The Aronszajn's theorem states that an equivalent condition to $\kappa$ being positive definite is the existence of a Hilbert space $\mathcal{H}$ and a mapping $\phi : \mathcal{X} \to \mathcal{H}$ such that for any $(\bm{x}, \bm{x'}) \in \mathcal{X}^2$,
\begin{align}\label{eq:kernel_inner_product}
    \kappa(\bm{x}, \bm{x'}) = \langle \phi(\bm{x}), \phi(\bm{x'}) \rangle_{\mathcal{H}} \:.
\end{align}
Of particular interest is the Hilbert space $\mathcal{H}$ associated to $\kappa$ called reproducing kernel Hilbert space (RKHS) which is a space of functions from $\mathcal{X}$ to $\mathbb{R}$ such that for any $\bm{x} \in \mathcal{X}$, the function $\kappa_{\bm{x}} : \bm{t} \to \kappa(\bm{x}, \bm{t})$ is in $\mathcal{H}$ and for any $f \in \mathcal{H}$, $f(\bm{x}) = \langle f, \kappa_{\bm{x}} \rangle_{\mathcal{H}}$. $\kappa$ is then called the reproducing kernel of $\mathcal{H}$. An important result is that if $\mathcal{H}$ is a RKHS, then it has a unique reproducing kernel. Conversely, a positive definite kernel $\kappa$ can be the reproducing kernel of at most one RKHS.


An equivalent definition of a RKHS on $\mathcal{X}$ is a Hilbert
space $\mathcal{H} \subset \mathbb{R}^{\mathcal{X}}$ (functions from $\mathcal{X}$ to $\mathbb{R}$) where all evaluation functionals $\delta_x : \mathcal{H} \to \mathbb{R}$,
defined by $\delta_x(f) = f(x)$, are continuous.

Equation \ref{eq:kernel_inner_product} tells us that the inner product between two feature vectors is given by the kernel $\kappa$. Hence we can evaluate the inner product of any two feature vectors efficiently without knowing an explicit form of either $\phi$ or $\mathcal{H}$. With this computation of inner product, many linear methods of classical data analysis can be extended to nonlinear ones using the kernel matrix which computation does not depend on the dimensionality of the feature space. Such strategy is known as the \textit{kernel trick} in the literature.
\end{rem2}



\subsection{Spectral methods}

%\footnote{\ie\ for any $(\bm{x}_1, ..., \bm{x}_N) \in \mathcal{X}^N$ and $(a_1, ..., a_N) \in \R^N, \sum_{ij} a_i a_j k_{\mathcal{X}}(\bm{x}_i, \bm{x}_j) \geq 0$.} 
When $\simiX(\mX)$ is a positive semi-definite matrix, \cref{eq:DR_criterion}
recovers spectral methods by choosing the quadratic loss $L = L_2$ and $\simi(\mZ) = (\langle \vz_i, \vz_j \rangle)_{(i,j) \in \integ{N}^2}$ the matrix of inner
products in the embedding space. Indeed, in this case, the objective value of \cref{eq:DR_criterion}
reduces to
\begin{equation*}
\label{eq:spectral_method}
	\sum_{(i,j) \in \integ{N}^2} \!\!\!\!\! L_2([\simiX(\mX)]_{ij}, \langle \vz_i, \vz_j \rangle) = \| \simiX(\mX) - \mZ\mZ^\top \|^2_F\,
\end{equation*}
where $\|\cdot\|_F$ is the Frobenius norm. This problem is commonly known as kernel Principal Component Analysis (PCA)~\cite{scholkopf1997kernel}
 and an optimal solution is given by
$\mZ^\star = (\sqrt{\lambda_1} \vv_{1}, ..., \sqrt{\lambda_d} \vv_{d})^\top$ where $\lambda_i$ is the $i$-th
largest eigenvalue of $\simiX(\mX)$ with corresponding eigenvector $\vv_{i}$ 
\cite{eckart1936approximation}.  


\begin{rem1}{Principal Component Analysis (PCA)}
detailed treatment given in 
\end{rem1}



As shown by \citet{ham2004kernel, ghojogh2021unified}, numerous dimension reduction methods can be categorized in this manner.
This includes PCA when $\simiX(\mX) = \mX\mX^\top$ is the matrix of inner products in the input space; (classical) multidimensional scaling \cite{borg2005modern}, when $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX \mH$ with $\mD_\mX$ the matrix of squared euclidean distance between the points in $\R^{p}$ and $\mH = \mI_N - \frac{1}{N} \one_N \one_N^\top$ is the centering matrix; Isomap \cite{tenenbaum2000global}, with $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX^{(g)} \mH$ with $\mD_\mX^{(g)}$ the geodesic distance matrix; Laplacian Eigenmap
\cite{belkin2003laplacian}, with $\simiX(\mX) = \mL_\mX^{\dagger}$ the pseudo-inverse of the Laplacian associated to some adjacency matrix $\mW_\mX$; but also Locally Linear Embedding \cite{roweis2000nonlinear}, and Diffusion Map \cite{coifman2006diffusion} (for all of these examples we refer to \citealt[Table 1]{ghojogh2021unified}).


\subsection{Neighbor Embedding Methods}

An alternative group of methods relies on neighbor embedding techniques which consists in minimizing in $\mZ$ the quantity
% \begin{equation}
% \label{eq:neighbor_techniques}
% 	- \sum_{(i,j) \in \integ{N}^2} [\mC_{\mX}]_{ij} \log [\simi(\mZ)]_{ij}\,.
% \end{equation}
\begin{equation}\tag{NE}
	\label{eq:neighbor_techniques}
	\sum_{(i,j) \in \integ{N}^2} L_{\KL}([\simiX(\mX)]_{ij}, [\simi(\mZ)]_{ij}) \:.
\end{equation}
Within our framework, this corresponds to \cref{eq:DR_criterion} with $L = L_{\KL}$. The objective function of popular methods such as stochastic neighbor embedding (SNE) \citep{hinton2002stochastic} or t-SNE \citep{van2008visualizing} can be derived from \cref{eq:neighbor_techniques} with a particular choice of $\mC_{X}, \mC_{Z}$. For instance SNE and t-SNE both consider in the input space a symmetrized version of the entropic affinity \citep{vladymyrov2013entropic,van2023snekhorn}. In the embedding space, $\mC_Z(\mZ)$ is usually constructed from a ‘‘kernel'' matrix $\mK_\mZ$ which undergoes a scalar \citep{van2008visualizing}, row-stochastic \citep{hinton2002stochastic} or doubly stochastic \citep{lu2019doubly,van2023snekhorn} normalization. Gaussian kernel $[\mK_\mZ]_{ij} = \exp(-\|\vz_i-\vz_j\|_2^2)$, or heavy-tailed Student-t kernel $[\mK_\mZ]_{ij} = (1+\|\vz_i-\vz_j\|_2^2)^{-1}$, are typical choices \citep{van2008visualizing}. We also emphasize that one can retrieve the UMAP objective \citep{mcinnes2018umap} from \cref{eq:DR_criterion} using the binary cross-entropy loss. As described in \citep{van2022probabilistic} from a probabilistic point of view, all these objectives can be derived from a common Markov random field model with various graph priors.
\begin{remark}
The usual formulations of neighbor embedding methods rely on the loss $L(x,y) = x \log(x/y)$. However, due to the normalization, the total mass $\sum_{ij} [\mC_Z(\mZ)]_{ij}$ is constant (often equal to $1$) in all of the cases mentioned above. Thus the minimization in $\mZ$ with the $L_{\KL}$ formulation is equivalent.
\end{remark}

\paragraph{Hyperbolic geometry.} The presented DR methods can also be extended to incorporate non-Euclidean geometries. Hyperbolic spaces \citep{Chami21, Fan_2022_CVPR, Guo22, Lin23} are of particular interest as they can capture hierarchical structures more effectively than Euclidean spaces and mitigate the curse of dimensionality by producing representations with lower distortion rates. 
For instance, \citet{Guo22} adapted t-SNE by using the Poincaré distance and by changing the Student's t-distribution with a more general hyperbolic Cauchy distribution.  Notions of projection subspaces can also be adapted, \eg \citet{Chami21} use horospheres as one-dimensional subspaces. 
