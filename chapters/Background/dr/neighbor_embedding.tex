\subsection{Neighbor Embedding Methods}\label{sec:neighbor_embedding}

In this section, we focus on the second major family of DR methods : the neighbor embedding (NE) algorithms. This class of DR techniques has gained increasing popularity in domains like cell-biology \citep{kobak2019art} or chemistry \citep{mai2022machine}. Unlike the previously mentioned spectral algorithms, NE methods place greater emphasis on preserving the local structure of the data.

The core idea of these methods is to preserve the neighborhood relationships of each point by relying on soft neighborhood graphs defined in both the input and latent spaces. These soft neighborhood graphs are usually constructed from a \emph{kernel-like} matrix which undergoes a scalar \citep{van2008visualizing}, row-stochastic \citep{hinton2002stochastic} or doubly stochastic \citep{lu2019doubly,van2023snekhorn} normalization. Gaussian kernel $[\mK_\mZ]_{ij} = \exp(-\|\vz_i-\vz_j\|_2^2 / \sigma)$, or heavy-tailed Student-t kernel $[\mK_\mZ]_{ij} = (1+\sigma^{-1}\|\vz_i-\vz_j\|_2^2)^{-1}$, are typical choices \citep{van2008visualizing}, where $\sigma > 0$ is a bandwidth parameter.
Within our framework, these methods correspond to \cref{eq:DR_criterion} with $L = \KL$ or $L=\BCE$.

\begin{remark}
    Note that the bandwidth $\sigma$ is often set to $1$ as it only influences the scale of the obtained embeddings thus has limited practical impact.
\end{remark}

\paragraph{Notations.}
In this section, we denote by $\C_\X = \left(\|\vx_{i}-\vx_{j}\|_2^2\right)_{ij}$ and $\C_\Z = \left(\|\vz_{i}-\vz_{j}\|_2^2\right)_{ij}$ the cost matrices derived from the samples in $\X$ and $\Z$ respectively. Additionally, as commonly done in the NE literature, we denote by $\mP$ and $\mQ$ the affinity matrices constructed respectively from the input data $\mX$ and latent embeddings $\mZ$.

\paragraph{Neighbor Embedding objective.}
NE objectives share a common structure: they aim to minimize the weighted sum of an \emph{attractive term} that pulls the embeddings closer together and a \emph{repulsive term} that pushes them apart and prevents trivial collapsed solutions \citep{van2022probabilistic}. Interestingly, the attractive term is often the cross-entropy between the input and output affinities. Additionally, sometimes for computational reasons, the repulsive term is typically a function of the output affinities only. Thus, the NE problem can be formulated as the following general objective, where the dependance in $\mZ$ is implictly encoded in the output affinity matrix $\mQ$:
\begin{align}\tag{NE}\label{eq:ne_attraction_repulsion}
    \min_{\mZ \in \R^{N \times d}} \: - \sum_{ij} [\mP]_{ij} \log [\mQ]_{ij} + \mathcal{L}_{\mathrm{rep}}(\mQ) \:.
\end{align}

In the above, \(\mathcal{L}_{\mathrm{rep}}(\mQ)\) represents the repulsive part of the loss function that takes different forms depending on the method. We now detail the two most popular NE methods: Stochastic Neighbor Embedding (SNE) and t-Distributed Stochastic Neighbor Embedding (t-SNE) as their principles are at the core of other NE approaches.
% while \(\gamma\) is a hyperparameter that controls the balance between attraction and repulsion.

\subsubsection{The Stochastic Neighbor Embedding Approach}\label{sec:sne}

\paragraph{Stochastic Neighbor Embedding (SNE).}The first neighbor embedding method introduced was Stochastic Neighbor Embedding (SNE) by \cite{hinton2002stochastic}. In SNE, the soft neighborhood graphs affinities give the probability of selecting a neighbor in each point. Hence they can be seen as Markov chain probabilities. The primary objective is to couple the two Markov chains by minimizing the Kullback-Leibler divergence between them. 

In the simplest scenario, the transition matrices of the Gaussian Markov chain can be formed by applying row-wise normalization to the Gaussian kernel in both the input and latent spaces. The transition matrices are defined as
\begin{align}
    \forall (i,j), \quad [\mP^{\mathrm{r}}]_{ij} = \frac{\exp{(-[\mC_{\mX}]_{ij} / \varepsilon)}}{\sum_\ell \exp{(-[\mC_{\mX}]_{i\ell} / \varepsilon)} } \:, \quad [\mQ^{\mathrm{r}}]_{ij} = \frac{\exp{(-[\mC_{\mZ}]_{ij})}}{\sum_\ell \exp{(-[\mC_{\mZ}]_{i\ell})}} \,
\end{align}
where $\varepsilon > 0$ is a bandwidth parameter. The exponent $\mathrm{r}$ in the above notations stands for row-wise normalization.

\begin{remark}
    To emphasize neighborhood relationships, it is common to set the diagonal elements of the affinity matrix to zero by imposing the constraint $[\mP]_{ii} = 0$. This is similar to setting $[\mC_{\mX}]_{ii} = +\infty$. In practice, applying this constraint often enhances the results in dimensionality reduction tasks.
\end{remark}

\paragraph{Entropic Affinities (EAs).}
Crucially, to account for varying noise levels in the data, the authors propose to use an entropic affinity matrix, denoted $\mP^{\mathrm{e}}$ for \emph{entropic}, instead of $\mP^{\mathrm{r}}$. The core principle of EAs presented in \citep{hinton2002stochastic} is to use \emph{adaptive} kernel bandwidths $(\varepsilon^\star_i)_{i \in \integ{n}}$ to better capture detailed structures in the data, as opposed to using constant bandwidths. The benefits of adaptive bandwidths have also been highlighted in other studies, such as \citep{van2018recovering} and \citep{zelnik2004self}. Adaptive bandwidths adjust distances to account for the varying density in different regions of the dataset.

Entropic affinities rely on adaptive bandwidths to control the entropy of each row of the affinity. More precisely, given $\xi \in \integ{n-1}$, the goal of EAs is to build a Gaussian Markov chain transition matrix $\mP^{\mathrm{e}}$ with prescribed entropy as
\begin{equation}
\begin{split}
    \forall i, \: &\forall j, \: P^{\mathrm{e}}_{ij} = \frac{\exp{(-C_{ij}/\varepsilon^\star_i)}}{\sum_\ell \exp{(-C_{i\ell}/\varepsilon^\star_i)}} \\
    &\text{with} \:\: \varepsilon^\star_i \in \mathbb{R}^*_+ \:\: \text{s.t.} \: \operatorname{H}(\mP^{\mathrm{e}}_{i:}) = \log{\xi} + 1\,. \label{eq:entropic_affinity_pb}
\end{split}\tag{EA}
\end{equation}
The hyperparameter $\xi$, which is also known as \emph{perplexity}, can be interpreted as the effective number of neighbors for each data point. Indeed, a perplexity of $\xi$ means that each row of $\mP^{\mathrm{e}}$ (which is a discrete probability since $\mP^{\mathrm{e}}$ is row-wise stochastic) has the same entropy as a uniform distribution over $\xi$ neighbors. Therefore, it provides the practitioner with an interpretable parameter specifying which scale of dependencies the affinity matrix should faithfully capture. In practice, a root-finding algorithm is used to find the bandwidth parameters $(\varepsilon_i^\star)_{i \in \integ{n}}$ that satisfy the constraints \citep{vladymyrov2013entropic}. Hereafter, with a slight abuse of language, we call $e^{\operatorname{H}(\mP_{i:})-1}$ the perplexity of the point $i$.

With this in place, SNE focuses on solving
\begin{align}\label{eq:SNE_pb}\tag{SNE}
    \min_{\mZ \in \R^{n \times d}} \: \mathcal{L}_{\mathrm{SNE}} \coloneqq \operatorname{KL}(\mP^{\mathrm{e}} | \mQ^{\mathrm{r}}) \;,
\end{align}
where $\mP^{\mathrm{e}}$ is the entropic affinity matrix computed from $\mX$ and $\mQ^{\mathrm{r}}$ is the row-normalized Gaussian kernel in the embedding space.

Focusing on non-constant terms, the above objective can be seen as a special instance of \Cref{eq:ne_attraction_repulsion} with the following repulsion
\begin{align}
    \mathcal{L}_{\mathrm{rep}}(\mQ) = \sum_{i} \log \Big(\sum_j [\mQ]_{ij} \Big)
\end{align}
where $\mQ = \exp{(-[\mC_{\mZ}])}$ is the Gaussian kernel matrix in the latent space.

\begin{remark}\label{rem:norm_as_repulsion}
    It's important to note that the normalization of the output affinity matrix is what controls the repulsive forces. If the $\KL$ loss were used with an unnormalized $\mQ$, it would lead to the embeddings collapsing.
\end{remark}

\paragraph{t-Distributed Stochastic Neighbor Embedding (t-SNE).} t-SNE introduced by \citet{van2008visualizing} is a widely used method that builds upon SNE while introducing several improvements to the algorithm. This enhanced version has since become the standard, replacing SNE in practical applications. t-SNE introduces two main changes. 
\begin{itemize}
    \item First, instead of the Gaussian kernel, t-SNE considers a different distribution in the latent space $[\mQ]_{ij} = (1 + [\C_{\Z}]_{ij})^{-1}$. In this formulation, $\mQ$ is a joint Student $t$-distribution that accounts for crowding effects: a relatively small
    distance in a high-dimensional space can be accurately represented by a
    significantly greater distance in the low-dimensional space. 
    \item Second, authors propose symmetric versions of the affinities used in SNE. In input space, t-SNE still relies on the entropic affinity matrix $\mP^{\mathrm{e}}$ and averages it with its transpose to make it symmetric : $\overline{\mP^{\mathrm{e}}} = \frac{1}{2}(\mP^{\mathrm{e}} + \mP^{\mathrm{e} \top})$. Note that the affinity matrix $\overline{\mP^{\mathrm{e}}}$ is the Euclidean (or Frobenius) projection of $\mP^{\mathrm{e}}$ on the space of symmetric matrices $\mathcal{S}$. In latent space, the affinity matrix is normalized by a scalar which is the sum on both axes \ie it defines $[\mQ^{\mathrm{b}}]_{ij} = [\mQ]_{ij} /\sum_{\ell,t}[\mQ]_{\ell t}$.
\end{itemize}

Therefore, the t-SNE problem can be expressed as
\begin{align}\label{eq:t-SNE_pb}\tag{t-SNE}
    \min_{\Z \in \R^{n \times q}} \: \mathcal{L}_{\mathrm{t-SNE}} \coloneqq \operatorname{KL}(\overline{\mP^{\mathrm{e}}} | \mQ^{\mathrm{b}}) \:.
\end{align}

Similarly to SNE, the repulsive term is obtained as the normalization of the output affinity $\mQ$. In this case it reads:
\begin{align}
    \mathcal{L}_{\mathrm{rep}}(\mQ) = \log \Big(\sum_{ij} [\mQ]_{ij} \Big) \:,
\end{align}
where $[\mQ]_{ij} = (1 + [\C_{\Z}]_{ij})^{-1}$ is the student-t kernel in the embedding space.

\begin{remark}{(Hyperbolic geometry)} 
The presented DR methods can also be extended to incorporate non-Euclidean geometries. Hyperbolic spaces \citep{Chami21, Fan_2022_CVPR, Guo22, Lin23} are of particular interest as they can capture hierarchical structures more effectively than Euclidean spaces and mitigate the curse of dimensionality by producing representations with lower distortion rates. 
For instance, \citet{Guo22} adapted t-SNE by using the Poincaré distance and by changing the Student's t-distribution with a more general hyperbolic Cauchy distribution.  Notions of projection subspaces can also be adapted, \eg \citet{Chami21} use horospheres as one-dimensional subspaces. 
\end{remark}

\paragraph{Connection with maximum likelihood estimation.}
The t-SNE optimization problem can be viewed as a form of parametric density estimation. In this interpretation, we aim to approximate a target distribution $p$ with a parametric distribution $q_{\theta}$. In the context of t-SNE, $p$ represents a probability distribution over the edges of the affinity graph that connects the data samples, encoded by the affinity matrix $\mP$, where $[\mP]_{ij}$ denotes the probability of sampling the edge $(i,j)$. The parametric distribution $q_{\theta}$ is defined by the output affinities $\mQ$, with the parameters $\theta$ corresponding to the latent embeddings $\mZ$. The goal is to find the embedding positions $\mZ$ that best approximate the input affinities. This problem can be formulated as the following Maximum Likelihood Estimation (MLE) problem \citep{damrich2022t}:
\begin{align}
    \max_{\theta = \mZ} \mathbb{E}_{ij \sim p} \log \left( q_{\theta}(ij) \right) \:.
\end{align}
As mentioned in \Cref{rem:norm_as_repulsion}, this formulation requires $q_{\theta}$ to be a normalized model to prevent trivial solutions. The normalized model is defined as $q_{\theta}(ij) = \phi(ij) / Z(\theta)$, where $Z(\theta) = \sum_{k \neq l} \phi(kl)$ acts as the partition function. Expanding the expectation yields the following objective:
\begin{align}
    \max_{\theta = \mZ}  - \sum_{ij} p(ij) \log \left( \phi(ij) \right) + \log \left( \sum_{kl} \phi(kl) \right),
\end{align}
which corresponds to the objective in \Cref{eq:t-SNE_pb}, originally formulated as a Kullback-Leibler divergence between the input and output affinities.


\paragraph{On the interplay between NE and self-supervised learning} We have seen in \Cref{chap:intro} that both dimensionality reduction (DR) and self-supervised learning are foundational to modern machine learning pipelines. Interestingly, methods from these two fields share key underlying principles. A notable work by \citet{wang2020understanding} offers a unified view of contrastive representation learning, demonstrating that these methods seek to optimize both an alignment loss and a uniformity loss. The alignment loss drives similar points to be positioned close to each other in the latent space, while the uniformity term ensures the embedding space is evenly populated, helping to retain as much information as possible. This mirrors the attraction and repulsion objectives in \Cref{eq:ne_attraction_repulsion}.

Additionally, as highlighted by \citet{hu2022your}, the popular contrastive learning method SimCLR \citep{chen2020simple} can be seen as a specific case of \Cref{eq:SNE_pb}. Both approaches are based on the widely used InfoNCE criterion \citep{oord2018representation}, which is a special case of the Kullback-Leibler divergence. In SimCLR, the input affinity matrix is a binary matrix encoding positive pairs as follows:
\begin{align}
[\tilde{\mP}]_{ij} = 
\begin{cases}
1 & \text{if } \tilde{\vx}_i \text{ and } \tilde{\vx}_j \text{ are positive pairs,} \\
0 & \text{otherwise.}
\end{cases}
\end{align}
This further supports the argument made in \Cref{chap:intro} that DR and self-supervised learning methods share common underlying principles. Along similar lines, \citet{balestriero2022contrastive} connects a broad range of popular self-supervised learning methods to the spectral embedding techniques discussed in \Cref{sec:spectral_methods}.

There are several important distinctions between the two families of methods. First, SSL methods are inherently parametric, as they rely on neural networks, whereas NE methods are non-parametric. Second, SSL methods are aimed at learning representations that are useful for downstream tasks, typically in a space of dimensionality $d=128$ or higher, while NE methods mostly focus on visualization with $d=2$. Lastly, SSL methods require various techniques to stabilize the training process \citep{balestriero2023cookbookselfsupervisedlearning}, whereas NE methods are comparatively very simple and fast to train.

Still, these insightful connections raise an intriguing question for expanding the scope of NE methods: can the principles of self-supervised learning (SSL) be applied to generate 2D embeddings for complex computer vision datasets where a meaningful ground metric is unavailable? In \citep{bohm2022unsupervised}, the authors explored this by attempting to directly project a SimCLR model into a 2D space using a student kernel, essentially adapting SimCLR to fit within the t-SNE framework. Their findings revealed that this approach is challenging to execute in a single step and performs more effectively when t-SNE is applied on top of a separately pre-trained SimCLR model.

\subsubsection{Neighbor embeddings in practice}

We now turn our attention to the practical aspects of the aforementioned NE methods. We introduce techniques to tackle key challenges commonly associated with these methods, namely: the quadratic memory and computational costs required for evaluating the objective function described in \Cref{eq:ne_attraction_repulsion}, the non-convexity of the objective function that often results in poor local minima when using standard gradient-based algorithms, and the loss of global structure in the embeddings due to an emphasis on local neighborhoods.

\paragraph{a) Handling the quadratic memory and computational cost.}
A straightforward computation of the objective in \Cref{eq:ne_attraction_repulsion} requires $\mathcal{O}(N^2)$ memory and computational complexity. We will now explore strategies to mitigate this burden by addressing the attraction and repulsion terms separately.

Let's first focus on the attraction term, \(\sum_{ij} -[\mathbf{P}]_{ij} \log [\mathbf{Q}]_{ij}\). In SNE and t-SNE, this term relies on entropic affinities \ie \(\mathbf{P} = \mathbf{P}^{\mathrm{e}}\), which, as discussed in the entropic affinities section, emphasizes a small number of effective neighbors determined by the perplexity parameter \(\xi\). Typically, \(\xi\) is set between 30 and 100, ensuring the preservation of local structures. In practice, it is often assumed that for any point \(i\), the affinity values \([\mathbf{P}^{\mathrm{e}}]_{ij}\) for points \(j\) beyond the \(3 \times \xi\)-th neighbor of \(i\) are negligible, contributing minimally to the objective function. Consequently, a common preprocessing step involves reducing the affinity matrix \(\mathbf{P}\) to a sparse \(N \times k\) matrix, where \(k = 3 \times \xi\), by retaining only the \(k\) nearest neighbors for each point. This can be easily done using for instance the KeOps library \citep{charlier2021kernel}. Such a preprocessing step significantly reduces both memory and computational costs, enabling the computation of the term \(\sum_{ij} -[\mathbf{P}]_{ij} \log [\mathbf{Q}]_{ij}\) in \(\mathcal{O}(Nk)\) time.


Second, we turn our attention to the repulsion term $\mathcal{L}_{\mathrm{rep}}(\mQ)$. Focusing specifically on t-SNE we have $\mathcal{L}_{\mathrm{rep}}(\mQ) = \log \Big(\sum_{ij} [\mQ]_{ij} \Big)$. Computing the sum over all pairs again induces a $\mathcal{O}(N^2)$ complexity. First methods to counter this issue by \cite{van2014accelerating} manages to reduce the complexity to $\mathcal{O}(N \log N)$ by approximating the repulsive term with a Barnes-Hut tree. This approximation reduces the complexity to $\mathcal{O}(N \log N)$ by approximating the repulsive term with a Barnes-Hut tree. The repulsive term is computed by recursively dividing the space into quadrants and approximating the contribution of distant points. One clear limitation of the latter is that it is bound to low dimensional embedding spaces as partitionment of the space scales poorly with the dimensionality. To overcome this limitation and provide more general approximation methods, another approach is to use Noise Contrastive Estimation \citep{gutmann2010noise} that builds upon the previously introduced characterization of t-SNE as maximum likelihood estimation.

\begin{mem1}{Noise contrastive estimation}\label{mem:NCE}
    We focus on a popular technique to bypass the heavy direct computation of the partition function: Noise Contrastive Estimation (NCE) \citep{gutmann2010noise}. The approach proposed in NCE is to estimate the partition function of a model by contrasting the data distribution with a noise distribution.
    Concretely, it turns the unsupervised problem of density estimation into a supervised problem in which the data samples need to be identified from a set containing the $N$ data samples and $m$ times as many noise samples $t_1, \dots, t_{mN}$ drawn from a noise distribution $\mathcal{U}$. It is often the uniform distribution over the samples. Briefly, NCE fits $\theta$ by minimizing the following binary cross-entropy criterion:
    \begin{align}
    \theta^* = \arg\min_{\theta} \left[ - \sum_{i=1}^{N} \log \left( \frac{q_{\theta}(s_i)}{q_{\theta}(s_i) + m\xi(s_i)} \right) - \sum_{i=1}^{mN} \log \left( 1 - \frac{q_{\theta}(t_i)}{q_{\theta}(t_i) + m\xi(t_i)} \right) \right] \, . \tag{2}
    \end{align}
    
    The key advantage of NCE lies in the following result which shows its theoretical ability to recover the data distribution.
    
    \begin{theorem}
    Let $\xi$ have full support and suppose there exists some $\theta^*$ such that $q_{\theta^*} = p$. Then $\theta^*$ is a minimum of
    \begin{equation}
    \mathcal{L}_{\text{NCE}}(\theta) = - \mathbb{E}_{s \sim p} \log \left( \frac{q_{\theta}(s)}{q_{\theta}(s) + m\xi(s)} \right) - m \mathbb{E}_{t \sim \xi} \log \left( 1 - \frac{q_{\theta}(t)}{q_{\theta}(t) + m\xi(t)} \right) \, . \tag{3}
    \end{equation}

    and the only other extrema of $\mathcal{L}_{\text{NCE}}$ are minima $\hat{\theta}$ which also satisfy $q_{\hat{\theta}} = p$.
    \end{theorem}

\end{mem1}

Applying the NCE technique to the t-SNE repulsion term $\mathcal{L}_{\mathrm{rep}}(\mQ)$ has led to the development of several methods that have proven effective in practice. For what follwos we introduce the matrix $\widetilde{\mQ}$ such that $[\widetilde{\mQ}]_{ij} = [\mQ]_{ij} / ([\mQ]_{ij} + 1)$. The first such approach was LargeVis \citep{tang2016visualizing}, which simplifies to the following objective to minimize over the embeddings $\mZ$:
\begin{align}\label{eq:LargeVis_pb}\tag{LargeVis}
    - \sum_{ij} [\mP]_{ij} \log [\widetilde{\mQ}]_{ij} + \sum_{i \in N(i)} \log (1 - [\widetilde{\mQ}]_{ij})
\end{align}
where $N(i)$ denotes the set of negative samples for point $i$, randomly drawn from the dataset. UMAP \citep{mcinnes2018umap}, a closely related algorithm, has since gained more popularity among practitioners than LargeVis. While LargeVis uses the same affinity matrix as t-SNE for both input, $ \mP = \overline{\mP^{\mathrm{e}}}$, and output, $ \mQ = \mQ^{\mathrm{b}}$ with a Student-t kernel, UMAP introduces customized affinity matrices inspired by Riemannian geometry and algebraic topology. Apart from these differences in affinity matrices, which have been shown to have limited impact in practice \citep{bohm2020unifying}, the design of UMAP and LargeVis is nearly identical with the use of the binary cross entropy as core objective. We refer to \citep{damrich2022t} for a comprehensive review of NCE-based neighbor embedding methods.


\paragraph{b) Escaping local minima with early exaggeration.} Unlike spectral methods (\Cref{sec:spectral_methods}), neighborhood embedding (NE) techniques do not have closed-form solutions and are solved using gradient-based approaches such as stochastic gradient descent or Adam \citep{kingma2014adam}. To better understand why the NE optimization problem, shown in \Cref{eq:ne_attraction_repulsion}, is particularly prone to local minima, let's consider the case of t-SNE, where the gradients are given by:
\begin{align}
    \frac{\partial \mathcal{L}_{\mathrm{t-SNE}}}{\partial \vz_i} = 4 \sum_j \left( [\mP^{\mathrm{e}}]_{ij} - [\mQ^{\mathrm{b}}]_{ij} \right) \left(1 + [\C_{\mZ}]_{ij}\right)^{-1} \left( \vz_i - \vz_j \right) \:.
\end{align}
The above gradient can be interpreted as the local forces exerted by a set of springs between the map point $\vz_i$ and all other map points $\vz_j$. Each spring exerts a force in the direction of $(\vz_i - \vz_j)$, either repelling or attracting the points, depending on whether the distance between them is too small or too large to represent the similarities between the corresponding high-dimensional data samples. Although we only gave the gradient for t-SNE, note that the same interpretation applies to other NE methods.

This multi-particle system of local forces is highly non-convex, meaning it can easily become trapped in poor local minima during optimization. In low-dimensional embeddings, depending on the initialization of the embeddings, neighboring points in the high-dimensional space can end up separated by non-neighbor points. These non-neighbor points create zones of high repulsion, making it impossible for the true neighbors to reconnect. This phenomenon highlights the challenges of preserving neighborhood relationships in dimensionality reduction. \todo{figure from torchdr}

To mitigate this issue, \citet{van2008visualizing} introduced a technique known as early exaggeration. This approach involves artificially increasing the attractive term in \Cref{eq:ne_attraction_repulsion} by scaling it with a factor $\eta > 1$ during the initial stages of optimization. This creates a "tunneling" effect, allowing particles to bypass regions of high repulsion and connect with their true neighbors. Early exaggeration has proven to be effective in practice and is now a standard component of many NE methods.


\paragraph{c) Improve the coarse-grained structure.} As mentioned earlier, neighborhood embedding (NE) methods are specifically designed to accurately capture the local structure of the data. However, this comes at the cost of losing the global structure, which NE methods often fail to represent accurately. As a result, the relative positions of the displayed clusters should be interpreted with caution, as they may not carry meaningful information \citep{kobak2019art}. The scale of dependencies captured by the embeddings is controlled by the \texttt{perplexity} parameter for entropic affinities (\Cref{sec:sne}) (or the \texttt{n\_neighbors} parameter in UMAP \citep{mcinnes2018umap}). However, simply increasing the perplexity to a high value is not a panacea, as it can lead to poor embeddings in terms of class separation and disentanglement, meaning the benefits of NE methods are diminished. To address this issue, the community has proposed two main strategies. First, using multiscale similarity as described in \citep{lee2015multi}, which involves averaging affinities across multiple scales to capture both local and global structures. While this approach aims to balance the advantages of different scales, it can be difficult to calibrate. The second, and more popular approach, is to ensure a good global structure at initialization, for example, by using a PCA embedding \citep{wattenberg2016use}. This informative initialization helps improve the relative positioning of the final clusters.
\todo{figure from torchdr}

\begin{remark}
    This section discusses the most relevant concepts that have emerged from the neighborhood embedding (NE) literature. However, it is not exhaustive, and many recently proposed approaches are omitted for the sake of brevity. For instance, TriMap \citep{amid2019trimap} introduces triplets consisting of three points $ (i, j, k) $, where point $ i $ is closer to point $ j $ than to point $ k $, extending the principles of t-SNE to 3D tensor affinities. Another noteworthy method is PaCMAP \citep{wang2021understanding}, which adds a third term to \Cref{eq:ne_attraction_repulsion}. This additional term aims to better balance near and far relationships by managing "mid-near" pairs. These mid-near pairs are obtained, from each point, by selecting its second closest point from a set of uniformly selected negative samples.
\end{remark}
