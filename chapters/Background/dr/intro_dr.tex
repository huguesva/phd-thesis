\newpage

This chapter reviews the existing literature on DR methods. In \Cref{sec:background_dr}, we give a general formulation of DR in terms of pairwise similarity matrices. 

Then, we revisit DR approaches through two different perspectives:
latent variable models in \Cref{sec:dr_proba_modelling} and optimal transport in \Cref{sec:dist_perspective_dr}. These perspectives open the door to a wide range of future extensions by allowing for the adaptation of underlying assumptions. However they are currently limited to a narrow class of DR methods.


\paragraph{Dimensionality reduction context.}
Throughout, we consider an input dataset with $N$ vectorial data points with dimensionality $p$ \ie $\mX = (\vx_1, ..., \vx_N) ^\top$ where for all i, $\vx_i \in \mathcal{X} \subset \R^{p}$. Dimensionality reduction focuses on constructing a low-dimensional representation or \emph{embedding} $\mZ = (\vz_1, ..., \vz_N)^\top \in \R^{N \times d}$, where $d< p$.

\section{Overview of Dimensionality Reduction Methods}\label{sec:background_dr}

In this section we review the most popular DR methods. These algorithm construct a low-dimensional representation of the data that best preserve a notion of geometric structure among the input samples. 

This structure is generally encoded via a symmetric pairwise similarity matrix obtained from the input data $\mX$. Throughout, we call \emph{affinity} the weight matrix of a graph that encodes this similarity. The higher the weight in position $(i,j)$, the
higher the similarity or proximity between samples $i$ and $j$. Simple usual affinities include the inner product $\left( \langle \vx_i, \vx_j \rangle \right)_{ij}$ or the Gaussian kernel $\left( \exp(-\|\vx_i - \vx_j\|_2^2) \right)_{ij}$.


% Seminal approaches relying on affinities include Laplacian
% eigenmaps \citep{belkin2003laplacian}, spectral clustering
% \citep{von2007tutorial} and semi-supervised learning \citep{zhou2003learning}. Numerous methods can be employed to construct such affinities.


Most popular DR methods optimize $\mZ$ such that a certain pairwise similarity matrix in the output space matches $\mC_{\mX}$ according to some criteria. We subsequently introduce the functions
\begin{equation}
\label{eq:sim_function}
\simiX: \R^{N \times p} \to \R^{N \times N}, \simi: \R^{N \times d} \to \R^{N \times N}\,,
\end{equation}
which define pairwise similarity matrices in the input and output space from embeddings $\mX$ and $\mZ$.


Therefore they essentially revolve around two key steps:

\begin{enumerate}
	\item Capturing the data geometry : Defining a notion of similarity between samples in the input space $\mathcal{X}$.
	\item Constructing a low-dimensional representation that . It ensures that the embeddings in the representation space (called \emph{latent space}) 
\end{enumerate}


 
\paragraph{DR general objective.} With this in place, the DR problem can be formulated quite generally as the optimization problem
%  minimizing over $\mZ$ the objective function
\begin{equation}
\label{eq:DR_criterion}\tag{DR}
\min_{\mZ \in \R^{N \times d}} \: \sum_{(i,j) \in \integ{N}^2}  L\big([\simiX(\mX)]_{ij}, [\simi(\mZ)]_{ij}\big) \,. %+ R(\mZ)\,.
\end{equation}
where $L:\R \times \R \rightarrow \R$ is a loss that quantifies how similar are two points in the input space $\R^{p}$ compared to two points in the output space $\R^{d}$. Various losses can be used, such as the quadratic loss $L_2(x,y) \coloneqq (x - y)^2$
%  the cross-entropy $L_{\CE}(x,y) \coloneqq - x \log(y)$  
or the Kullback-Leibler divergence $L_{\KL}(x,y) \coloneqq x \log (x/y) - x +y$.
In what follows, we recall several popular methods that can be placed within this framework.
%$R: \R^{N \times d} \to \R$ is a regularizer term on the embeddings $\mZ$.  



