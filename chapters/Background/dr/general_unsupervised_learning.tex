\section{Affinity-Based Unsupervised Learning}\label{sec:affinity_unsupervised_learning}

%\paragraph{Unsupervised learning.}
One major objective of unsupervised learning~\cite{Hastie2009} is to provide interpretable and meaningful approximate representations of the data that best preserve its structure \ie the underlying geometric relationships between the data samples.
Similar in essence to Occam's principle frequently employed in supervised learning, the preference for unsupervised data representation often aligns with the pursuit of simplicity, interpretability or visualizability in the associated model.
These aspects are determinant in many real-world applications, such as cell biology
\cite{cantini2021benchmarking, ventre2023one}, where the interaction with domain experts is paramount for interpreting the results and extracting meaningful insights from the model. 
% \nc{or to assess the separability of data embeddings within deep neural layers.}

\paragraph{Dimensionality reduction and clustering.}
When faced with the question of extracting
interpretable representations, from a dataset $\mX = (\vx_1, ..., \vx_N) ^\top
\in \R^{N \times p}$ of $N$ samples in $\R^p$, the machine learning community
has proposed a variety of methods. Among them, dimensionality reduction (DR) algorithms have been widely used to summarize data in a low-dimensional space
$\mZ = (\vz_1, ..., \vz_N) ^\top \in \R^{N \times d}$ with $d \ll p$, allowing
for visualization of every individual points for small enough $d$ \cite{agrawal2021minimum,van2009dimensionality}.
Another major approach is to cluster the data into $n$ groups, with $n$
typically much smaller than $N$, and to summarize these groups through their centroids \cite{saxena2017review,ezugwu2022comprehensive}.
%by clusteres defined as  centroids or groups of samples. 
Clustering is particularly interpretable since it provides a
smaller number of points that can be easily inspected. %visualized or interpreted. 
The cluster assignments can also be analyzed. %but also assignment of each original point to a cluster. 
Both DR and clustering follow a
similar philosophy of summarization and reduction of the dataset using a smaller size representation.
% matrix. 

\paragraph{Two sides of the same coin.} As a matter of fact, methods from both families share many similitudes, %among which is
including the construction of a similarity graph between input samples. In clustering, many popular approaches design a reduced or coarsened version of the initial similarity graph while preserving some of its spectral properties~\cite{von2007tutorial, schaeffer2007graph}. 
%attributes related to the graph spectrum 
In DR, the goal is to solve the inverse problem of finding low-dimensional embeddings that generate a similarity graph close to the %initial
one computed from input data points \cite{ham2004kernel,hinton2002stochastic}.
%With so many similarities
Our work builds on these converging viewpoints and addresses the following question: \emph{can DR and clustering  be expressed in a common and unified framework ?}