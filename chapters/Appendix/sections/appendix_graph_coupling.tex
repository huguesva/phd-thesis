% Appendix Template

\section{Appendix of \Cref{chapter:GraphCoupling}}


\subsection{Proof of \Cref{PCA_graph_coupling}}

\PCAgraphcoupling*

\begin{proof}
We consider the following hierarchical model, for $\nu_{X}, \nu_{Z} \geq n$:
\begin{align*}
    \bm{\Theta}_{X} &\sim  \mathcal{W}(\nu_{X}, \bm{I}_n) \\
    \mathrm{vec}(\X) | \bm{\Theta}_{X} &\sim \mathcal{N}(\bm{0}, \bm{\Theta}_{X}^{-1} \otimes \bm{I}_p) \\
    \bm{\Theta}_{Z} &\sim  \mathcal{W}(\nu_{Z}, \bm{I}_n) \\
    \mathrm{vec}(\Z) | \bm{\Theta}_{Z} &\sim \mathcal{N}(\bm{0}, \bm{\Theta}_{Z}^{-1} \otimes \bm{I}_q) \:.
\end{align*}
With this at hand, the posteriors for $\bm{\Theta}_X$ and $\bm{\Theta}_Z$ can be derived in closed form: 
\begin{align*}
    \bm{\Theta}_{X} | \X &\sim  \mathcal{W}(\nu_{X}+p, \left(\bm{I}_n + \X \X^\top\right)^{-1}) \\
    \bm{\Theta}_{Z} | \Z &\sim  \mathcal{W}(\nu_{Z} + q, \left(\bm{I}_n + \Z \Z^\top\right)^{-1}) \:.
\end{align*}

Keeping terms of $-\mathbb{E}_{\bm{\Theta}_{X}}[\log \mathbb{P}(\bm{\Theta}_{Z} = \bm{\Theta}_{X}| \Z )| \X]$ that depends on $\Z$, one has the optimization problem:
\begin{align*}
    \min_{\Z \in \mathbb{R}^{n \times q}} \quad \frac{\nu_{X}+p}{2}\operatorname{tr}\left(\Z^\top(\bm{I}_n +  \X\X^\top)^{-1}\Z\right) - \frac{\nu_{Z}+q}{2}\log |\bm{I}_n +  \Z\Z^\top|
\end{align*}
Consider the eigendecomposition of the sample covariance matrices: $\X\X^\top = \bm{V D} \bm{V}^\top$ and $\Z\Z^\top = \bm{U \Lambda} \bm{U}^\top$ where $\bm{D}=\operatorname{diag}(\bm{d})$ and $\bm{\Lambda}=\operatorname{diag}(\bm{\lambda})$ such that $d_1 \geq ... \geq d_n$ and $\lambda_1 \geq ... \geq \lambda_n$. Denoting $\gamma = (\nu_{X}+q)/(\nu_{Z}+p)$, we consider the following problem:
\begin{align}
   \min_{\bm{U} \in \mathcal{O}(n), \bm{\Lambda}} \quad & \operatorname{tr}\left(\bm{U} \bm{\Lambda} \bm{U}^\top \bm{V} (\bm{I}_n + \bm{D})^{-1} \bm{V}^\top\right) - \gamma \log |\bm{I}_n + \bm{\Lambda}| \label{eq:optim_eigenvalues_eigenvectors} \\
    \textrm{s.t.} \quad & \bm{\Lambda} \succcurlyeq \bm{0} \label{eq:positive_definite_constraint}\\
    & \operatorname{rank}(\bm{\Lambda}) \leq q \label{eq:rank_constraint}
\end{align}
Note that the above problem is non-convex because of the rank constraint (\ref{eq:rank_constraint}). 

First, we focus on finding the optimal eigenvectors. To that extent, let us denote, $\bm{R} = \bm{U}^\top\bm{V}$. Only the left term in (\ref{eq:optim_eigenvalues_eigenvectors}) depends on $\bm{R}$. The optimization problem for eigenvectors writes:
\begin{align}
   \min_{\bm{R} \in \mathcal{O}(n)} \quad & \operatorname{tr}\left(\bm{R}^\top \bm{\Lambda} \bm{R} (\bm{I}_n + \bm{D})^{-1} \right) \label{eq:optim_eigenvalues_eigenvectors}
\end{align}
The objective (\ref{eq:optim_eigenvalues_eigenvectors}) can be expressed as: $\sum_{(i,j) \in \integ{n}^2} \lambda_i (1 + d_j)^{-1} R_{ij}^2$. Now one can notice that since $\bm{R}$ is orthogonal, $\bm{R} \odot \bm{R}$ is doubly stochastic (\textit{i.e.}\ sum of coefficients on each row and column is equal to one). Therefore thanks to the Birkhoffâ€“von Neumann theorem, there exists $\theta_1, ..., \theta_L \geq 0$, $\sum_{\ell \in \integ{L}} \theta_\ell = 1$ and permutation matrices $\bm{P}_1, ..., \bm{P}_L$ such that:
$$\bm{R} \odot \bm{R} = \sum_{\ell \in \integ{L}} \theta_\ell \bm{P}_\ell$$
where for all $\ell \in \integ{L}$, there exists a permutation $\sigma_\ell$ of $\integ{n}$ such that $P_{\ell,ij} = \ind_{\sigma_{\ell}(i) = j}$ for $(i,j) \in \integ{n}^2$. 

With this at hand, objective (\ref{eq:optim_eigenvalues_eigenvectors}) writes: $\sum_{\ell \in \integ{L}} \theta_\ell \sum_{i \in \integ{n}} \lambda_i (1 + d_{\sigma_\ell(i)})^{-1}$. There exists a permutation $\sigma^\star$ such that the quantity $\sum_{i \in \integ{n}} \lambda_i (1 + d_{\sigma_\ell(i)})^{-1}$ is minimal. Note that the identity permutation \textit{i.e.}\ for $i \in \integ{n}$, $ \sigma(i) = i$ is optimal in this case as the $(\lambda_i)_{i \in \integ{n}}$ and the $(d_i)_{i \in \integ{n}}$ are in decreasing order. Then choosing for $\ell \in \integ{L}$, $\theta_\ell = \ind_{\sigma_\ell = \sigma^\star}$ minimizes the latter quantity. Therefore the solution of (\ref{eq:optim_eigenvalues_eigenvectors}) $\bm{R}^{\star}$ is such that for $(i,j) \in \integ{n}^2$, $R^\star_{ij} = \pm \ind_{\sigma^\star(i)=j}$. Thus an optimum in $\bm{U}$ of $\ref{eq:optim_eigenvalues_eigenvectors}$ is such that $\bm{U}^\star = \bm{V} \bm{R}^\star$. 

Hence $\bm{U} = \bm{V}$, in particular, is optimal. We will choose this $\bm{U}$ in what follows as the sign of the axes do not influence the characterization of the final result in $\Z$ as a PCA embedding. Such a choice gives $\Z \Z^\top = \bm{V} \bm{\Lambda} \bm{V}^\top$. 

Now it remains to find the optimal eigenvalues $(\lambda_i)_{i \in \integ{n}}$. The rank constraint (\ref{eq:rank_constraint}) can be easily dealt with: since the eigenvalues are sorted in decreasing order, the constraint implies that for $i \geq q$, $\lambda_i=0$.  Thus the eigenvalue problem can be formulated in $\mathbb{R}^q$:
\begin{align}
    \min_{\bm{\lambda} \in \mathbb{R}^q} \quad & \bm{\lambda}^\top (\bm{1} + \bm{d})^{-1} - \gamma \bm{1}^\top \log (\bm{1} + \bm{\lambda}) \label{eq:objective_lambda}\\
    \textrm{s.t.} \quad & \forall i \in [q], \quad  \lambda_i \geq 0 , \quad \lambda_1 \geq ... \geq \lambda_q \label{eq:feasibility_lambda}
\end{align}
where (\ref{eq:feasibility_lambda}) accounts for (\ref{eq:positive_definite_constraint}). The above is convex. (\ref{eq:objective_lambda}) is minimized for $\bm{\lambda} = \gamma (\bm{1} + \bm{d}) - \bm{1}$. Taking the feasibility constraint (\ref{eq:feasibility_lambda}) into account one has a solution $\bm{\lambda}^*$ such that:
$$\forall i \in \integ{n}, \quad 
\lambda_i^* = \left\{
    \begin{array}{ll}
        \max(0, \gamma(1 + d_i) - 1) \quad &\mbox{if} \quad i \leq q \\
        0 \quad &\mbox{otherwise} \:.
    \end{array}
\right. $$

Note that this solution is not unique if there are repeated eigenvalues. Notice also that one has the freedom to choose the Wishart prior parameters such that $\gamma=1$. Doing so, the solution satisfies $\Z^\star \Z^{\star \:T} = \bm{V}_{[:,q]} \bm{D}_{[q,q]} \bm{V}^\top_{[q,:]}$. Therefore there exists $\bm{R}$ an orthogonal matrix of size $q$ such that $\Z^{\star} = \bm{V}_{[:,q]}\bm{D}_{[q,q]}^{\frac{1}{2}}\bm{R}$. The latter is the output of a PCA model of $\X$ with $q$ components, which is defined up to a rotation.
\end{proof}

\subsection{Proof of \Cref{prop:integrability_pairwise_MRF}} \label{proof:lambda_perp_integrability}

\integrabilitypairwiseMRF*

\begin{proof}
$\W \in \mathcal{S}_W$ is the weight matrix of a graph with $R$ connected components $\{C_1, ..., C_R\}$ partitioning $\integ{n}$. Since $k$ is upper bounded by a constant, there exists $M_+ > 1$ that upper bounds $k$. Let $\bm{\mathcal{T}}$ be the adjacency matrix of a spanning forest of $\W$, since each edge of $\W$ is bounded by $n$, one has:
\begin{align}
    \int f_{k}(\X, \W) \lambda_{\mathcal{S}_{C}}(d\X) &= \int \prod_{(i,j) \in \integ{n}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{W_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) \nonumber \\
    &\leq M_+^{n^{3}} \int \prod_{(i,j) \in \integ{n}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{\mathcal{T}_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) \nonumber \\
    &\leq M_+^{n^{3}} \prod_{r \in \integ{R}} \int \prod_{(i,j) \in C_{r}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{\mathcal{T}_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) \:. \label{bound_MRF_product_CC}
\end{align}
Let $r \in \integ{R}$. The spanning tree corresponding to the $r^{th}$ connected component called $\bm{\mathcal{T}}^r$ has exactly $n_r-1$ edges. There exists a leaf node $\ell \in \integ{n}$ of $\bm{\mathcal{T}}^r$ and let $\tilde{\ell}$ be the node linked to it. Consider a bijective map $\sigma \colon C_r \backslash \{\ell\} \to \integ{n_r - 1}$ such that $\sigma(\tilde{\ell}) = 1$ and for $(i,j) \in (C_r \backslash \{\ell\})^2$, $\sigma(i) \leq \sigma(j)$ implies that node $i$ has a shorter path on $\overline{\bm{\mathcal{T}}^r}$\footnote{Symmetrized version \textit{i.e.}\ $\overline{\bm{\mathcal{T}}^r} = \bm{\mathcal{T}}^r + \bm{(\mathcal{T}}^r)^\top$.} to $\ell$ than node $j$. There exists a bijective map $e \colon \integ{2:n_r - 1} \to \integ{n_r - 2}$ such that for $i \in \integ{2:n_R-1}$, $\overline{\bm{\mathcal{T}}^r}_{\sigma^{-1}(i), \sigma^{-1}(e(i))} > 0$ and node $\sigma^{-1}(e(i))$ has a shorter path on $\overline{\bm{\mathcal{T}}^r}$ to node $\ell$ than node $\sigma^{-1}(i)$.

Recall that since $\X \in \mathcal{S}_{C}$ one has: $\sum_{i \in C_r} \mathbf{x}_i = 0$ hence $\mathbf{x}_{\ell} = - \sum_{i \neq \ell} \mathbf{x}_i$. Let us now consider the linear map $\phi^r$ such that:
\begin{align*}
\forall i \in [n_r - 1], \quad \phi^r(\mathbf{x}_i) = \left\{
    \begin{array}{ll}
        \mathbf{x}_{\sigma^{-1}(i)} + \sum_{j \in \integ{n_r - 1}} \mathbf{x}_{\sigma^{-1}(j)} & \mbox{if i = 1}\\
        \mathbf{x}_{\sigma^{-1}(i)} - \mathbf{x}_{\sigma^{-1}(e(i))} & \mbox{otherwise} \:.
    \end{array}
\right.
\end{align*}

We now show that the change of variable $\phi^r$ is a $\mathcal{C}^1$ diffeomorphism by proving that its Jacobian has full rank. Ordering the columns with the map $\sigma$, the latter takes the form:
\[
    \mathbf{J}_{\phi^r} = \left(
    \begin{array}{ccccc}
    2 & 1 & 1 & \dots & 1 \\
      & 1 & 0 & \dots & 0 \\
      &   & \ddots &  \ddots & \vdots \\
      & \Ab &   & \ddots & 0 \\
      &               &   &   & 1
    \end{array}
    \right)
\]
where $\Ab$ is a strictly lower triangular matrix such that for all $i \in \integ{2:n_r-1}$, $A_{ie(i)} = -1$ and for all $t \neq e(i)$, $A_{it}=0$. The above can be factorized as:
\[
\mathbf{J}_{\phi^r} = 
\left(
    \begin{array}{ccccc}
    \alpha_{n_r-1} & \alpha_{n_r-2} & \dots & \alpha_2 & \alpha_1 \\
    0  & 1 & 0 & \dots & 0 \\
    \vdots & \ddots & \ddots & \ddots & \vdots \\
    \vdots & & \ddots & \ddots & 0 \\
    0 & \dots & \dots & 0 & 1
    \end{array}
    \right)^{-1}
\left(
    \begin{array}{ccccc}
    1 & 0 & \dots & \dots & 0 \\
      & 1 & \ddots & & \vdots\\
      & & \ddots & \ddots & \vdots \\
      & \Ab & & \ddots & 0 \\
      & & & & 1
    \end{array}
\right)
\]
where $\alpha_{1}=-1$ and for $\ell > 1$, $\alpha_\ell = \sum_{j < l} \alpha_j\ind_{e(n_r - j)=n_r -\ell} - 1$. With this in place, for $i \in \integ{n_r -1}$, $\alpha_i \neq 0$ in particular $\alpha_{n_r-1} \neq 0$ therefore $|\mathbf{J}_{\phi^r}| \neq 0 $ and $\phi^r$ is a $\mathcal{C}^1$ diffeomorphism. This change of variable yields:
\begin{align*}
\int \prod_{(i,j) \in C_{r}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{\mathcal{T}_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) 
&= \int \bigotimes_{i \in \integ{n_r - 1}} k(\mathbf{y}_i) |\mathbf{J}_{\phi^r}(\mathbf{Y})|^{-1} \lambda_{\mathbb{R}^p}(d\mathbf{Y}) \\
&= |\mathbf{J}_{\phi^r}|^{-1} \prod_{i \in \integ{n_r - 1}} \int k(\mathbf{y}_i) \lambda_{\mathbb{R}^p}(d\mathbf{y}_i)
\end{align*}
using the Fubini Tonelli theorem. The result follows from $\lambda_{\mathbb{R}^p}$-integrability of $k$ and upper bound \ref{bound_MRF_product_CC}.
\end{proof}


\subsection{Proof of \Cref{prop:posterior_W}}
\label{proof:posterior_limit}

\posteriorW

\begin{proof}
Let $\mathcal{P} \in \{B, D, E\}$, $k$ be a valid kernel (assumptions of \cref{prop:integrability_pairwise_MRF}) with $\K_{X} = (k(\mathbf{x}_{i} - \mathbf{x}_{j}))_{(i,j) \in \integ{n}^2}$ and $\bm{\pi} \in \mathbb{R}_+^{n \times n}$. Let $\W \sim \mathbb{P}_{\mathcal{P},k}^{\varepsilon}(\cdot \: ; \bm{\pi},1)$. Inversion of conditional with Bayes rule gives:
\begin{align}
    \forall \bm{W} \in \mathcal{S}_{W}, \quad \mathbb{P}(\W | \bm{X}) \propto
    \mathcal{C}_{k}^{\varepsilon}(\W)^{-1} f^{\varepsilon}(\X, \W) f_{k}(\X, \W) \mathbb{P}^{\varepsilon}_{\mathcal{P},k}(\W; \bm{\pi}, 1) \label{inversion_Conditional}
\end{align}
where the prior reads:
\begin{align}
    \mathbb{P}_{\mathcal{P},k}^{\varepsilon}(\bm{W}; \bm{\pi}, 1) \propto \mathcal{C}^{\varepsilon}_k(\W) \Omega_{\mathcal{P}}(\W) \prod_{(i,j) \in \integ{n}^2} \pi_{ij}^{W_{ij}} \:.
\end{align}
Hence the joint normalizing constant simplifies such that:
\begin{align}
    \forall \bm{W} \in \mathcal{S}_{W}, \quad \mathbb{P}(\W | \bm{X}) &\propto
    f^{\varepsilon}(\X, \W) \Omega_{\mathcal{P}}(\W) \prod_{(i,j) \in \integ{n}^2} \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \\
    &\xrightarrow[\varepsilon \to 0]{} \Omega_{\mathcal{P}}(\W) \prod_{(i,j) \in \integ{n}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}}
\end{align}
which ends the proof. As a complement, we now explicit the simple forms taken by the posterior limit graph in each case.

\paragraph{$B$-Prior.}
Recall that in this case the prior reads:
\begin{align*}
    \mathbb{P}_{B}^{\varepsilon}(\bm{W}; \bm{\pi},1) &\propto \mathcal{C}_{k}^{\varepsilon}(\bm{W}) \prod_{(i,j) \in \integ{n}^2} \pi_{ij}^{W_{ij}} \ind_{W_{ij} \leq 1} \:.
\end{align*}
Therefore the posterior limit graph has the distribution:
\begin{align*}
    \mathbb{P}_{B}(\W ;\bm{\pi} \odot \bm{K}_{X})
    &= \frac{\prod_{(i,j) \in \integ{n}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{ij} \leq 1}}{\sum_{\W \in \mathcal{S}_{W}} \prod_{(i,j) \in \integ{n}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{ij} \leq 1}} \\
    &= \prod_{(i,j) \in \integ{n}^2}  \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{1+\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}\right)^{W_{ij}} \left(\frac{1}{1+\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}\right)^{1-W_{ij}} \ind_{W_{ij} \leq 1} \:.
\end{align*}

This distribution amounts to: $\forall (i,j) \in \integ{n}^2, \quad \W_{ij} \stackrel{\perp\!\!\!\!\perp}{\sim} \mathcal{B}\left( \frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{1+\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)} \right)$.

\paragraph{$D$-Prior.} The prior writes:
\begin{align*}
    \mathbb{P}_{D}^{\varepsilon}(\bm{W}; \bm{\pi}, 1) &\propto \mathcal{C}_{k}^{\varepsilon}(\bm{W}) \prod_{(i,j) \in \integ{n}^2} \pi_{ij}^{W_{ij}} \ind_{W_{i+} = 1} \:.
\end{align*}
The distribution of the posterior limit then becomes:
\begin{align*}
    \mathbb{P}_{D}(\W ;\bm{\pi} \odot \bm{K}_{X}) &= \frac{\prod_{(i,j) \in \integ{n}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{i+} = 1}}{\sum_{\W \in \mathcal{S}_{W}} \prod_{(i,j) \in \integ{n}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{i+} = 1}} \\
    &= \frac{\prod_{(i,j) \in \integ{n}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{i+} = 1}}{\prod_{i \in \integ{n}} \sum_{\ell \in \integ{n}} \pi_{i\ell} k(\mathbf{x}_i - \mathbf{x}_{\ell})} \\
    &= \prod_{(i,j) \in \integ{n}^2} \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{\ell \in \integ{n}} \pi_{i\ell} k(\mathbf{x}_i - \mathbf{x}_{\ell})}\right)^{W_{ij}} \ind_{W_{i+} = 1} \:.
\end{align*}

This distribution amounts to: $\forall i \in \integ{n}, \quad \W_{i} \stackrel{\perp\!\!\!\!\perp}{\sim} \mathcal{M}\left(1, \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{\ell \in \integ{n}} \pi_{i\ell} k(\mathbf{x}_i - \mathbf{x}_{\ell})}\right)_{j \in \integ{n}}\right)$.

\paragraph{$E$-Prior.}
In this case the prior reads:
\begin{align*}
    \mathbb{P}_{E}^{\varepsilon}(\bm{W}; \bm{\pi}, 1) &\propto \mathcal{C}_{k}^{\varepsilon}(\bm{W}) \prod_{(i,j) \in \integ{n}^2} \frac{\pi_{ij}^{W_{ij}}}{W_{ij}!} \ind_{W_{++} = n} \:.
\end{align*}
Finally, deriving the distribution of the posterior graph limit:
\begin{align*}
    \mathbb{P}_{E}(\W ;\bm{\pi} \odot \bm{K}_{X}) &= \frac{\prod_{(i,j) \in \integ{n}^2}  (W_{ij}!)^{-1}\left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{++} = n}}{\sum_{\W \in \mathcal{S}_{W}} \prod_{(i,j) \in \integ{n}^2} (W_{ij}!)^{-1} \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{++} = n}} \\
    &= n! \prod_{(i,j) \in \integ{n}^2} (W_{ij})^{-1} \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{(\ell,t) \in \integ{n}^2} \pi_{\ell t} k(\mathbf{x}_{\ell} - \mathbf{x}_t)}\right)^{W_{ij}} \ind_{W_{++} = n} \:.
\end{align*}

This distribution amounts to: $\W \sim \mathcal{M}\left(n, \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{(\ell,t) \in \integ{n}^2} \pi_{\ell t} k(\mathbf{x}_{\ell} - \mathbf{x}_t)}\right)_{(i,j) \in \integ{n}^2}\right)$.
\end{proof}