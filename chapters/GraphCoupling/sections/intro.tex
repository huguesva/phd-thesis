\section{Introduction}\label{intro}


This chapter is based on material from the following publication:

\begin{mdframed}
\begin{center} 
    \bibentry{van2022probabilistic} 
\end{center}
\end{mdframed}


This work addresses \Cref{prob:probabilistic_models} and focuses on extending the latent variable perspective to neighbor embedding methods. The goal is twofold: first, to provide a stronger theoretical understanding of these methods. Due to the lack of clear probabilistic foundations, the properties of neighbor embedding algorithms remain largely empirical. This gap between theory and practice can be problematic, as practitioners may adopt suboptimal strategies for their specific use cases.  
While recent software developments have made these methods more scalable \citep{chan2018t,pezzotti2019gpgpu,linderman2019fast}, including our toolbox \emph{TorchDR} \citep{vanassel2024torchdr}, which further broadens their applicability, the need for a well-defined probabilistic framework is increasingly critical.  
In this work, we define a generative probabilistic model that unifies current embedding methods and establish new connections with the well-known PCA model discussed in \Cref{sec:dr_proba_modelling}.  
Second, this work aims to facilitate further extensions of these models by providing mechanisms to incorporate structural priors, a direction we explore in \Cref{sec:supp_material_gc}.

\paragraph{General approach.}
Our approach is similar to that of \Cref{sec:dual_view} in that we devise a parametric model for the data and introduce priors on the parameters. Hence the starting point is to define a proper likelihood. 

\begin{mem1}{Matrix normal model}\label{memo:matrix_normal}
    AAA
\end{mem1}

In the same philosophy,  the priors of the parameters are taken as conjugate with the predefined likelihood. 

However, the originality of our approach lies in the fact that we use two independent models for the data $\mX$ and the embeddings $\mZ$. We then focus on coupling the posterior distributions of the parameters in both models. Inspired by the general formulation of dimensionality reduction as affinity matching algorithms \Cref{sec:background_dr}, the parameters we consider are the affinity matrices in both domains.

In this work we explore We demonstrate that such graphs essentially capture conditional independencies among rows through a pairwise Markov Random Field (MRF) model whose construction can be found in \cref{sec:graph_structure}. 



\begin{mem1}{GMRF models}
    AAA
\end{mem1}
\todo{REMARK : far from iid assumption}


\paragraph{Outline.} 
The rationale of our framework is to suppose that the observations $\Xb$ and $\Zb$ are structured by two latent graphs with $\Wb_{X}$ and $\Wb_{Z}$ standing for their $N$-square weight matrices.
As the goal of DR is to preserve the input's structure in the latent space, we propose to find the best low-dimensional representation $\Zb$ of $\Xb$ such that $\Wb_{X}$ and $\Wb_{Z}$ are close. To build a flexible and robust probabilistic framework, we consider random graphs distributed according to some predefined prior distributions. Our objective is to match the posterior distributions of $\Wb_{X}$ and $\Wb_{Z}$. Note that as they share the same dimensionality the latter graphs can be easily compared unlike $\Xb$ and $\Zb$. The coupling is done with a cross-entropy criterion, the minimization of which will be referred to as graph coupling.

In this work, our main contributions are as follows.

\begin{itemize}
    \item We show in \Cref{sec:pca_graph_coupling} that for Gaussian MRFs, when adapting graph coupling to precision matrices with suitable conjugate priors, one is able to recover PCA embeddings as solutions of the coupling problem in its continuous version (\cref{PCA_graph_coupling}).
    \item We show that SNE, t-SNE, LargeVis and UMAP are all instances of graph coupling and characterized by different choices of prior for discrete latent structuring graphs (\cref{sec:GC_unified}). We demonstrate that such graphs essentially induce a MRF structure whose construction can be found in \cref{sec:graph_structure}.
    \item We uncover the intrinsic probabilistic property explaining why such methods perform poorly on conserving the large-scale structure of the data as a consequence of the degeneracy of the MRF when shift-invariant kernels are used (\cref{prop:integrability_pairwise_MRF}). Such degeneracy induces the loss of the relative positions of clusters corresponding to the connected components of the posterior latent graphs whose distributions are identified (\cref{prop:posterior_W}). These findings are highlighted by a new initialization of the embeddings (\cref{sec:towards_large_scale}).
\end{itemize}
