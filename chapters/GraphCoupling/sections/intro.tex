\section{Introduction}\label{intro}


This chapter is based on material from the following publication:

\begin{mdframed}
\begin{center} 
    \bibentry{van2022probabilistic} 
\end{center}
\end{mdframed}


This work addresses \Cref{prob:probabilistic_models} and focuses on extending the latent variable perspective to neighbor embedding methods. The goal is twofold.

\begin{itemize}
    \item First, to provide a stronger theoretical understanding of these methods. Due to the lack of clear probabilistic foundations, the properties of neighbor embedding algorithms remain largely empirical. This gap between theory and practice can be problematic, as practitioners may adopt suboptimal strategies for their specific use cases.  
    While recent software developments have made these methods more scalable \citep{chan2018t,pezzotti2019gpgpu,linderman2019fast}, including our toolbox \emph{TorchDR} \citep{vanassel2024torchdr}, which further broadens their applicability, the need for a well-defined probabilistic framework is increasingly critical.  
    In this work, we define a generative probabilistic model that unifies current embedding methods and establish new connections with the well-known PCA model discussed in \Cref{sec:dr_proba_modelling}.
    \item Second, this work aims to facilitate further extensions of these models by providing mechanisms to incorporate structural priors, a direction we explore in \Cref{sec:towards_large_scale}.
\end{itemize}  

\paragraph{General approach.}
Our approach is similar to that of \Cref{sec:dual_view} in that we devise a parametric model for the data and introduce priors on the parameters. The starting point is to define an appropriate likelihood. Crucially, we depart from the \textit{i.i.d.} assumption and aim to model the dependencies between samples in $\mX$. To capture these dependencies, we employ Markov Random Field (MRF) models as described in \Cref{memo:MRFs}.

\begin{mem1}{Markov Random Fields}\label{memo:MRFs}
    Consider a collection of random variables $(\vx_1, \dots, \vx_N)$. A Markov Random Field (MRF) is an undirected graphical model representing the conditional independence structure, or Markov properties, satisfied by these variables. Specifically, it is a graph $G = (V, E)$ where the set of vertices $V = \{1, \dots, N\}$ contains the indices of the random variables, and the set of edges $E$ encodes the statistical dependence structure of as follows: for any disjoint subsets $A$, $B$, and $C$ of $V$, $\vx_A$ and $\vx_B$ are conditionally independent given $\vx_C$ if $C$ separates $A$ and $B$ in the graph $G$. That is, if after removing the nodes in $C$ from $G$, there is no path between sets $A$ and $B$ in the resulting graph, then the conditional independence property 
    \begin{align}
        \vx_A \perp\!\!\!\perp \vx_B \mid \vx_C 
    \end{align}
    holds. In this case, we say that $G$ is a Markov network for the distribution of the variables $(\vx_1, \dots, \vx_N)$. The Hammerley-Clifford theorem \citep{clifford1990markov} is a celebrated result stating that any distribution that can be represented via a Markov network can be factorized as a product of potential functions defined on the cliques of the graph $G$. A classical example of such a density is the multivariate Gaussian distribution, which factorizes as a product of pairwise potentials (we refer to \citep{rue2005gaussian} for an exhaustive treatment of Gaussian MRFs). In this case, the edge weights of the associated Markov network are given by the inverse covariance matrix, called the \emph{precision matrix}. Interestingly, the conditional independence properties are directly encoded by the sparsity of the precision matrix. Specifically, the $(i, j)$ entry of the precision matrix is zero if and only if the variables $\vx_i$ and $\vx_j$ are conditionally independent given the rest.
\end{mem1}

The rationale of our framework is to assume that the observations $\Xb$ and $\Zb$ are each structured by their own Markov network graph. Since the goal of dimensionality reduction (DR) is to preserve the structure of the input in the latent space, we propose finding the best low-dimensional representation $\Zb$ of $\Xb$ such that the two structuring graphs are close. This is conceptually similar to the general formulation of dimensionality reduction as affinity matching algorithms, as presented in \Cref{sec:background_dr}.

To build a flexible and robust probabilistic framework, we consider random graphs distributed according to some predefined prior distributions. In the same vein as in \Cref{sec:dual_view}, the priors of the parameters (here the Markov graphs) are chosen to be conjugate with the predefined likelihood (in this case, the Markov Random Field model). This choice allows us to derive closed-form expressions for the posterior distributions of the graphs.

Our global objective is to match the posterior distributions of the two graphs. Note that as they share the same dimensionality the latter graphs can be easily compared unlike $\Xb$ and $\Zb$. The coupling is done with a cross-entropy criterion, the minimization of which will be referred to as graph coupling.

\paragraph{Outline.} 

In this work, our main contributions are as follows.

\begin{itemize}
    \item We show in \Cref{sec:pca_graph_coupling} that for Gaussian MRFs, when adapting graph coupling to precision matrices with suitable conjugate priors, one is able to recover PCA embeddings as solutions of the coupling problem in its continuous version (\Cref{PCA_graph_coupling}).
    \item We show that SNE, t-SNE, LargeVis and UMAP are all instances of graph coupling and characterized by different choices of prior for discrete latent structuring graphs (\Cref{sec:GC_unified}). We demonstrate that such graphs essentially induce a MRF structure whose construction can be found in \Cref{sec:graph_structure}.
    \item We uncover the intrinsic probabilistic property explaining why such methods perform poorly on conserving the large-scale structure of the data as a consequence of the degeneracy of the MRF when shift-invariant kernels are used (\Cref{prop:integrability_pairwise_MRF}). Such degeneracy induces the loss of the relative positions of clusters corresponding to the connected components of the posterior latent graphs whose distributions are identified (\Cref{prop:posterior_W}). These findings are highlighted by a new initialization of the embeddings (\Cref{sec:towards_large_scale}).
\end{itemize}
