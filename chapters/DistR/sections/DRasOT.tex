
% !TEX root = ../paper.tex

\section{Dimensionality Reduction as OT}\label{sec:DR_as_OT}


In this section, we present the strong connections between the classical \cref{eq:DR_criterion} and the GW problem.

%We will elaborate on this interpretation to define a novel dimensionality reduction framework in Section \ref{sec:DDR}.

%\subsection{}

\paragraph{Gromov-Monge interpretation of DR.} As suggested by
\cref{eq:DR_criterion}, dimension reduction seeks to find embeddings $\mZ$ so
that the similarity between the $(i,j)$ samples of the input data is as close as
possible to the similarity between the $(i,j)$ samples of the embeddings. Under
reasonable assumptions about $\simiZ$, this also amounts to identifying the embedding $\mZ$ \emph{and} the best permutation that realigns the two similarity matrices. %To precise this idea we first recall that the function $\simiZ$ is
Recall that the function $\simiZ$ is equivariant by permutation, if, for any $N \times N$ permutation matrix $\mP$ and any
$\mZ$, $\simiZ(\mP \mZ) = \mP \simiZ(\mZ) \mP^\top$ \citep{bronstein2021geometric}.
This type of assumption is natural for $\simiZ$: if we rearrange the order of
samples (\ie, the rows of $\mZ$), we expect the similarity matrix between the
samples to undergo the same rearrangement. 
\begin{restatable}{lemma}{GMproblemequiv}
\label{lemma:GMproblemequiv}
Let $\simiZ$ be a permutation equivariant function and $L$ any loss. The minimum \cref{eq:DR_criterion} is equal to 
\begin{equation}
\label{eq:gm}
\min_{\mZ \in \R^{N \times d}} \min_{\sigma \in S_N} \: \sum_{ij} L([\simiX(\mX)]_{ij}, [\simiZ(\mZ)]_{\sigma(i) \sigma(j)}) \:.
\end{equation}
Also, any sol. $\mZ$ of \cref{eq:DR_criterion} is such that $(\mZ, \operatorname{id})$ is sol. of \cref{eq:gm} and conversely any $(\mZ, \sigma)$ sol. of \cref{eq:gm} is such that $\mZ$ is a sol of \cref{eq:DR_criterion} up to $\sigma$.
\end{restatable}
See proof in \Cref{proof:GMproblemequiv}. The correspondence established between \cref{eq:DR_criterion} and \cref{eq:gm}
unveils a \emph{quadratic problem} similar to GW. Specifically, \cref{eq:gm} relates to
the Gromov-Monge problem\footnote{Precisely $\min_{\sigma \in S_N} \sum_{ij} L([\simiX(\mX)]_{ij}, [\simiZ(\mZ)]_{\sigma(i) \sigma(j)})$ is the Gromov-Monge discrepancy between two discrete distributions, with the same number of atoms and uniform weights.}
\citep{memoli2018gromov} which seeks to identify, by solving a quadratic assignment problem \citep{cela2013quadratic}, the permutation $\sigma$ that best
aligns two similarity matrices. \Cref{lemma:GMproblemequiv} therefore shows that the best permutation is the identity when we also optimize the embedding $\mZ$.
We can delve deeper into these comparisons and demonstrate that the general formulation of dimension reduction is also equivalent to minimizing the Gromov-Wasserstein objective, which serves as a relaxation of the Gromov-Monge problem \citep{memoli2022comparison}.


\paragraph{DR as GW Minimization.} We suppose that the distributions have the same number of points ($N = n$) and uniform weights ($\vh_Z = \vh_X = \frac{1}{N} \one_N$). We recall that a matrix $\mC \in \R^{N \times N}$ is conditionally positive definite (CPD), \textit{resp.} conditionally negative definite (CND), if it is symmetric and $\forall \vx \in \R^{N}, \vx^\top \mathbf{1}_N = 0 \text{ s.t. } \vx^\top \mC \vx \geq 0$, \textit{resp.} $\leq 0$. 

%The main contribution of this section is to demonstrate that \Cref{lemma:GMproblemequiv} can be extended to the GW problem for classical DR loss functions.


We thus have the following theorem that extends \Cref{lemma:GMproblemequiv} to the GW problem (proof can be found in \Cref{proof:theo:main_theo}).
\begin{restatable}{theorem}{maintheodistr}
\label{theo:main_theo}
The minimum \cref{eq:DR_criterion} is equal to $\min_{\mZ}\GW_L(\simiX(\mX), \simiZ(\mZ), \frac{1}{N}\one_N, \frac{1}{N}\one_N)$ in the following settings:
\begin{enumerate}[label=(\roman*), rightmargin=25pt]
\item (spectral methods)  $\simiX(\mX)$ is any matrix, $L = L_2$ and $\simiZ(\mZ) = \mZ \mZ^\top$. 
\item (neighbor embedding methods) $\im(\simiX) \subseteq \R_{>0}^{N \times N}$, $L = L_{\KL}$, the matrix $\simiX(\mX)$ is CPD and, for any $\mZ$,
\begin{equation}
\simiZ(\mZ) = \diag(\alphab_\mZ) \mK_\mZ \diag(\betab_\mZ)\,,
\end{equation}
where $\alphab_\mZ, \betab_\mZ \in \R^{N}_{> 0}$ and $\mK_\mZ \in \R^{N \times N}_{> 0}$ is such that $\log(\mK_\mZ)$ is CPD.
\end{enumerate}
\end{restatable}
% $C_Z$is infinitely divisible, resultat qui pourrait etre ameliorer C_Z = W_Z/alpha_Z tel que log W_Z est CPD

Remarkably, this result shows that \emph{all spectral DR methods} can be seen as OT problems in disguise, as they all equivalently minimize a GW problem. The second point of the theorem also provides some insights into this equivalence in the case of neighbor embedding methods. For instance, the Gaussian kernel $\mK_\mZ$, used extensively in DR, satisfies the hypothesis as $\log(\mK_\mZ) = (-\|\vz_i-\vz_j\|_2^2)_{ij}$ is CPD (see \eg \citealt{maron2018probably}). The terms $\alphab_\mZ, \betab_\mZ$ also allow for considering all the usual normalizations of $\mK_\mZ$: by a scalar so as to have $\sum_{ij} [\simiZ(\mZ)]_{ij}=1$, but also any row-stochastic or doubly stochastic normalization (with the Sinkhorn-Knopp algorithm \citealt{sinkhorn1967concerning}).

Matrices satisfying $\log(\mK_\mZ)$ being CPD are well-studied in the literature
and are known as infinitely divisible matrices \citep{bhatia2006infinitely}. It
is noteworthy that the t-Student kernel does not fall into this category.
Moreover,  in the aforementioned neighbor embedding methods, the matrix
$\simiX(\mX)$ is generally not CPD. The intriguing question of generalizing this
result with weaker assumptions on $\simiZ$ and $\simiX$ remains open for future
research. Interestingly, we have observed in the numerical experiments performed in \Cref{sec:exps} that the
symmetric entropic affinity of \citet{van2023snekhorn} was systematically CPD. 
% is often CPD in practice. 

\begin{remark}
In \Cref{corr:equivCE} of the appendix we also provide other sufficient conditions for neighbor embedding methods with the cross-entropy loss $L(x,y) = x \log(x/y)$. They rely on specific structures for $\simiZ$ but do not impose any assumptions on $\simiX$. Additionally, in \Cref{sec:necessary_and_sufficient}, we provide a \emph{necessary} condition based on a bilinear relaxation of the GW problem. Although its applicability is limited due to challenges in proving it in full generality, it requires minimal assumptions on $\simiX, \simiZ$ and $L$.
\end{remark} 

%\tv{NON LE POINT II NE MARCHE PAS DU TOUT}

% Le second point du théorème,  permet de donner quelques insight sur cette equivalence dans le cas des neighbor embedding methods. En effet, dans certaines méthodes comme tSNE \citep{van2008visualizing} une similarité possible $\simiZ(\mZ)$ peut s'écrire comme un noyau Gaussien entre les points, normalisé par sa somme total. Dans ce cas $\log(\simiZ(\mZ))$ est proportionnel à une matrice de distance au carré ce qui est bien  Bien que les matrices $\simiX$ usuellement utilisée 


% Certain neighbor embedding methods can also be interpreted within this framework. For instance, when the input similarities is 



% when the similarity between two points is translation-invariant and logarithmically concave. For instance, SNE considers  $f(\vz_i -\vz_j) = \exp(-\|\vz_i-\vz_j\|_2^2)$ which satisfies the conditions of the theorem. The normalization constant $R$ serves as a ‘‘repulsion'' term and aims, among other objectives, to avoid the trivial solution $\mZ = 0$ \citep{van2022probabilistic}. 

% \hva{dans SNE c'est pas exactement cette norm, somme que sur un indice}
% It is typically chosen such that $\simiZ(\mZ)$  sums to $1$ (\textit{e.g.,}
% In t-SNE for instance, $R(\mZ) = \sum_{n m} \exp(-\|\vz_n-\vz_m\|_2^2$). 

% It should be noted that the assumptions on $R$ do not encompass all mentioned neighbor embedding methods (e.g., in SNE, $R$ is permutation invariant but not convex). The intriguing question about the generalization of this result to any type of normalizing factor remains for future research.
% \tv{to finish ! parler normalisation bisto, normalisation par ligne/colonne, infinitely divisible, par contre $\simiX$ pas CPD mais yolo, parler du gaussien, qaudn pas CPD on dit aussi en appendix on a d'autres types de conditions pour tout $C_X$ pour une loss cross-entropy}
% \begin{remark} \Cref{theo:main_theo} provides sufficient conditions.  
% %Also, in \Cref{corr:equivCE}
% \end{remark}

In essence, both \Cref{lemma:GMproblemequiv} and \Cref{theo:main_theo} indicate that dimensionality reduction can be reframed from a distributional perspective, with the search for an empirical distribution that aligns with the data distribution in the sense of optimal transport, through the lens of GW. In other words, DR is informally solving $\min_{\vz_1, \cdots, \vz_N} \GW(\frac{1}{n} \sum_{i=1}^{N} \delta_{\vx_i}, \frac{1}{n} \sum_{i=1}^{N} \delta_{\vz_i})$.

% \tv{et là en mettre un couche sur: on peut{} faire varier la masse et le nombre de point c'est ici qu'il faut appuyer}

% The Gromov-Wasserstein formulation offers greater flexibility, allowing us to consider more general forms for the embedding distribution, as we propose in the next section.

% \tv{todo plein de commentaires: parler de l'applicabitlié des commentaires, des sol triviales, des loss, de la normalisation, du doublement concave et dire que ça va se généraliser future works}

% \begin{corollary}
% We have the following equivalences
%  \begin{enumerate}[label=(\roman*)]
% \item For any $\mC_\mX$ spectral methods are equivalent to GW minimization,  \ie\ minimizing \cref{eq:spectral_method} is equivalent to minimizing $\operatorname{GW}_{L_{2}}(\mC_\mX, \mZ \mZ^\top, \frac{1}{N}\mathbf{1}_N, \frac{1}{N}\mathbf{1}_N)$ in $\mZ$.
% \item Neighbor embedding methods are equivalent to GW minimization, \ie\ minimizing \cref{eq:neighbor_techniques} is equivalent to minimizing $\operatorname{GW}_{L_{\operatorname{KL}}}(\mC_\mX, \simiZ(\mZ), \frac{1}{N}\mathbf{1}_N, \frac{1}{N}\mathbf{1}_N)$, whenever 1) $\mC_\mX$ is any matrix and $\forall \mZ, \simiZ(\mZ) = f(\bm{z}_i - \bm{z}_j)$ for $f:\R^{d} \to \R_+^{*}$ that either $\log$-convex or $\log$-concave or 2) $\mC_\mX$ and $\forall \mZ, -\log(\simiZ(\mZ))$ are CPD or CND.
% \end{enumerate}
% \end{corollary}
% 
%\citep{van2023snekhorn}


 % is  can be $L_2$ or $L_{\mathrm{KL}}$. As detailed in \Cref{sec:DR_methods}, the definitions of $c_{\mathcal{X}}$ and $c_{\mathcal{Z}}$ as well as $L$ are what differentiate each method.

% In what follows, $k_{\mathcal{X}}$ denotes a positive definite kernel on $\mathcal{X} \times \mathcal{X}$ \ie . For any such kernel, there exists a mapping $\phi^\mathcal{X}$ from $\mathcal{X}$ onto a Hilbert space $\mathcal{H}_{\mathcal{X}}$ such that $k_{\mathcal{X}}(\bm{x}_i, \bm{x}_j) = \langle \phi^{\mathcal{X}}(\bm{x}_i), \phi^{\mathcal{X}}(\bm{x}_j) \rangle_{\mathcal{H}_{\mathcal{X}}}$ \citep{aronszajn1950theory}. 

% \begin{remark}
% 	When used as a cost inside \eqref{eq:GW_pb}, it amounts to GW between the measures in feature space with the inner product cost
% $$\operatorname{GW}(k_{\mathcal{X}}, k_{\mathcal{Z}}, \mu, \nu) = \operatorname{GW} \Big(\langle,\rangle_{\mathcal{H}_{\mathcal{X}}}, \langle,\rangle_{\mathcal{H}_{\mathcal{Z}}}, \phi^{\mathcal{X}}_{\#}\mu, \phi^{\mathcal{Z}}_{\#}\nu \Big).$$
% \end{remark}


 
%= \{\mT \in \R_{+}^{N \times N}: \mT \one_N = \one_N, \mT^\top \one_N = \one_N\}
% We recall that a matrix $\mC \in \R^{N \times N}$ is conditionally positive definite (CPD), \textit{resp.} negative definite (CND), if $\forall \bm{x} \in \R^{N}, \bm{x}^\top \mathbf{1}_N = 0 \text{ s.t. } \bm{x}^\top \mC \bm{x} \geq 0$, \textit{resp.} $\leq 0$. 
% The proposition below (proof can be found in \Cref{proof:proposition:mainproposition}) gives sufficient conditions for this claim.
% \begin{restatable}{proposition}{mainproposition}
%  %\begin{theorem}
%  \label{proposition:mainproposition}
%  Let $\Omega \subseteq \R$ such that $\im(\simiX) \subseteq \Omega^{N \times N}$. Consider a loss $L$ such that $L(a, \cdot)$ is convex and non-decreasing for any $a \in \Omega$. Suppose that the function $\simiZ$ satisfies 
% \begin{equation}
% 	\label{eq:another_condition_based_on_convexity_inside}
% 	\forall \mZ, \forall \mT \in \operatorname{DS}, \exists \mY, \ \simiZ(\mY) \leq \mT \simiZ(\mZ) \mT^\top\,,
% \end{equation}
% where $\leq$ is understood element-wise, \ie, $\bm{A} \leq \bm{B} \iff \forall (i,j), \ A_{ij} \leq B_{ij}$. Then minimizing in $\mZ$ the objective \cref{eq:DR_criterion} is equivalent to minimizing $\GW_L(\simiX(\mX), \simiZ(\mZ), \frac{1}{N}\mathbf{1}_N, \frac{1}{N}\mathbf{1}_N)$ in $\mZ$. Moreover, when $L(a, \cdot)$ is only convex for any $a \in \Omega$, the same conclusion holds by replacing the inequality with an equality in \cref{eq:another_condition_based_on_convexity_inside}.
% % Let $\mC_\mX$ be any matrix. Minimizing in $\mZ$ the objective \cref{eq:DR_criterion} is equivalent to minimizing $\mathrm{GW}_L (\mC_{\mX}, \mC_{\mZ}, \frac{1}{N} \bm{1}_N , \frac{1}{N} \bm{1}_N)$ in $\mZ$ when any of the following conditions on $L, \simiZ$ are met:
% %  \begin{enumerate}[label=(\roman*)]

% % \item The function $L(a,\cdot)$ is convex for any $a$ and 
% % \begin{equation}
% % \forall \mZ,  \forall \mT \in \operatorname{DS}, \exists \mY, \ \mT \simiZ(\mZ) \mT^\top = \simiZ(\mY)\,.
% % \end{equation}
% % \item The similarity in the output space can be written as \tv{no sorry}
% % \begin{equation}
% % \forall \mZ, \ [\simiZ(\mZ)]_{ij} = h\big(f(\bm{z}_{i}-\bm{z}_{j}), \ g(\mZ)\big) \,,
% % \end{equation}
% % where $h: \R \times \R \to \R, g: \R^{N \times d} \to \R$ and $f: \R^d \to \R$ are such that $\bm{z} \to L(a, h(f(\bm{z}), b))$ is convex for any $a, b$.
% % \item $L(a,\cdot)$ is convex and non-decreasing for any $a$, and $\forall \mZ, \ [\simiZ(\mZ)]_{ij} = f(\bm{z}_{i}-\bm{z}_{j}) \times g(\mZ)$ for some convex function $f: \R^d \to \R$ and $g: \R^{N \times d} \to \R_{+}$.
% % \item $L$ can be written as $L(a,b) = f_1(a) + f_2(b) + h_1(a) h_2(b)$ and $h_1(\mC_\mX)$ and $\forall \mZ, h_2(\simiZ(\mZ))$ are CND or CPD.
%  %\end{enumerate}
% \end{restatable}
% This proposition  provides sufficient conditions for the equivalence between classical DR problems and GW ones. We emphasize that this result holds for any matrix in the input space, as long as the convexity hypothesis of the loss is verified on the values of this matrix. The first condition can be interpreted as showing that to the image of $\simiZ$ is stable by $N \times N$ doubly stochastic matrices, which holds in particular when $\simiZ(\mZ) = \mZ \mZ^\top$. 

% We denote by $\overline{\mu} = \frac{1}{N} \sum_{i \in \integ{N}} \delta_{\bm{x}_i}$ the input data distribution. One can notice that the DR problem of \eqref{eq:DR_criterion} is equivalent to the following Gromov-Monge formulation
% \begin{align}\tag{DR-2}
% 	\min_{(\vz_1,...,\vz_N)} \operatorname{GM} \Big(c_{\mathcal{X}}, c_{\mathcal{Z}}, \overline{\mu}, \frac{1}{N} \sum_{i \in \integ{N}} \delta_{\bm{z}_i} \Big) \:.
% \end{align}
% with the Monge map $T(\{\vx_i\}) = \{\vz^\star_i\}$ for $(\vz^\star_1,...,\vz^\star_N)$ solving the above. Interestingly, the following result shows that it is also equivalent to the Gromov-Wasserstein relaxation.
% \begin{proposition}
% Let $c_{\mathcal{X}}$ and $c_{\mathcal{Z}}$ be positive definite and $L$ be either $L_2$ or $L_{\mathrm{KL}}$. Then,
% \begin{align}\tag{DR-3}
% \min_{(\vz_1, ..., \vz_N)} \operatorname{GW} \Big(c_{\mathcal{X}}, c_{\mathcal{Z}}, \overline{\mu}, \frac{1}{N} \sum_{i \in \integ{N}} \delta_{\bm{z}_i} \Big)
% \end{align}
% is equivalent to the dimensionality reduction problem of \eqref{eq:DR_criterion} with corresponding $L$, $c_{\mathcal{X}}$ and $c_{\mathcal{Z}}$.
% \end{proposition}

% \begin{proof}
% 	$E(c_{\mathcal{X}}, c_{\mathcal{Z}}, \pi)$ is concave in $\pi$ when $c_{\mathcal{X}}$ and $c_{\mathcal{Z}}$ are positive definite thus there exists on optimal solution lying on an extremal point of $\Pi(\bm{1}, \bm{1})$.
% \end{proof}
% This pivotal result paves the way to optimizing couplings for dimensionality reduction. 
