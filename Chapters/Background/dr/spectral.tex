\subsection{Spectral methods}

In this first part, we focus on dimensionality reduction methods that have closed form solutions. We call them \emph{spectral methods} as they rely on the spectral decomposition of the input affinity matrix.

We are going to see that these methods are special instances of \Cref{eq:DR_criterion} with the element-wise loss $L_2$ and $[\mC_Z(\mZ)]_{ij} = \langle \vz_i, \vz_j \rangle$.
Closed form solutions are based on the following celebrated result from \cite{eckart1936approximation}.

\begin{theorem}{\cite{eckart1936approximation}}\label{thm:eckart}
	Let $\mA \in \R^{m \times n}$ be a real matrix with singular value decomposition (SVD) given by $\mA = \sum_{i=1}^{r} \sigma_i u_i v_i^\top$. Let $k \in \N$ with $k \leq r$ and define 
	\begin{align}
		\mP_k = \sum_{i=1}^{k} u_i u_i^\top \quad \text{and} \quad \mA_k = \mP_k \mA = \sum_{i=1}^{k} \sigma_i u_i v_i^\top
	\end{align}
	which are respectively the orthogonal projection matrix onto the subspace spanned by the top $k$ left singular vectors of $\mA$ and the rank-$k$ matrix obtained by truncating the SVD of $\mA$ after the first $k$ terms. It holds that
	\begin{enumerate}[label=(\alph*)]
        \item for any $\mB$ of rank at most $k$ we have $\| \mA - \mA_k \|_F \leq \| \mA - \mB \|_F$,
        \item for any $m \times m$ orthogonal projection matrix $\mP$ of rank $k$, we have $$\| \mA - \mP_k \mA \|_F \leq \| \mA - \mP \mA \|_F \: .$$
    \end{enumerate} 
\end{theorem}

\subsubsection{Principal Component Analysis}

We start by considering the most canonical instance of \Cref{eq:DR_criterion} with the $L_2$ loss and the inner product affinity on both sides: $\mC_X(\mX) = \mX \mX^\top$ and $\mC_Z(\mZ) = \mZ \mZ^\top$. It amounts to minimizing in $\mZ \in \R^{N \times d}$ the following objective
\begin{equation*}\label{eq:DR_IP}\tag{DR-IP}
	\sum_{(i,j) \in \integ{N}^2} (\langle \vx_i, \vx_j \rangle - \langle \vz_i, \vz_j \rangle)^2 = \| \mX \mX^\top - \mZ \mZ^\top \|_F^2 \:.
\end{equation*}

\begin{remark}
	Throughout this section, we consider that the features of the input data $\mX \in \R^{N \times p}$ are centered, \ie $\one_N \mX = \bm{0}_p$. Note that this can be ensured as a preprocessing step by applying the transformation $\mX \leftarrow \mX - \one_N \bm{\mu}^\top$ where $\bm{\mu} = \frac{1}{N} \one_N^\top \mX$ is the empirical mean along the rows of $\mX$.
\end{remark}

We now give a formal defintion of a Principal Component Analysis (PCA) embedding.

\begin{definition}{(PCA embedding)}\label{def:PCA_embedding}
	Let $\mX \in \R^{N \times p}$ of rank $r$ such that $\one_N \mX = \bm{0}_p$ (centered) and consider its SVD $\mX = \mU \mS \mV^\top$ with $\mS = \diag(s_1, ..., s_r)$ such that $s_1 \geq ... \geq s_r > 0$. A matrix
	$\mZ \in \R^{N \times d}$ is called a PCA embedding of $\mX$ if there exists an orthogonal matrix $\mR$ such that $\mZ = \mU_{[:,:d]} \mS_{[:d,:d]} \mR$.
\end{definition}

\begin{proposition}
	Solutions of \Cref{eq:DR_IP} are PCA embeddings of $\mX$.
\end{proposition}

\begin{proof}
	We consider the eigendecomposition of $\mX \mX^\top$ as $\mU \mS^2 \mU^\top$ (equivalent to SVD as the eigenvalues of $\mX \mX^\top$ are positive). Thanks to part (a) of \Cref{thm:eckart}, the closest rank-$d$ matrix to $\mX \mX^\top$ in terms of Frobenius norm is given by $\mU_{[:,:d]} \mS_{[:d,:d]}^2 \mU_{[:,:d]}^\top$.
	The result follows by observing that any PCA embedding $\mZ$ yields $\mZ \mZ^\top = \mU_{[:,:d]} \mS_{[:d,:d]}^2 \mU_{[:,:d]}^\top$.
\end{proof}

PCA is a widely-used method with interpretations that extend beyond its formulation as a preservation of inner product affinities, as seen in \Cref{eq:DR_IP}. In \Cref{memo:PCA}, we provide a brief overview of its most common formulations. Throughout this thesis, we will explore PCA embeddings from multiple perspectives. Beyond offering a fresh understanding of the method, these alternative viewpoints will reveal intriguing possibilities for extending PCA in various ways.

\begin{mem1}{A primer on PCA}\label{memo:PCA}
	We recall two common intuitive formulations that give rise to PCA embeddings as defined in \Cref{def:PCA_embedding}. We keep the same notations with $\mX = \mU \mS \mV^\top$ the ordered SVD of $\mX$ which is already centered.

	\paragraph{Maximum projected variance.} PCA seeks to find the $d$-dimensional linear subspace $\mathrm{Span}(\vb_1, ..., \vb_d)$ that maximizes the variance of the data projected onto it. We take the $\vb_i$'s as an orthonormal basis of the subspace, \ie $\vb_i^\top \vb_i = 1$ for any $i$ and $\vb_i^\top \vb_j = 0$ for $i \neq j$.
	To derive this characterization, we consider the $p \times p$ covariance matrix $\mX^\top \mX$ and consider maximizing the projected variance as follows, where $\mB = (\vb_1, ..., \vb_d)^\top$:
	\begin{align}
		\max_{\mB \in \R^{d \times p}, \mB \mB^\top = \mI_d} \: \mB^\top \left(\mX^\top \mX \right) \mB \:.
	\end{align}
	The above quadratic form on the Stiefel manifold $\{\mB \in \R^{d \times p}, \mB \mB^\top = \mI_d \}$ is minimized by the top $d$ eigenvectors of $\mX^\top \mX$ \citep{absil2008optimization} which in our context is precisely $\mV_{[:d,:]}$ the first $d$ right singular vectors of $\mX$. The recovered reduced projection expressed in the new basis $(\vb_1, ..., \vb_d)$ is then given by $\mX \mV_{[:d,:]}^\top = \mU_{[:,:d]} \mS_{[:d,:d]}$ which is a PCA embedding of $\mX$ (\Cref{def:PCA_embedding}).

	\paragraph{Minimum reconstruction error.} Alternatively, one can explicitly look for the best projection as the one that minimizes a reconstruction error. If we consider the Frobenius norm, this amounts to 
	\begin{align}
		\min_{\mB \in \R^{d \times p}, \mB \mB^\top = \mI_d} \: \| \mX - \mX \mB^\top \mB \|_F^2 \:.
	\end{align}
	By part (b) of \Cref{thm:eckart}, the above directly leads to a PCA embedding.
\end{mem1}
	

\subsubsection{Kernel Principal Component Analysis}

Let $\mC_{\mX}$ denote the affinity matrix constructed from $\mX$. It is positive definite iif for any $\va \in \R^N$, $\va^\top \mC_{\mX} \va \geq 0$.
Interestingly, any positive definite kernels can be seen as an inner product in a suitable space, as detailed in \Cref{rem:kernels}.

When $\simiX(\mX)$ is a positive semi-definite matrix, \cref{eq:DR_criterion}
recovers spectral methods by choosing the quadratic loss $L = L_2$ and $\simi(\mZ) = (\langle \vz_i, \vz_j \rangle)_{(i,j) \in \integ{N}^2}$ the matrix of inner
products in the embedding space. Indeed, in this case, the objective value of \cref{eq:DR_criterion}
reduces to
\begin{equation*}
\label{eq:spectral_method}
	\sum_{(i,j) \in \integ{N}^2} \!\!\!\!\! ([\simiX(\mX)]_{ij}, \langle \vz_i, \vz_j \rangle)^2 = \| \simiX(\mX) - \mZ\mZ^\top \|^2_F\,
\end{equation*}
where $\|\cdot\|_F$ is the Frobenius norm. This problem is commonly known as kernel Principal Component Analysis (PCA)~\citep{scholkopf1997kernel}
 and an optimal solution is given by
$\mZ^\star = (\sqrt{\lambda_1} \vv_{1}, ..., \sqrt{\lambda_d} \vv_{d})^\top$ where $\lambda_i$ is the $i$-th
largest eigenvalue of $\simiX(\mX)$ with corresponding eigenvector $\vv_{i}$ 
\citep{eckart1936approximation}.

\begin{mem1}{Kernels and Reproducing Kernel Hilbert Spaces}\label{rem:kernels}
	A kernel $\kappa(\cdot, \cdot)$ is positive definite if for an arbitrary number of points $(\vx_1, ..., \vx_n) \in \mathcal{X}^n$, the kernel matrix $\bm{K} = (\kappa(\vx_i, \vx_j))_{(i,j) \in [n]^2}$ is positive definite. The Aronszajn's theorem \citep{aronszajn1950theory} states that an equivalent condition to $\kappa$ being positive definite is the existence of a Hilbert space $\mathcal{H}$ and a mapping $\phi : \mathcal{X} \to \mathcal{H}$ such that for any $(\vx, \vx') \in \mathcal{X}^2$,
\begin{align}\label{eq:kernel_inner_product}
    \kappa(\vx, \vx') = \langle \phi(\vx), \phi(\vx') \rangle_{\mathcal{H}} \:.
\end{align}
Of particular interest is the Hilbert space $\mathcal{H}$ associated to $\kappa$ called reproducing kernel Hilbert space (RKHS) which is a space of functions from $\mathcal{X}$ to $\mathbb{R}$ such that for any $\vx \in \mathcal{X}$, the function $\kappa_{\vx} : \bm{t} \to \kappa(\vx, \bm{t})$ is in $\mathcal{H}$ and for any $f \in \mathcal{H}$, $f(\vx) = \langle f, \kappa_{\vx} \rangle_{\mathcal{H}}$. $\kappa$ is then called the reproducing kernel of $\mathcal{H}$. An important result is that if $\mathcal{H}$ is a RKHS, then it has a unique reproducing kernel. Conversely, a positive definite kernel $\kappa$ can be the reproducing kernel of at most one RKHS.

An equivalent definition of a RKHS on $\mathcal{X}$ is a Hilbert
space $\mathcal{H} \subset \mathbb{R}^{\mathcal{X}}$ (functions from $\mathcal{X}$ to $\mathbb{R}$) where all evaluation functionals $\delta_x : \mathcal{H} \to \mathbb{R}$,
defined by $\delta_x(f) = f(x)$, are continuous.

Equation \ref{eq:kernel_inner_product} tells us that the inner product between two feature vectors is given by the kernel $\kappa$. Hence we can evaluate the inner product of any two feature vectors efficiently without knowing an explicit form of either $\phi$ or $\mathcal{H}$. With this computation of inner product, many linear methods of classical data analysis can be extended to nonlinear ones using the kernel matrix which computation does not depend on the dimensionality of the feature space. Such strategy is known as the \textit{kernel trick} in the literature.
\end{mem1}


As shown by \citet{ham2004kernel, ghojogh2021unified}, numerous dimension reduction methods can be categorized in this manner.
This includes 
% PCA when $\simiX(\mX) = \mX\mX^\top$ is the matrix of inner products in the input space; 
(classical) multidimensional scaling \citep{borg2005modern}, when $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX \mH$ with $\mD_\mX$ the matrix of squared euclidean distance between the points in $\R^{p}$ and $\mH = \mI_N - \frac{1}{N} \one_N \one_N^\top$ is the centering matrix; Isomap \citep{tenenbaum2000global}, with $\simiX(\mX) = -\frac{1}{2} \mH \mD_\mX^{(g)} \mH$ with $\mD_\mX^{(g)}$ the geodesic distance matrix; Laplacian Eigenmap
\citep{belkin2003laplacian}, with $\simiX(\mX) = \mL_\mX^{\dagger}$ the pseudo-inverse of the Laplacian associated to some adjacency matrix $\mW_\mX$; but also Locally Linear Embedding \citep{roweis2000nonlinear}, and Diffusion Map \citep{coifman2006diffusion} (for all of these examples we refer to the work by \citealt[Table 1]{ghojogh2021unified}).

