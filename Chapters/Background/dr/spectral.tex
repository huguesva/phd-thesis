\subsection{Spectral methods}\label{sec:spectral_methods}

In this initial section, we focus on dimensionality reduction methods that have closed-form solutions. These methods are commonly referred to as \emph{spectral methods} because they are based on the spectral decomposition of the input affinity matrix. We will demonstrate that these methods are particular cases of \Cref{eq:DR_criterion} with the element-wise $L_2$ loss and $[\simiZ]_{ij} = \langle \vz_i, \vz_j \rangle$. Closed form solutions are based on the following key result by \cite{eckart1936approximation}.

\begin{theorem}{\cite{eckart1936approximation}}\label{thm:eckart}
	Let $\mM \in \R^{m \times n}$ be a real matrix with singular value decomposition (SVD) given by $\mM = \sum_{i=1}^{r} \sigma_i \vu_i \vv_i^\top$ where $(\vu_1, \dots, \vu_r)$ and $(\vv_1, \dots, \vv_r)$ are orthonormal families of vectors in $\R^m$ and $\R^n$ respectively, and $\sigma_1 \geq \dots \geq \sigma_r > 0$ are the singular values of $\mM$. Let $k \in \N$ with $k \leq r$ and define 
	\begin{align}
		\mS_k = \sum_{i=1}^{k} \vu_i \vu_i^\top \quad \text{and} \quad \mM_k = \mS_k \mM = \sum_{i=1}^{k} \sigma_i \vu_i \vv_i^\top
	\end{align}
	which are respectively the orthogonal projection matrix onto the subspace spanned by the top $k$ left singular vectors of $\mM$ and the rank-$k$ matrix obtained by truncating the SVD of $\mM$ after the first $k$ terms. It holds that
	\begin{enumerate}[label=(\alph*)]
        \item for any $\mB$ of rank at most $k$ we have $\| \mM - \mM_k \|_F \leq \| \mM - \mB \|_F$,
        \item for any $m \times m$ orthogonal projection matrix $\mS$ of rank $k$, we have $$\| \mM - \mS_k \mM \|_F \leq \| \mM - \mS \mM \|_F \: .$$
    \end{enumerate} 
\end{theorem}

\subsubsection{Principal Component Analysis}

We start by considering the most canonical instance of \Cref{eq:DR_criterion} with the $L_2$ loss and the inner product affinity on both sides: $\simiX = \mX \mX^\top$ and $\simiZ = \mZ \mZ^\top$. It amounts to minimizing in $\mZ \in \R^{N \times d}$ the following objective
\begin{equation*}\label{eq:DR_IP}\tag{DR-IP}
	\sum_{(i,j) \in \integ{N}^2} (\langle \vx_i, \vx_j \rangle - \langle \vz_i, \vz_j \rangle)^2 = \| \mX \mX^\top - \mZ \mZ^\top \|_F^2 \:.
\end{equation*}

We give a formal definition of a Principal Component Analysis (PCA) embedding.

\begin{definition}{(PCA embedding)}\label{def:PCA_embedding}
	Let $\mX \in \mathbb{R}^{N \times p}$ be a matrix of rank $r$ such that $\mathbf{1}_N \mX = \mathbf{0}_p$ (i.e., it is centered). Suppose $\mX$ has the singular value decomposition (SVD) given by $\mX = \sum_{i=1}^{r} s_i \vu_i \vv_i^\top$, where $s_1 \geq \dots \geq s_r > 0$ are its positive singular values. A matrix $\mZ \in \mathbb{R}^{N \times d}$ is called a \emph{PCA embedding} of $\mX$ of dimension $d \leq r$ if there exists an orthonormal family of vectors $(\vq_1, \dots, \vq_d)$ in $\mathbb{R}^d$ and a family of positive scalars $(\lambda_1, \dots, \lambda_d)$ such that $\mZ = \sum_{i=1}^{d} \lambda_i \vu_i \vq_i^\top$. Moreover, if $\lambda_i = s_i$ for any $i \in \integ{d}$, then $\mZ$ is called a \emph{standard PCA embedding} of $\mX$.
\end{definition}
	
\begin{remark}
	Note that PCA embeddings are defined up to rotation of the features in the latent space via the orthonormal matrix $\mQ = (\vq_1, ..., \vq_d) \in \R^{d \times d}$ that can be chosen arbitrarily. This is a direct consequence of the invariance of the inner product under such orthonormal transformations.
\end{remark}

Throughout this section, we consider that the features of the input data $\mX \in \R^{N \times p}$ are centered, \ie $\one_N \mX = \bm{0}_p$. Note that this can be ensured as a preprocessing step by applying the transformation $\mX \leftarrow \mX - \one_N \bm{\mu}^\top$ where $\bm{\mu} = \frac{1}{N} \one_N^\top \mX$ is the empirical mean along the rows of $\mX$.

\begin{proposition}\label{prop:PCA_embedding}
    The embeddings $\mZ \in \mathbb{R}^{N \times d}$ that minimize \ref{eq:DR_IP} are PCA embeddings of $\mX$.
\end{proposition}

\begin{proof}
    Consider the eigendecomposition of $\mX \mX^\top$ given by $\mX \mX^\top = \sum_{i=1}^{r} s_i^2 \vu_i \vu_i^\top$, where $s_i$ are the singular values of $\mX$, and $\vu_i$ are the corresponding eigenvectors. According to part (a) of \Cref{thm:eckart}, the closest rank-$d$ matrix to $\mX \mX^\top$ in terms of the Frobenius norm is given by
    $
    \sum_{i=1}^{d} s_i^2 \vu_i \vu_i^\top.
    $
    The result follows by noting that any PCA embedding $\mZ$ yields 
    $
    \mZ \mZ^\top = \sum_{i=1}^{d} s_i^2 \vu_i \vu_i^\top,
    $
    which matches the rank-$d$ approximation of $\mX \mX^\top$.
\end{proof}


PCA is a widely-used method with interpretations that extend beyond its formulation as a preservation of inner product affinities, as seen in \Cref{eq:DR_IP}. In \Cref{memo:PCA}, we provide a brief overview of its most common formulations.
	
\begin{mem1}{A primer on PCA}\label{memo:PCA}
	We recall two common intuitive formulations that give rise to PCA embeddings as defined in \Cref{def:PCA_embedding}. We maintain the same notation, with $\mX = \sum_{i=1}^{r} s_i \vu_i \vv_i^\top$ being the ordered SVD of $\mX$, which is assumed to be centered.

	\paragraph{Maximum projected variance.} PCA seeks to find the $d$-dimensional linear subspace $\mathrm{Span}(\vb_1, \dots, \vb_d)$ that maximizes the variance of the data projected onto it. We take the $\vb_i$'s as an orthonormal basis of the subspace, i.e., $\vb_i^\top \vb_i = 1$ for any $i$, and $\vb_i^\top \vb_j = 0$ for $i \neq j$. 
	To derive this characterization, consider the $p \times p$ covariance matrix $\mX^\top \mX$ and maximize the projected variance as follows, where $\mB = (\vb_1, \dots, \vb_d)^\top$:
	\begin{align}
	\max_{\mB \in \mathbb{R}^{d \times p}, \mB \mB^\top = \mI_d} \: \mB^\top \left(\mX^\top \mX\right) \mB \:.
	\end{align}
	This quadratic form on the Stiefel manifold $\{\mB \in \mathbb{R}^{d \times p}, \mB \mB^\top = \mI_d \}$ is maximized by the top $d$ eigenvectors of $\mX^\top \mX$, which in our context corresponds to $(\vv_1, \dots, \vv_d)^\top$, the first $d$ right singular vectors of $\mX$. The recovered reduced representation expressed in the new basis $(\vb_1, \dots, \vb_d)$ is then given by
	$
	\mX \mB^\top = \sum_{i=1}^{d} s_i \vu_i \ve_i^\top
	$
	where $\ve_i$ is the $i$-th canonical basis vector in $\R^d$. The latter is a PCA embedding of $\mX$ (\Cref{def:PCA_embedding}).

	\paragraph{Minimum reconstruction error.} Alternatively, one can explicitly seek the best projection by minimizing the reconstruction error. Considering the Frobenius norm, this amounts to 
	\begin{align}
	\min_{\mB \in \mathbb{R}^{d \times p}, \mB \mB^\top = \mI_d} \: \| \mX - \mX \mB^\top \mB \|_F^2 \:.
	\end{align}
	By part (b) of \Cref{thm:eckart}, the above directly leads to a PCA embedding.

\end{mem1}


Throughout this thesis, we will explore PCA embeddings from multiple perspectives. Beyond offering a fresh understanding of the method, these alternative viewpoints will reveal intriguing possibilities for extending PCA in various ways. We now focus on the generalization of PCA to nonlinear embeddings applicable when $\simiX$ is a positive definite kernel.

\subsubsection{Kernel Principal Component Analysis}

The presented PCA problem presented above can be extended to a more general setting by considering positive definite affinities or kernels \citep{scholkopf1998nonlinear}. 

In what follows, we define $\simiX$ as a positive definite kernel if for any $\mX \in \R^{N \times p}$, $\simiX$ is positive semidefinite (or equivalently $\simiX \in \mathcal{S}^N_{+}$), meaning that for any $\va \in \R^N$, $\va^\top \simiX \va \geq 0$. For a positive definite kernel $\simiX$, we can define the following criterion that generalizes PCA:
\begin{align}\label{eq:DR_kIP}\tag{DR-kIP}
	\sum_{(i,j) \in \integ{N}^2} \!\!\!\!\! ([\simiX]_{ij}, \langle \vz_i, \vz_j \rangle)^2 = \| \simiX - \mZ\mZ^\top \|^2_F\ \:.
\end{align}

The above is a generalization of \Cref{eq:DR_IP} to the case of positive definite kernels.
Interestingly, if there exists a function $\kappa$ such that for any $(i,j)$, $[\simiX]_{ij} = \kappa(\vx_i, \vx_j)$, which may not always be the case due to various normalizations, then $\simiX$ can be seen as an inner product in a suitable space accessible through a nonlinear transformation often called the \emph{feature map}. Thus the above \Cref{eq:DR_kIP} can be seen as a nonlinear generalization of \Cref{eq:DR_IP}. We provide more details in the following \Cref{mem:kernels}.

\begin{mem1}{Kernels and Reproducing Kernel Hilbert Spaces}\label{mem:kernels}
	A kernel $\kappa(\cdot, \cdot)$ is positive definite if for an arbitrary number of points $(\vx_1, ..., \vx_n) \in \mathcal{X}^n$, the kernel matrix $\bm{K} = (\kappa(\vx_i, \vx_j))_{(i,j) \in [n]^2}$ is positive definite. The Aronszajn's theorem \citep{aronszajn1950theory} states that an equivalent condition to $\kappa$ being positive definite is the existence of a Hilbert space $\mathcal{H}$ and a mapping $\phi : \mathcal{X} \to \mathcal{H}$ such that for any $(\vx, \vx') \in \mathcal{X}^2$,
\begin{align}\label{eq:kernel_inner_product}
    \kappa(\vx, \vx') = \langle \phi(\vx), \phi(\vx') \rangle_{\mathcal{H}} \:.
\end{align}
Of particular interest is the Hilbert space $\mathcal{H}$ associated to $\kappa$ called reproducing kernel Hilbert space (RKHS) which is a space of functions from $\mathcal{X}$ to $\mathbb{R}$ such that for any $\vx \in \mathcal{X}$, the function $\kappa_{\vx} : \bm{t} \to \kappa(\vx, \bm{t})$ is in $\mathcal{H}$ and for any $f \in \mathcal{H}$, $f(\vx) = \langle f, \kappa_{\vx} \rangle_{\mathcal{H}}$. $\kappa$ is then called the reproducing kernel of $\mathcal{H}$. An important result is that if $\mathcal{H}$ is a RKHS, then it has a unique reproducing kernel. Conversely, a positive definite kernel $\kappa$ can be the reproducing kernel of at most one RKHS.

An equivalent definition of a RKHS on $\mathcal{X}$ is a Hilbert
space $\mathcal{H} \subset \mathbb{R}^{\mathcal{X}}$ (functions from $\mathcal{X}$ to $\mathbb{R}$) where all evaluation functionals $\delta_x : \mathcal{H} \to \mathbb{R}$,
defined by $\delta_x(f) = f(x)$, are continuous.

Equation \ref{eq:kernel_inner_product} tells us that the inner product between two feature vectors is given by the kernel $\kappa$. Hence we can evaluate the inner product of any two feature vectors efficiently without knowing an explicit form of either $\phi$ or $\mathcal{H}$. With this computation of inner product, many linear methods of classical data analysis can be extended to nonlinear ones using the kernel matrix which computation does not depend on the dimensionality of the feature space. Such strategy is known as the \textit{kernel trick} in the literature.
\end{mem1}

Similarly to PCA, we can compute kernel PCA embeddings via the spectral decomposition of the affinity matrix.

\begin{definition}{(kernel PCA embedding)}
    Let $\simiX \in \mathcal{S}^N_{+}$ be a matrix of rank $r$, and consider its compact eigendecomposition $\simiX = \sum_{i=1}^{r} s_i^2 \vu_i \vu_i^\top$, where $s_1 \geq \dots \geq s_r > 0$ are its positive eigenvalues. A matrix $\mZ \in \mathbb{R}^{N \times d}$ is called a \emph{kernel PCA embedding} of $\simiX$ of dimension $d \leq r$ if there exists an orthonormal family of vectors $(\vq_1, \dots, \vq_d)$ in $\mathbb{R}^d$ and a family of positive scalars $(\lambda_1, \dots, \lambda_d)$ such that $\mZ = \sum_{i=1}^{d} \lambda_i \vu_i \vq_i^\top$. Moreover, if $\lambda_i = s_i$ for any $i \in \integ{d}$, then $\mZ$ is called a \emph{standard kernel PCA embedding} of $\simiX$.
\end{definition}

\begin{proposition}\label{prop:kernel_PCA_embedding}
    The embeddings $\mZ \in \mathbb{R}^{N \times d}$ that minimize \ref{eq:DR_kIP} are kernel PCA embeddings of $\simiX$.
\end{proposition}

The above \Cref{prop:kernel_PCA_embedding} follows the same principles as the proof of \Cref{prop:PCA_embedding}. The result is obtained by replacing $\mX \mX^\top$ by any $\simiX \in \mathcal{S}^N_{+}$.

\paragraph{Generality of the kernel PCA problem.}
As shown by \citet{ham2004kernel, ghojogh2021unified}, numerous dimension reduction methods can be categorized in this manner.
This includes 
% PCA when $\simiX(\mX) = \mX\mX^\top$ is the matrix of inner products in the input space; 
(classical) multidimensional scaling \citep{borg2005modern}, when $\simiX = -\frac{1}{2} \mH \mC_\mX \mH$ with $\mC_\mX$ the matrix of squared euclidean distance between the points in $\R^{p}$ and $\mH = \mI_N - \frac{1}{N} \one_N \one_N^\top$ is the centering matrix; Isomap \citep{tenenbaum2000global}, with $\simiX = -\frac{1}{2} \mH \mD_\mX^{(g)} \mH$ with $\mD_\mX^{(g)}$ the geodesic distance matrix; Laplacian Eigenmap
\citep{belkin2003laplacian}, with $\simiX = \mL_\mX^{\dagger}$ the pseudo-inverse of the Laplacian associated to some adjacency matrix $\mW_\mX$; but also Locally Linear Embedding \citep{roweis2000nonlinear} (for all of these examples we refer to the work by \citealt[Table 1]{ghojogh2021unified}). Particularly noteworthy approaches include Diffusion Maps \citep{coifman2006diffusion} and PHATE (Potential of Heat-diffusion for Affinity-based Transition Embedding) \citep{moon2019visualizing}, both of which rely on local similarities transformed into transition probabilities by normalizing the affinity matrix. This type of affinity will be central to the following sections of this thesis. These methods allow for multi-step random walks by exponentiating the affinity matrix, thereby controlling the scale of dependencies captured. While Diffusion Maps employ the traditional $L_2$ criterion, PHATE introduces a novel \emph{potential distance}, based on the logarithm of the affinity matrix, which is more robust to capturing global relationships.

\begin{remark}
    Interestingly, one could reverse \Cref{eq:DR_kIP} by considering the inner product kernel on the input data: $\simiX = \mX \mX^\top$ and a nonlinear kernel $\simiZ$ on the latent space. This is precisely what Gaussian Process Latent Variable Models (GPLVM) by \cite{lawrence2005probabilistic} achieve, but with a $\KL$ divergence loss instead of the $L_2$ loss.
\end{remark}
