\subsection{Symmetric Optimal Transport for Normalizing Affinity Matrices}\label{sec:doubly_sto}

In this section, we introduce an unconventional formulation of optimal transport (OT), where the input measure is transported onto itself. We refer to this problem as \emph{symmetric} or \emph{self} optimal transport. A noteworthy initial observation is that the unregularized OT problem of \Cref{eq:OT} has a trivial solution under this setting: the identity coupling, which assigns each point's mass to itself. This occurs because if $c$ is a distance then the cost matrix $\C = (c(\vx_i, \vx_j))_{ij}$ has zero entries along the diagonal, resulting in a total transport cost of zero. However, by introducing constraints as in \Cref{eq:cot}, the mass is encouraged to distribute among neighboring points. This results in producing a variant of soft k-nearest neighbor graph, as presented in \Cref{sec:background_dr}.

We refer to these matrices as \emph{symmetric doubly stochastic matrices} \footnote{Doubly stochastic (DS) affinities are non-negative matrices whose rows and columns each have a unit $\ell_1$ norm.}. We first describe their construction under the specific choice of $D_\psi = \KL$ \ie the entropic OT setting. We then explore their interesting properties.

\paragraph{Symmetric Entropy-Constrained Optimal Transport.} 
We consider a setting where $N_S = N_T = n$. In the special case of uniform
marginals, and for $\varepsilon^\star > 0$, entropic OT computes the minimum of $\Pb
\mapsto \langle \Pb, \C \rangle -\varepsilon^\star \sum_{i} \operatorname{H}(\Pb_{i:})$
over the space of doubly stochastic matrices $\{\Pb \in \R_{+}^{n \times n} :
\Pb \bm{1} = \Pb^\top \bm{1} =\bm{1}\}$. The optimal solution is the
\emph{unique} doubly stochastic matrix $\Pb^{\mathrm{ds}}$ of the form $\Pb^{\mathrm{ds}}=\diag(\mathbf{u})\K
\diag(\mathbf{v})$ where $\K = \exp(-\C/\varepsilon^\star)$ is the Gibbs energy derived from
$\C$ and $\mathbf{u}, \mathbf{v}$ are positive vectors that can be found with
the celebrated Sinkhorn-Knoppâ€™s algorithm \citep{cuturi2013sinkhorn,
sinkhorn1964relationship} (see \Cref{mem:sinkhorn}). Interestingly, when the cost $\C$ is \emph{symmetric}
(\eg $\C \in \mathcal{D}$) we can take $\mathbf{u} = \mathbf{v}$ \cite[Section
5.2]{idel2016review} so that the unique optimal solution is itself symmetric and writes  
\begin{align}\label{eq:plan_sym_sinkhorn}
    \Pb^{\mathrm{ds}} = \exp \left((\mathbf{f}^\star \oplus \mathbf{f}^\star - \Cb) / \varepsilon^\star \right) \text{ where } \mathbf{f} \in \R^n \,.
\tag{DS}
\end{align}
To see this, notice that $\mP^{\star \top}$ is also doubly stochastic and has exactly the same cost. Indeed,
\begin{align}
    \langle \mC, \mP^{\star \top} \rangle = \sum_{i,j} C_{ij} P^{\star}_{ji} = \sum_{i,j} C_{ji} P^{\star}_{ij} = \langle \mC, \mP^{\star} \rangle \:.
\end{align}
Now, if we assume that $\mP^\star$ is not symmetric, the strict convexity of the objective $\Pb
\mapsto \langle \Pb, \C \rangle -\varepsilon^\star \sum_{i} \operatorname{H}(\Pb_{i:})$ and Jensen's inequality would give us that $\frac{1}{2} (\mP^\star + \mP^{\star \top})$ has a strictly better cost than $\mP^{\star}$, which is not possible. Therefore $\mP^\star$ is necessarily symmetric and has the form given in \Cref{eq:plan_sym_sinkhorn}.

\begin{remark}{(Deriving the Sinkhorn Update)}
In this case, an equivalent constrained formulation for the symmetric entropic OT problem is
\begin{equation}
\tag{SEOT}
\label{eq:entropy_constrained_OT}
\min_{\Pb \in \R_{+}^{n \times n}} \quad \langle \Pb, \Cb \rangle \quad \text{s.t.} \quad \Pb \bm{1} = \bm{1}, \: \Pb = \Pb^\top \text{ and } \sum_i \operatorname{H}(\Pb_{i:}) \geq \eta \:,
\end{equation}
where $0 \leq \eta \leq n (\log n + 1)$ is a constraint on the global entropy $\sum_i \operatorname{H}(\Pb_{i:})$ of the OT plan $\Pb$ which happens to be saturated at optimum. We are going to see in what follows how to retrieve the formulation of the symmetric OT plan of \Cref{eq:plan_sym_sinkhorn} as well as the Sinkhorn iteration from \Cref{eq:entropy_constrained_OT}.

For the above convex problem of \Cref{eq:entropy_constrained_OT} the Lagrangian writes, where $\varepsilon \in \mathbb{R}_+$, $\mathbf{f} \in \mathbb{R}^n$ and $\bm{\Gamma} \in \mathbb{R}^{n \times n}$:
\begin{align}
    \mathcal{L}(\Pb, \mathbf{f}, \varepsilon, \bm{\Gamma}) &= \langle \Pb, \C \rangle + \Big\langle \varepsilon, \eta - \sum_{i \in \integ{n}} \operatorname{H}(\Pb_i) \Big\rangle + 2\langle \mathbf{f}, \bm{1} - \Pb \bm{1} \rangle + \big\langle \bm{\Gamma}, \Pb - \Pb^\top \big\rangle \:.
\end{align}
Strong duality holds and the first order KKT condition gives for the optimal primal $\Pb^\star$ and dual $(\varepsilon^\star, \mathbf{f}^\star, \bm{\Gamma}^\star)$ variables: 
\begin{align}
    \nabla_{\Pb} \mathcal{L}(\Pb^\star, \mathbf{f}^\star, \varepsilon^\star, \bm{\Gamma}^\star) &= \C + \varepsilon^\star \log{\Pb^\star} - 2\mathbf{f}^\star \bm{1}^\top + \bm{\Gamma}^\star - \bm{\Gamma}^{\star\top} = \bm{0} \:.
\end{align}
Since $\Pb^\star, \C \in \mathcal{S}$ we have $\bm{\Gamma}^\star - \bm{\Gamma}^{\star\top} = \mathbf{f}^\star \bm{1}^\top - \bm{1}\mathbf{f}^{\star \top}$. Hence $\C + \varepsilon^\star \log{\Pb^\star} - \mathbf{f}^\star \oplus \mathbf{f}^\star = \bm{0}$. Suppose that $\varepsilon^\star = 0$ then the previous reasoning implies that $\forall (i,j), C_{ij} = f_i^\star + f_j^\star$. Using that $\C \in \mathcal{D}$ we have $C_{ii} = C_{jj} = 0$ thus $\forall i,  f^\star_i = 0$ and thus this would imply that $\C = 0$ which is not allowed by hypothesis. Therefore $\varepsilon^\star \neq 0$ and the entropy constraint is saturated at the optimum by complementary slackness. Isolating $\Pb^\star$ then yields:
\begin{align}
    \Pb^{\star} &= \exp{\left( (\mathbf{f}^\star \oplus \mathbf{f}^{\star} - \C) / \varepsilon^\star \right)} \:.
\end{align}
$\Pb^\star$ must be primal feasible in particular $\Pb^\star \bm{1} = \bm{1}$. This constraint gives us the Sinkhorn fixed point relation in the symmetric setting. It gives for $\mathbf{f}^\star$:
\begin{align}
    \forall i \in \integ{n}, \quad [\mathbf{f}^\star]_i = - \varepsilon^\star \operatorname{LSE} \big((\mathbf{f}^\star - \C_{:i}) / \varepsilon^\star \big)\,,
\end{align}
where for a vector $\bm{\alpha}$, we use the notation
$\operatorname{LSE}(\bm{\alpha}) = \log \sum_{k} \exp (\alpha_k)$.
\end{remark}

\paragraph{Benefits of the doubly stochastic normalization.}
In many applications, it has been demonstrated that DS affinity normalization (\ie determining the nearest DS matrix to a given affinity matrix) offers numerous benefits. First, and it is well-established that it enhances spectral clustering performances \citep{Ding_understand,Zass,beauchemin2015affinity}. Additionally, DS matrices present the benefit of being invariant to the various Laplacian normalizations \citep{von2007tutorial}. Recent observations indicate that the DS projection of the Gaussian kernel under the $\KL$ geometry is more resilient to heteroscedastic noise compared to its stochastic counterpart \citep{landa2021doubly}. It also offers a more natural analog to the heat kernel \citep{marshall2019manifold}.
These properties have led to a growing interest in DS affinities, with their use expanding to various applications such as smoothing filters \citep{Milanfar}, subspace clustering \citep{lim2020doubly} and transformers \citep{sander2022sinkformers}. It is worth noting that, beyond the context of affinities, the symmetric OT problem solved by \Cref{eq:plan_sym_sinkhorn} also arises as the debiasing term in the Sinkhorn divergence \citep{feydy2019interpolating}.


\begin{remark}
    Interestingly, \citet{zass2005unifying} demonstrated that the kernel k-means problem can be reformulated via algebraic manipulations as follows, where $\mG$ represents the cluster assignment matrix:
    \[
        \max_{\mG \in \R^{N \times K}_+} \quad \langle \mG \mG^\top, \mK \rangle \quad \text{s.t.} \quad \mG \mG^\top \mathbf{1} = \mathbf{1}, \quad \mG^\top \mG = \mI_N,
    \]
    where $N$ is the number of input samples, $K$ is the number of desired clusters, and $\mK$ is the kernel defined on the input data. 

    Notably, this problem is equivalent to the symmetric optimal transport (OT) problem with uniform marginals and the additional orthogonality constraint $\mG^\top \mG = \mI_N$. To address this, they propose first projecting $\mK$ onto the set of doubly stochastic matrices and then performing eigendecomposition to satisfy the orthogonality constraint, similar to \Cref{thm:eckart}. Moreover, they demonstrate that various graph cut techniques can be expressed as doubly stochastic projections of the graph weight matrix. In their framework, each graph cut variant corresponds to a particular choice of geometry $D_\psi$ under which the projection is carried out.
\end{remark}


\vspace{0.5cm}
\begin{prob}{Deriving Adaptive Versions of Doubly Stochastic Affinities}\label{prob:adaptive_ds}
    As discussed in \Cref{sec:background_dr}, popular neighbor embedding techniques, with t-SNE being the most prominent example, rely on an input affinity matrix known as the entropic affinity. This matrix adjusts the bandwidth at each point to account for varying noise levels in the data.

    We have seen that doubly stochastic (DS) affinities correspond to a symmetric entropic optimal transport (OT) problem with a constraint on the \emph{global entropy}, i.e., the sum of the entropies of each row or column of the OT plan. Thus, a natural question arises: can we derive an adaptive version of DS affinities that adjusts the entropy constraint at each point? Such an adaptation would allow us to account for the heteroscedastic noise inherent in real-world data, while still retaining desirable properties of DS affinities, such as symmetry.
\end{prob}

\vspace{0.5cm}
