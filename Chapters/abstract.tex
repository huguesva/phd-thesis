\newpage
\section*{Abstract}

This thesis provides new theoretical insights and practical methodologies for dimensionality reduction through the lenses of probabilistic modeling and optimal transport. By unifying both linear and nonlinear dimensionality reduction (DR) methods within a probabilistic framework, we extend latent variable models, traditionally limited to linear techniques, to include popular neighbor embedding methods. This unification facilitates the incorporation of priors and clarifies underlying modeling assumptions, thereby enhancing the practical application of DR methods.

We then propose a unified approach that treats both data and embeddings as probability distributions, interpreting DR methods as optimal transport problems. This perspective allows us to control the granularity of the low-dimensional representation by identifying prototype points that represent multiple data points. Consequently, we achieve simultaneous clustering and dimensionality reduction, effectively balancing the trade-offs between these two aspects.

In the final part of the thesis, we introduce new similarity functions to better capture the geometry of datasets. We address issues in DR practices involving the symmetrization of Markov chain matrices, which define transition probabilities between data points. By symmetrizing under the appropriate geometry, we resolve inconsistencies in existing approaches, allowing for accurate pointwise control of entropy and thereby better handling heteroscedastic noise in the data.