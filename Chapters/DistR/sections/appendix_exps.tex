\section{Appendix of experimental section}\label{sec:appendix_exps}

We report in the following subsections of this section:
\begin{itemize}
	\item \ref{sec:algorithms}: details on the algorithms used in our experiments.
	\item \ref{sec:implementation_details}: implementation details, validation of hyperparameters, datasets and metrics.
	\item \ref{sec:coot_exp}: comparison with COOT clustering.
	\item \ref{sec:full_sensitivity}: complete scores on all datasets.
	\item \ref{sec:supp_hom_vs_sil}: study homogeneity vs silhouette score for various numbers of prototypes.
	\item \ref{sec:supp_hom_vs_nmi}: study homogeneity vs k-means score for various numbers of prototypes.
	\item \ref{sec:compute_time}: computation time study.
	\item \ref{sec:hyperbolic}: Proofs of concepts with hyperbolic DR kernels.
\end{itemize}


\subsection{Algorithmic details}\label{sec:algorithms}

We detail in the following the algorithms mentioned in Section \ref{sec:DDR} to address the semi-relaxed GW divergence computation in our Block Coordinate Descent algorithm for the DistR problem. We begin with details on the computation of an equivalent objective function and its gradient, potentially under low-rank assumptions over structures $\mC$ and $\overline{\mC}$.

\subsubsection{Objective function and gradient computation.}\label{subsec:GWloss} 

\paragraph{Problem statement.} Let consider any matrices $\mC \in \R^{n \times n}$, $\overline{\mC} \in \R^{m \times m}$, and a probability vector $\vh \in \Sigma_n$. In all our use cases,  we considered inner losses $L: \R \times \R \rightarrow \R_+$ which can be decomposed following Proposition 1 in \citep{peyre2016gromov}. Namely we assume the existence of functions $f_1, f_2, h_1, h_2$ such that 
\begin{equation}\label{eq:loss_decomposition}
\forall a, b \in \Omega^2, \quad L(a, b) = f_1(a) + f_2(b) - h_1(a) h_2(b)
\end{equation}
More specifically we considered
\begin{equation} \tag{L2}\label{eq:L2_loss}
\begin{split}
	L_2(a,b) = (a-b)^2 &\implies f_1(a) = a^2, \: f_2(b) = b^2, \: h_1(a) = a, \: h_2(b) = 2b \\
	L_{KL}(a,b ) = a \log \frac{a}{b} - a +b &\implies f_1(a) = a \log a -a, \: f_2(b) = b, \: h_1(a) = a, \: h_2(b) = \log b \\
\end{split}
\end{equation}
In this setting, we proposed to solve for the equivalent problem to $\srGW_L$ :
\begin{equation}\label{eq:srgw_eqpb}\tag{srGW-2}
\min_{\mT \in \mathcal{U}_n(\vh)} F(\mT)
\end{equation}
where the objective function reads as 
\begin{equation}\label{eq:expressionF}
\begin{split}
	F(\mT) &:= \scalar{F_1(\overline{\mC}, \mT) - F_2(\mC, \overline{\mC}, \mT)}{\mT} \\ 
	&= \scalar{\bm{1}_N \bm{1}_N^\top \mT f_2(\overline{\mC})}{ \mT} - \scalar{h_1(\mC) \mT h_2(\overline{\mC})^\top}{\mT}
\end{split}
\end{equation}

Problem \ref{eq:srgw_eqpb} is usually a non-convex QP with Hessian $\mathcal{H} = f_2(\overline{\mC}) \otimes \bm{1}\bm{1}^\top - h_2(\overline{\mC}) \otimes_K h_1(\mC)$. In all cases this equivalent form is interesting as it avoids computing the constant term $\scalar{f_1(\mC)}{\vh \vh^\top} $ that requires $O(N^2)$ operations in all cases.

The gradient of $F$ w.r.t $\mT$ then reads as 
\begin{equation}\label{eq:srgw_gradient}
\nabla_{\mT} F(\mC, \overline{\mC}, \mT) = F_1(\overline{\mC}, \mT) +  F_1(\overline{\mC}^\top, \mT) - F_2(\mC, \overline{\mC}, \mT) -F_2(\mC^\top, \overline{\mC}^\top, \mT) 
\end{equation}
When $C_X(\mX)$ and $C_Z(\mZ)$ are symmetric, which is the case in all our experiments,  this gradient reduces to $\nabla_{\mT} F = 2(F_1 - F_2)$.

\paragraph{Low-rank factorization.} Inspired from the work of \citep{scetbon2022linear}, we propose implementations of $\srGW$ that can leverage the low-rank nature of $\mC_X(\mX)$ and $\mC_Z(\mZ)$. Let us assume that both structures can be exactly decomposed as follows, $\mC_X(\mX) = \mA_1 \mA_2^\top$ where $\mA_1, \mA_2 \in \R^{N \times r}$ and $\mC_Z(\mZ) = \mB_1 \mB_2^\top$ with $\mB_1, \mB_2 \in \R^{n \times s}$, such that $r << N$ and $s << n$, can differ respectively from respective dimensions $p$ and $d$ (\eg for used squared Euclidean distance matrices $r=p+2$ and $s=d +2$ ). For both inner losses $L$ we make use of the following factorization:

\underline{$L=L_2$}: Computing the first term $F_1$ coming for the optimized second marginal can benefit from being factored if $d^2 << n$. Indeed, as $f_2(\mC_Z(\mZ)) = \mC_Z(\mZ)^2 = (\mB_1 \mB_2^\top) \odot (\mB_1 \mB_2^\top)$, one can use the flattened out product operator described in \cite[Section 5]{scetbon2022linear}, to compute $\mC_Z(\mZ)^2 \mT^\top \bm{1}_N = \vx$ in $O(min(n^2, ns^2))$. This way $F_1(\mT)$ results from stacking $N$ times $\vx$ in $O(1)$ operations for a total number of computions of $N + O(min(n^2, ns^2))$.%Hence computing $F_2(\cdot, \cdot, \mT)$,  following this paranthesis $\bm{1}_N (( \mT \bm{1}_n)^\top) f_2(\overline{\mC})^\top)$ in $O(Nn + n^2)$ operation.
And its scalar product with $\mT$ to compute the loss comes down to $O(Nn)$ additional operations. \\
Then computing $F_2(\mT)$ and its scalar product with $\mT$ can be done following the development of \cite[Section 3]{scetbon2022linear} for the corresponding GW problem, in $O(Nn(r + s) + rs(N+n))$ operations. So the overall complexity at is $O(Nn(r + s) + rs(N+n) + min(n^2, ns^2))$.


\underline{$L=L_{KL}$}: In this setting $f_2(\mC_Z(\mZ)) = \mC_Z(\mZ)$ and $h_1(\mC_X(\mX)) = \mC_X(\mX)$ naturally preserves the low-rank nature of input matrices, but $h_2(\mC_Z(\mZ)) = \log(\mC_Z(\mZ))$ does not. So computing the first term $F_1$, can be performed following this paranthesis order $\bm{1}_N ((  \bm{1}_N^\top \mT) \mA_1)) \mA_2^\top) $ in $O(N(n + s))$ operations. While the second term $F_2$ should be computed following this order $\mA_1 ((\mA_2^\top \mT)  \log(\mC_Z(\mZ)))$ in $O(Nnr + rn^2)$ operations. While their respective scalar product can be computed in $O(Nn)$. So the overall complexity is $O(Nnr + n^2r)$.\\
Notice that in the gaussian kernel case for neighbor embedding methods, where $[\mC_Z(\mZ)]_{ij} = \exp(-\|\vz_i-\vz_j\|_2^2)$ up to some normalization. We have $[h_2(\mC_Z(\mZ))]_{ij} = -\|\vz_i-\vz_j\|_2^2$ which admits a low-rank factorization such that we can recover the complexity illustrated above for $L=L_2$.

\subsubsection{Solvers.}
We develop next our extension of both the Mirror Descent and Conditional Gradient solvers first introduced in \citep{vincent2021semi}, for any inner loss $L$ that decomposes as in \eqref{eq:loss_decomposition} .

\paragraph{Mirror Descent algorithm.} This solver comes down to solve for the \emph{exact} srGW problem using mirror-descent scheme w.r.t the KL geometry. At each iteration $(i)$, the solver comes down to, first computing the gradient $\nabla_{\mT}F(\mT^{(i)})$ given in \eqref{eq:srgw_gradient} evaluated in $\mT^{(i)}$, then updating the transport plan using the following closed-form solution to a KL projection:

\begin{equation}
\mT^{(i+1)} \leftarrow \diag\left( \frac{\vh}{\mK^{(i)}\bm{1}_n} \right) \mK^{(i)}
\end{equation}
where $\mK^{(i)} = \exp \left(\ \nabla_{\mT}F(\mT^{(i)}) - \varepsilon \log(\mT^{(i)})  \right)$ and $\varepsilon > 0$ is an hyperparameter to tune. Proposition 3 and Lemma 7 in \cite[Chapter 6]{vincent2023optimal} provides that the Mirror-Descent algorithm converges to a stationary point non-asymptotically when $L=L_2$. A quick inspection of the proof suffices to see that this convergence holds for any losses $L$ satisfying \eqref{eq:loss_decomposition}, up to adaptation of constants involved in the Lemma.

\paragraph{Conditional Gradient algorithm.} This algorithm, known to converge to local optimum \citep{lacoste2016convergence}, iterates over the 3 steps summarized in Algorithm \ref{alg:CGsolver}:
.\begin{algorithm}[H]
\caption{CG solver for $\srGW_L$\label{alg:CGsolver}}
\begin{algorithmic}[1]
	\REPEAT
	\STATE $\mF^{(i)}\leftarrow $  Compute gradient  w.r.t $\mT$ of \eqref{eq:srgw_gradient}.
	\STATE $\mX^{(i)}\leftarrow \min_{\substack{\mX\bm{1}_m =\vh\\ \mX \geq 0}} \scalar{\mX}{\mF^{(i)}} $%: Solve independent subproblems on rows of $\mG^{(t)}$. 
	\STATE $\mT^{(i+1)} \leftarrow (1-\gamma^\star)\mT^{(i)} + \gamma^\star \mX^{(i)}$   with $\gamma^\star \in [0,1]$ from exact-line search.
	\UNTIL{convergence.}
\end{algorithmic}
\end{algorithm}

This algorithm consists in solving at each
iteration $(i)$ a linearization $\scalar{\mX}{\mF^{(i)}}$ of the problem \eqref{eq:srgw_eqpb}
where $\mF(\mT^{(i)})$ is the gradient of the objective in \eqref{eq:srgw_gradient}.  The solution of the linearized problem provides a \emph{descent direction} $\mX^{(i) }-\mT^{(i)}$, and a line-search whose optimal step can be found in closed form to
update the current solution $\mT^{(i)}$. We detail in the following this line-search step for any loss that can be decomposed as in \eqref{eq:loss_decomposition}. It comes down for any $\mT \in \mathcal{U}_n(\vh)$, to solve the following problem:
\begin{equation}
\gamma = \argmin_{\gamma \in [0,1]} g(\gamma) := F(\mT + \gamma (\mX - \mT))
\end{equation}
Observe that this objective function can be developed as a second order polynom $g(\gamma) = a \gamma^2 + b \gamma +c$. To find an optimal $\gamma$ it suffices to express coefficients $a$ and $b$ to conclude using Algorithm 2 in \citep{vayer2018optimal}.


Denoting $\mX^\top \bm{1}_n = \vq_X$ and $\mT^\top \bm{1}_n = \vq_T$ and following \eqref{eq:expressionF}, we have
\begin{equation}
\begin{split}
	a &= \scalar{ \bm{1}_{n} (\vq_X - \vq_T)^\top f_2(\overline{\mC})^\top - h_1(\mC) (\mX- \mT) h_2(\overline{\mC})^\top}{\mX - \mT} \\
	&= \scalar{F_1(\mX) - F_1(\mT) - F_2(\mX) + \mF(\mT)}{\mX - \mT} 
\end{split}
\end{equation}

Finally the coefficient $b$ of the linear term is
\begin{equation}
\begin{split}
	b &=  \langle  F_1(\mT) - F_2(\mT) , \mX-\mT \rangle + \langle  F_1(\mX - \mT) - F_2(\mX- \mT), \mT  \rangle \\
\end{split}
\end{equation}



\subsection{Experimental setting}\label{sec:implementation_details}

\paragraph{Implementation.} 
Throughout, the spectral clustering implementation of \texttt{scikit-learn} \citep{pedregosa2011scikit} is used to perform either the clustering steps or the initialization of transport plans. 
For all methods, $\mZ$ is initialized from \emph{i.i.d.} sampling of the standard Gaussian distribution $\mathcal{N}(0,1)$ and further optimized using \texttt{PyTorch}'s automatic differentiation \citep{paszke2017automatic} with Adam optimizer \citep{kingma2014adam}. OT-based solvers are built upon the \texttt{POT} \citep{flamary2021pot} library.
k-Means is performed using the \texttt{scikit-learn}~\citep{pedregosa2011scikit} implementation.

\paragraph{Validated hyperparameters.} For the SEA based similarities, we validated \texttt{perplexity} across the set $\{20, 50, 100, 150, 200, 250\}$. For all kernels, the number of output samples $n$ spans a set of $10$ values, starting at the number of classes in the data and incrementing in steps of $20$. For the computation of $\mT$ in DistR (see \Cref{sec:DDR_ob}), we benchmark our Conditional Gradient solver, and the Mirror Descent algorithm whose hyperparameter $\varepsilon$ is validated in the two first values within the set $\{10^{i}\}_{i=-3}^3$ leading to stable optimization.

\paragraph{Datasets.}
We provide details about the datasets used in our study. 
For image datasets, we use COIL-20\footnote{\url{https://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php}} \citep{nene1996columbia}, MNIST and fashion-MNIST\footnote{taken from Torchvision \citep{marcel2010torchvision}.} \citep{xiao2017fashion}. Regarding single-cell genomics datasets, we rely on PBMC 3k\footnote{downloaded from Scanpy \citep{wolf2018scanpy}.} \citep{wolf2018scanpy}, SNAREseq\footnote{\url{https://github.com/rsinghlab/SCOT}} chromatin and gene expression \citep{chen2019high} and the scRNA-seq dataset\footnote{\url{https://github.com/solevillar/scGeneFit-python}} from \citep{zeisel2015cell} with two hierarchical levels of label. 
Dimensions are provided in \Cref{tab:dataset_details}
When the dimensionality of a dataset exceeds $50$, we pre-process it by applying a PCA in dimension $50$, as done in practice \citep{van2008visualizing}.

\begin{table}[h!] \vspace{-5mm}
\centering
\caption{Dataset Sizes.}\label{tab:dataset_details}
\scalebox{0.8}{
	\begin{tabular}{|c||c|c|c|} \hline
		& Number of samples & Dimensionality & Number of classes \\ \hline \hline
		MNIST & $10000$ & $784$ & $10$ \\ \hline
		F-MNIST & $10000$ & $784$ & $10$ \\ \hline
		COIL & $1440$ & $16384$ & $20$ \\ \hline
		SNAREseq (chromatin) & $1047$ & $19$ & $5$ \\
		\hline
		SNAREseq (gene expression) & $1047$ & $10$ & $5$ \\
		\hline
		Zeisel & 3005 & 5000 & (8, 49) \\
		\hline
		PBMC & 2638 & 1838 & 8 \\
		\hline
\end{tabular}}
\end{table}


\subsection{Comparison with COOT clustering}\label{sec:coot_exp}

The CO-Optimal-Transport (COOT) problem, proposed in \citep{redko2020co}, is as follows,
\begin{align}\tag{COOT}
	\label{eq:coot_pb}
	\min_{\mT_r \in \gU(\vh_r, \overline{\vh}_r)} \min_{ \mT_c \in \gU(\vh_c, \overline{\vh}_c)} \: \sum_{ijkl} (X_{ik} - Z_{jl})^2 [\mT_r]_{ij} [\mT_c]_{kl} \,,
\end{align}
where $\vh_r \in \Sigma_N$, $\overline{\vh}_r \in \Sigma_n$, $\vh_c \in \Sigma_p$ and $\overline{\vh}_c \in \Sigma_d$. 
One can seek to optimize the above objective with respect to $\mZ$ to obtain a competitor method to DistR. This problem is called COOT clustering in \citep{redko2020co}. In the latter, $\mT_r$ then plays the role of a soft clustering matrix of the rows of $\mX$ while $\mT_c$ can be seen as a soft clustering matrix of its columns. The above is thus a linear DR model.


\begin{table}[h!]
	\centering
	\scalebox{0.6}{
		\begin{tabular}{|c|c|c||c|c|c|c|c|c|} \hline
			$\mathcal{Z}$ & methods & $\mC_X$ / $\mC_Z$ & COIL & MNIST & FMNIST & PBMC & SNA1 & SNA2 \\ \hline \hline
			\multirow{4}{*}{$\mathbb{R}^{2}$}
			& DistR (ours) & SEA / St. & 99.50 (0.60) & 97.80 (0.00) & 93.40 (0.00) & 97.10 (0.00) & 100.00 (0.00) & 100.00 (0.00) \\
			& DR$\to$C & - & 98.10 (0.50) & 93.30 (3.40) & 94.30 (2.80) & 96.30 (0.90) & 100.00 (0.00) & 100.00 (0.00) \\
			& C$\to$DR & - & 100.00 (0.00) & 97.80 (0.00) & 93.40 (0.00) & 97.10 (0.00) & 100.00 (0.00) & 100.00 (0.00) \\
			& COOT & NA & 43.90 (0.70) & 9.80 (3.00) & 8.50 (0.90) & 15.90 (1.90) & 44.00 (4.60) & 49.70 (8.60) \\
			\hline \hline
			\multirow{4}{*}{$\mathbb{R}^{10}$}
			& DistR (ours) & $\langle, \rangle_{\R^p}$ / $\langle, \rangle_{\R^d}$ & 96.80 (0.70) & 97.00 (0.90) & 93.40 (0.00) & 97.10 (0.40) & 80.50 (0.00) & 100.00 (0.00) \\
			& DR$\to$C & - & 73.30 (1.80) & 98.30 (3.40) & 93.20 (1.90) & 90.20 (1.40) & 72.70 (6.00) & 90.00 (20.00) \\
			& C$\to$DR & - & 83.70 (0.00) & 100.00 (0.00) & 93.40 (0.00) & 93.30 (0.00) & 80.50 (0.00) & 100.00 (0.00) \\
			& COOT & NA & 45.50 (1.60) & 13.70 (2.10) & 9.30 (2.80) & 16.10 (2.40) & 45.60 (5.90) & 76.50 (16.60) \\
			\hline
	\end{tabular}}
	\vspace{0.2cm}
	\caption{Best homogeneity scores for $n$ validated in a span up to $200$ with increments of $20$.}
	\label{tab:coot_scores}
	% \vspace{-0.5cm}
\end{table}
In \Cref{tab:coot_scores}, we display the homogeneity values obtained with COOT along with the methods described \Cref{sec:exps}. Precisely, it measures to what extent the clustering given by $\mT_r$ groups points with the same ground truth label. One can notice that COOT falls short compared to its competitors that leverage affinity matrices as in state-of-the-art (non-linear) DR methods.


\subsection{Complete scores}\label{sec:full_sensitivity}

We complete the results shown in \Cref{sec:exps} by providing the scores obtained on all datasets and models. Scores are plotted in \Cref{fig:sensitivity_sne} for the t-SNE model and in \Cref{fig:sensitivity_ip} for the PCA model. 
\begin{figure*}[h!]
	\begin{center}
		\centerline{\includegraphics[width=0.9\columnwidth]{figures/DistR/sensitivity_SNE_dim_2/full_sensitivity.pdf}}
		\caption{Scores ($\times 100$) with respect to the number of prototypes (in $\R^2$) produced by DistR using the t-SNE model: SEA similarity for $\simiX$ \citep{van2023snekhorn}, Student's kernel for $\simiZ$ and loss $L_{\KL}$.}
	\label{fig:sensitivity_sne}
	\end{center}
\end{figure*}
% \newpage

\begin{figure*}[h!]
	\begin{center}
		\centerline{\includegraphics[width=0.9\columnwidth]{figures/DistR/sensitivity_IP_dim_10/full_sensitivity.pdf}}
		\caption{Scores ($\times 100$) with respect to the number of prototypes (in $\R^{10}$) produced by DistR using the PCA model: $\langle, \rangle_{\R^p}$ similarity for $\simiX$ \citep{van2023snekhorn}, $\langle, \rangle_{\R^d}$ for $\simiZ$ and loss $L_{2}$.}
	\label{fig:sensitivity_ip}
	\end{center}
	\vspace{-0.8cm}
\end{figure*} 
% \newpage


\subsection{Dynamics between homogeneity and silhouette scores across numbers of prototypes}\label{sec:supp_hom_vs_sil}
For each method depending on some hyperparameters, models performing the best on average across all numbers of prototypes are illustrated in \Cref{fig:PCA_hom_vs_sil} for spectral methods and in \Cref{fig:SNE_hom_vs_sil} for neighbor embedding ones. 
	\begin{figure*}[h!]
		\begin{center}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/legend.pdf}}\vspace{-1mm}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/scatterplot_PCA_mean_sum_percentile_normalized_by_dataset.pdf}
			}
			\caption{Trade-off between homogeneity vs silhouette scores using PCA model across various numbers of prototypes $n$. The illustration follows the same principal than \cref{fig:trade_off}}
			\label{fig:PCA_hom_vs_sil}
		\end{center}
	\end{figure*}
	\newpage
	\begin{figure*}[h!]
		\begin{center}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/legend.pdf}}\vspace{-1mm}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/scatterplot_t-SNE_mean_sum_percentile_normalized_by_dataset.pdf}
			}
			\caption{Trade-off between homogeneity vs silhouette scores using t-SNE model across various numbers of prototypes $n$. The illustration follows the same principal than \cref{fig:trade_off}}
			\label{fig:SNE_hom_vs_sil}
		\end{center}
		\vspace{-0.8cm}
	\end{figure*}
	% \newpage

	\subsection{Dynamics between homogeneity and NMI scores across numbers of prototypes}\label{sec:supp_hom_vs_nmi}
	For each method depending on some hyperparameters, models performing the best on average across all numbers of prototypes are illustrated in \Cref{fig:PCA_hom_vs_nmi} for spectral methods and in \Cref{fig:SNE_hom_vs_nmi} for neighbor embedding ones. 
	\begin{figure*}[h!]
		\begin{center}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/legend.pdf}}\vspace{-1mm}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/kmeansscatterplot_PCA_mean_sum_percentile_normalized_by_dataset.pdf}
			}
			\caption{Trade-off between homogeneity vs silhouette scores using PCA model across various numbers of prototypes $n$. The illustration follows the same principal than \cref{fig:trade_off}}
			\label{fig:PCA_hom_vs_nmi}
		\end{center}
		\vspace{-0.8cm}
	\end{figure*}
	\begin{figure*}[h!]
		\begin{center}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/legend.pdf}}\vspace{-1mm}
			\centerline{\includegraphics[width=\columnwidth]{figures/DistR/scatter_plots/kmeansscatterplot_t-SNE_mean_sum_percentile_normalized_by_dataset.pdf}
			}
			\caption{Trade-off between homogeneity vs silhouette scores using t-SNE model across various numbers of prototypes $n$. The illustration follows the same principal than \cref{fig:trade_off}}
			\label{fig:SNE_hom_vs_nmi}
		\end{center}
		\vspace{-0.8cm}
	\end{figure*}
	% \newpage


\begin{comment}
	\subsection{Assessing clustering performances with NMI}
	To complement our analysis, we also propose to assess clustering performances using the Normalized Mutual Information (NMI) score \citep{kvaalseth2017normalized}.
	
	$\mathcal{N}$ (NMI) scores are provided in \cref{tab:nmi_scores} and $\overline{\mathcal{SN}}$ scores in \cref{tab:SN_scores}. For each table, each value displayed is the one obtained with the best configuration (best choice of hyperparameters) maximizing the score of interest.
	
	\textbf{Analysis.} For $\overline{\mathcal{SN}}$, our approaches obtain the best results for $80\%$ of the considered datasets and affinities, showing consistency with the results obtained in \cref*{tab:benchmark_distdr} with $\overline{\mathcal{SH}}$.
\end{comment}



\subsection{Computation time comparison}\label{sec:compute_time}

We compare in the following the computation time for all methods benchmarked in Table 1 when using a spectral method and a SNE method. All experiments were done on a server using a GPU (Tesla V100-SXM2-32GB) and composed of 18 cores Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz. All DR steps benefit from our GPU compatible implementation, while spectral clustering (SC) was performed on CPU using scikit-learn implementation running on CPUs.\\ Notice that to run our experiments we precomputed and saved SC steps for the maximum number of prototypes ran on the input structure $C_X(\mX)$, for all benchmarked methods e.g CDR used for clustering or DistR and COOT used for initialization of the transport plans. As DRC performs SC over learned embeddings using $C_Z(\mZ)$ it cannot be precomputed, so we include the time of performing SC for all $n$ for all methods to achieve a fair comparison. Results for three datasets of increasing sample sizes SNA1, ZEISEL and MNIST (See Appendix E.1) are reported in Figure \ref{fig:runtimes1} with pre-computation and in Figure \ref{fig:runtimes2}  without.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=\columnwidth]{figures/DistR/runtimes/IP_dim_10_runtimes_runtimes_with_precomputing.pdf}
        \includegraphics[width=\columnwidth]{figures/DistR/runtimes/SNE_dim_2_runtimes_runtimes_with_precomputing.pdf}
    \end{center}
    \caption{\label{fig:runtimes2}Computation time comparison depending on the number of prototypes $n$ for all methods over 3 datasets, for 5 different initializations, while precomputing spectral clustering on the input structure $C_X(\mX)$.}
\end{figure}
\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=\columnwidth]{figures/DistR/runtimes/IP_dim_10_runtimes_runtimes_without_precomputing.pdf}
        \includegraphics[width=\columnwidth]{figures/DistR/runtimes/SNE_dim_2_runtimes_runtimes_without_precomputing.pdf}
    \end{center}
    \caption{\label{fig:runtimes1}Computation time comparison depending on the number of prototypes $n$ for all methods over 3 datasets, for 5 different initializations,  \underline{without} precomputing spectral clustering on the input structure $C_X(\mX)$.}
\end{figure}

Overall our DistR models are competitive with all benchmarked methods in terms of computation time too and can be run (and further validated) in a few seconds using a GPU after precomputing the spectral embeddings for medium size dataset (N=10000 for MNIST), by efficiently leveraging low-rank kernels often used in the DR literature.



\subsection{Proofs of concepts with hyperbolic kernels}\label{sec:hyperbolic}

Hyperbolic spaces \citep{Chami21, Fan_2022_CVPR, Guo22, Lin23} are of particular interest as they can capture hierarchical structures more effectively than Euclidean spaces and mitigate the curse of dimensionality by producing representations with lower distortion rates. For instance, \citep{Guo22} adapted t-SNE by using the Poincaré distance and by changing the Student's t-distribution with a more general hyperbolic Cauchy distribution.  Notions of projection subspaces can also be adapted, \eg \citep{Chami21} use horospheres as one-dimensional subspaces. To match our experiments with the neighbor embeddings in Euclidean settings, we adapt the \emph{Symmetric Entropic Affinity} (SEA) from \Cref{chapter:SNEkhorn} for $\mP$ and the scalar-normalized student similarity for $\mQ$ \citep{van2008visualizing}, by simply changing the Euclidean distance by an hyperbolic distance.


\paragraph{Implementation details.} Computations in Hyperbolic spaces are done with \texttt{Geoopt}~\citep{geoopt2020kochurov} and the RAdam optimizer \citep{becigneul2018riemannian} replaces Adam. A Wrapped Normal distribution in Hyperbolic spaces~\citep{Nagano19} is used to initialize $\mZ$ in the hyperbolic setting.
All the computations were conducted in the Lorentz model~\citep{Nickel18}, which is less prone to numerical errors. As such, we used the associated distance function to form $\mC_Z$. After optimization, results are projected back to the Poincaré ball for visualization purposes. In this hyperbolic context, we adopted the formulation of~\citep{Guo22} which generalizes Student's t-distribution by Hyperbolic Cauchy distributions (denoted as H-Student in the results). Notice that \citep{Guo22} considered weighted sums of DR objective depending respectively on $L_2$ and $L_{KL}$, including 2 additional hyperparamaters, plus various validated curvature levels for the inner hyperbolic distances. In the following experiments, we only kept $L_{KL}$ for comparison with the Euclidean SNE-based methods illustrated in \Cref{sec:exps_distr} and previous Sections of \ref{sec:appendix_exps}, while validating the same hyperparameters and setting the space curvature to $1$. The silhouette score was adapted to this kernel considering the Hyperbolic distance instead of the Euclidean one, and we implemented a Hyperbolic Kmeans whose barycenters are estimated using the RAdam optimizer to compute the NMI scores.

\paragraph{Results.} We first report in \Cref{fig:trade_off_hyp} a relative comparison of the best trade-off between local and global metrics achieved by all methods. Similarly to visualizations of the main paper, we considered for each method and dataset, the model maximizing the sum of the two normalized metrics to account for their different ranges. DistR, being once again present on the top-right of all plots, provides on average the most discriminant
low-dimensional representations endowed with a simple geometry, seconded by
C$\rightarrow$DR. 

\begin{figure*}[h!]
	\begin{center}
		\centerline{\includegraphics[width=0.8\columnwidth]{figures/DistR/scatter_plots/legend.pdf}}\vspace{-1mm}
		\centerline{
			\includegraphics[width=0.3\columnwidth]{figures/DistR/scatter_plots/scatterplot_modelselectionmax_sum_t-HSNE_percentile_bestmodels_normalized_by_datasetV2.pdf}
			\includegraphics[width=0.3\columnwidth]{figures/DistR/scatter_plots/kmeansscatterplot_modelselectionmax_sum_t-HSNE_percentile_bestmodels_normalized_by_datasetV2.pdf}
		}
		\caption{Best trade-off between homogeneity vs silhouette (2 first plots), and homogeneity vs NMI (2 last plots). Scores are normalized in $\left[0, 1\right]$ via min-max scaling over a dataset. Small markers represent scores for 5 runs for a given dataset, while big ones are their mean. For each method we illustrate the 20-80\% percentiles of normalized scores as a colored surface.
		}
		\vspace{-0.5cm}
		\label{fig:trade_off_hyp}
	\end{center}
	\vspace{-0.3cm}
\end{figure*}

Then we report in \Cref{fig:sensitivity_hyp} absolute performances for all methods and all datasets across various $n$, as done in \Cref{sec:full_sensitivity}. We can observe that both DistR and C$\rightarrow$DR achieve fairly high NMI and homogeneity scores across all settings, while DistR performs significantly better on average across the tested number of prototypes $n$.  However, DR$\rightarrow$C struggles significantly to learn both globally and individually discriminant prototypes. Notice that DR$\rightarrow$C's homogeneity scores are significantly lower on average than both benchmarked Euclidean kernels. This mitigates drastically the significance of the silhouette scores computed for this method, letting essentially DistR and C$\rightarrow$DR to compare. Even though DistR outperforms consistently C$\rightarrow$DR w.r.t silhouette scores, these scores remain significantly lower than with the other t-SNE kernels. As the latter is equivalent to a null curvature, this indicates that further fine-tuning of the curvature within these hyperbolic kernels could be beneficial. Nevertheless, these results confirm the versatility of our DistR approach, capable of operating on non-Euclidean geometries.

\begin{figure*}[h]
	\begin{center}
		\centerline{\includegraphics[width=0.9\columnwidth]{figures/DistR/sensitivity_hyp_dim_2/full_sensitivity.pdf}}
		\caption{Scores ($\times 100$) with respect to the number of prototypes (in $\R^{10}$) produced by DistR using the Hyperbolic Student model.}
		\label{fig:sensitivity_hyp}
	\end{center}
	\vspace{-0.8cm}
\end{figure*} \newpage

