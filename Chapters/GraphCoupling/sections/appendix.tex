% Appendix Template

\section{Supplementary Material}\label{sec:supp_material_gc}

\subsection{Towards Capturing Large-Scale Dependencies}\label{sec:towards_large_scale}

In this section, we investigate the ability of graph coupling to faithfully represent the global structure in low dimensions. To gain intuition on the case where the distribution induced by the graph is not degenerate, we consider a proper Gaussian graph coupling model and show its equivalence with PCA. We then provide a new initialization procedure to alleviate the large-scale deficiency of graph coupling when degenerate MRFs are used.

\begin{figure}
    \centering
    \begin{tikzpicture}[
    roundnode_w/.style={circle, draw=black!60, fill=white, minimum size=8.5mm, line width=0.1mm},
    roundnode_g/.style={circle, draw=black!60, fill=black!15, minimum size=8.5mm, line width=0.1mm},
    squarednode/.style={rectangle, draw=black!60, fill=black!5, minimum size=7mm}
    ]
    %Nodes
    \node[roundnode_w]  (Theta_X)     {$\bm{\Theta}_{\scaleto{X}{4pt}}$};
    \node[roundnode_w]      (W_X)   [above left=of Theta_X]  {$\W_{\scaleto{X}{4pt}}$};
    \node[roundnode_g]        (X_C)     [below left =of Theta_X]  {$\X_{\scaleto{C}{4pt}}$};
    \node[roundnode_g]        (X_M)     [below =of Theta_X]  {$\X_{\scaleto{M}{4pt}}$};
    \node[roundnode_w]      (Theta_Z)       [right=of Theta_X]    {$\bm{\Theta}_{\scaleto{Z}{4pt}}$};
    \node[roundnode_w]      (W_Z)       [above right=of Theta_Z]    {$\W_{\scaleto{Z}{4pt}}$};
    \node[roundnode_g]        (Z_C)     [below right=of Theta_Z]  {$\Z_{\scaleto{C}{4pt}}$};
    \node[roundnode_g]        (Z_M)     [below =of Theta_Z]  {$\Z_{\scaleto{M}{4pt}}$};
    
    %Lines
    \draw[->] (W_X) -- (X_C);
    \draw[->] (W_X) -- (Theta_X);
    \draw[->] (Theta_X) -- (X_M);
    \draw[->] (W_Z) -- (Z_C);
    \draw[->] (W_Z) -- (Theta_Z);
    \draw[->] (Theta_Z) -- (Z_M);
    \draw[dotted,<->] (W_X) -- (W_Z);
    \draw[dotted,<->] (Theta_X) -- (Theta_Z);
    \end{tikzpicture}
    \caption{Graphical representation of the hierarchical model considered in \cref{sec:hierarchical_modelling}. Plain directed arrows represent conditional dependencies while dotted arrows represent the coupling links. \Cref{corollary_ccPCA} provides a solution for the coupling between $\bm{\Theta}_{\scaleto{X}{4pt}}$ and $\bm{\Theta}_{\scaleto{Z}{4pt}}$.}
    \label{fig:graphical_model_hierarchical}
\end{figure}

\subsubsection{Hierarchical Graph Coupling}\label{sec:hierarchical_modelling}

The goal of this section is to show that global structure in SNE-like embeddings can be improved by structuring the CCs' positions. We consider the following hierarchical model for $\Xb$, where $\mathcal{P}_{X} \in \{B,D,E\}$, $k_x$ satisfies the assumptions of \cref{prop:integrability_pairwise_MRF} and $\nu_{X} \geq n$:
\begin{align*}
    \Wb_{X} \sim \mathbb{P}_{\mathcal{P}_X,k_x}^{\varepsilon}(\cdot \: ; \bm{1},1), &\quad \bm{\Theta}_{X} | \Wb_{X} \sim \mathcal{W}(\nu_{X}, \bm{I}_{R}) \\
    \Xb_{C} |\Wb_{X} \sim \mathbb{P}_{k_x}(\cdot \:| \Wb_{X}), &\quad \mathrm{vec}(\Xb_{M}) | \bm{\Theta}_{X} \sim \mathcal{N}\left(\bm{0}, \left(\varepsilon \Ub_{[:R]}  \bm{\Theta}_{X}\Ub^\top_{[R]}\right)^{-1} \otimes \bm{I}_p\right)
\end{align*}
where $\Ub_{R}$ are the eigenvectors associated to the Laplacian null-space of $\overline{\Wb}_{X}$. Given a graph $\Wb_{X}$, the idea is to structure the CCs' relative positions with a full-rank Gaussian model.
The same model is considered for $\Wb_{Z}$, $\bm{\Theta}_{Z}$ and $\Zb$, choosing $\nu_{Z} = \nu_{X} + p - q$ for the Wishart prior to satisfy the assumption of \cref{PCA_graph_coupling}.  With this in place, we aim at providing a complete coupling objective, matching the pairs  $(\Wb_{X},\bm{\Theta}_{X})$ and  $(\Wb_{Z},\bm{\Theta}_{Z})$. The joint negative cross-entropy can be decomposed as follows:
\begin{align}
    &\mathbb{E}_{(\Wb_{X}, \bm{\Theta}_{X})|\Xb}\left[\log \mathbb{P}((\Wb_{Z},\bm{\Theta}_{Z}) = (\Wb_{X},\bm{\Theta}_{X}) | \Zb)\right] \nonumber\\
    &= \mathbb{E}_{\Wb_{X}|\Xb}\left[\log \mathbb{P}(\Wb_{Z} = \Wb_{X} | \Zb)\right] + \label{eq:loss_LW} \\
    & \mathbb{E}_{(\Wb_{X},\bm{\Theta}_{X})|\Xb}\left[ \log \mathbb{P}(\bm{\Theta}_{Z} = \bm{\Theta}_{X}| \Wb_{Z} = \Wb_{X}, \Zb) \right] \label{eq:add_term_Coupling}
\end{align}
where (\ref{eq:loss_LW}) is the usual coupling criterion of $\Wb_X$ and $\Wb_Z$ capturing intra-CC variability while (\ref{eq:add_term_Coupling}) is a penalty resulting from the Gaussian structure on $\mathcal{S}_{M}$. Constructed as such, the above objective allows a trade-off between local and global structure preservation. Following current trends in DR \citep{kobak2021initialization}, we propose to take care of the global structure first \textit{i.e.}\ focusing on (\ref{eq:add_term_Coupling}) before (\ref{eq:loss_LW}). The difficulty of dealing with (\ref{eq:add_term_Coupling}) lies in the hierarchical construction of the graph and the Gaussian precision (see \cref{fig:graphical_model_hierarchical}). We state the following result.

\begin{wrapfigure}[17]{R}{0.5\textwidth}
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/GraphCoupling/tSNE_truth.png}}
\caption{Left: MNIST t-SNE (perp: 30) embeddings initialized with i.i.d $\mathcal{N}(0,1)$ coordinates. Middle: using these t-SNE embeddings, mean coordinates for each digit are represented. Right: we compute a matrix of mean input coordinates for each of the $10$ digits and embed it using PCA. For t-SNE embeddings, the positions of clusters vary across different runs and don't visually match the PCA embeddings of input mean vectors (right plot).}
\label{fig:tSNE-clusters-truth}
\end{center}
\end{wrapfigure}

\begin{corollary}\label{corollary_ccPCA}
Let $\Wb_{X} \in \mathcal{S}_{W}$, $\bm{L} = L(\overline{\Wb}_{X})$ and $\mathcal{S}^q_{M}= (\ker \bm{L}) \otimes \mathbb{R}^q$, then for all $\varepsilon > 0$, given the above hierarchical model, the solution of the problem:
$$\min_{\Zb \in \mathcal{S}^q_{M}} \: -\mathbb{E}_{\bm{\Theta}_{X}| \Xb}\left[ \log \mathbb{P}(\bm{\Theta}_{Z} = \bm{\Theta}_{X}| \Wb_{Z} = \Wb_{X}, \Zb) \right]$$
is a PCA embedding of $\Ub_{[:R]}\Ub_{[R]}^\top\Xb$ where $\Ub_{[:R]}$ are the CCs' membership vectors of $\overline{\Wb}_{X}$.
\end{corollary}

\begin{remark}
Note that while (\ref{eq:loss_LW}) approximates the objective of SNE-like methods when $\varepsilon \to 0$, the minimizer of (\ref{eq:add_term_Coupling}) given by \cref{corollary_ccPCA} is stable for all $\varepsilon$.
\end{remark}

From this observation, we propose a simple heuristic to minimize (\ref{eq:add_term_Coupling}) that consists in computing a PCA embedding of $\mathbb{E}_{\mathbb{P}_{\mathcal{P}_X}(\cdot;\Kb_{X})}\left[ \Ub_{[:R]}\Ub_{[R]}^\top \right]\Xb$. The distribution of the connected components of the posterior of $\Wb_{X}$ being intractable, we resort to a Monte-Carlo estimation of the above expectation. The latter procedure called \textit{ccPCA} aims at recovering the inter-CC structure that is filtered by SNE-like methods. \textit{ccPCA} may then be used as initialization for optimizing (\ref{eq:loss_LW}) which is done by running the DR method corresponding to the graph priors at hand (\cref{sec:retrieving_DR_methods}). This second step essentially consists in refining the intra-CC structure. 

\subsubsection{Experiments with \textit{ccPCA}}\label{sec:ccPCA}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/GraphCoupling/cluster_positions.png}}
\caption{Top: MNIST embeddings produced by PCA, Laplacian eigenmaps, \textit{ccPCA} and finally t-SNE launched after the previous three embeddings to improve the fine-grain structure. Bottom: mean coordinates for each digit using the embeddings of the first row. The color legend is the same as in \cref{fig:tSNE-clusters-truth}. t-SNE was trained during $1000$ iterations using default parameters with the openTSNE implementation \citep{polivcar2019opentsne}.}
\label{fig:methods_embeddings}
\end{center}
\end{figure*}

\Cref{fig:tSNE-clusters-truth} shows that a t-SNE embedding of a balanced MNIST dataset of 10000 samples \citep{deng2012mnist} with isotropic Gaussian initialization performs poorly in conserving the relative positions of clusters. As each digit cluster contains approximately $1000$ points, with a perplexity of $30$, sampling an edge across digit clusters in the graph posterior $\mathbb{P}_{\mathcal{P}_X}(\cdot;\Kb_{X})$ is very unlikely. Recall that the perplexity value \citep{maaten2008tSNE} corresponds to the approximate number of effective neighbors of each point. Hence images of different digits are with very high probability in different CCs of the graph posterior and their CC-wise means are not coupled as discussed in \cref{sec:interpretations}. To remedy this in practice, PCA or Laplacian eigenmaps are usually used as initialization \citep{kobak2021initialization}. 

These strategies are tested (\cref{fig:methods_embeddings}) together with \textit{ccPCA}. This shows that 
\textit{ccPCA} manages to retrieve the digits that mostly support the large-scale variability as measured by the peripheral positioning of digits $0$ (blue), $2$ (green), $6$ (pink) and $7$ (grey) given by the right side of \cref{fig:tSNE-clusters-truth}. Other perplexity values for \textit{ccPCA} are explored in appendix \ref{sec:other_perp} while the experimental setup is detailed in appendix \ref{sec:setup_exp}. In appendix \ref{sec:quantitative_evaluation}, we perform quantitative evaluations of \textit{ccPCA} for both t-SNE and UMAP on various datasets using K-ary neighborhood criteria. We find that using \textit{ccPCA} as initialization is in general more reliable than PCA and Laplacian eigenmaps for preserving global structure using both t-SNE and UMAP. 

Compared to PCA, \textit{ccPCA} manages to aggregate points into clusters, thus filtering the intra-cluster variability and focusing solely on the inter-cluster structure. Compared to Laplacian eigenmaps which perform well at identifying clusters but suffer from the same deficiency as t-SNE for positioning them, \textit{ccPCA} retains more of the coarse-grain structure. These observations support our unifying probabilistic framework and the theoretical results about the MRF degeneracy which are the leading contributions of this article. The \textit{ccPCA} initialization appears as a first stepping stone towards more grounded DR methods based on the probabilistic model presented in this article.

\subsection{Proof of \Cref{PCA_graph_coupling}}

\PCAgraphcoupling*

\begin{proof}
We consider the following hierarchical model, for $\nu_{X}, \nu_{Z} \geq N$:
\begin{align*}
    \bm{\Theta}_{X} &\sim  \mathcal{W}(\nu_{X}, \bm{I}_N) \\
    \mathrm{vec}(\X) | \bm{\Theta}_{X} &\sim \mathcal{N}(\bm{0}, \bm{\Theta}_{X}^{-1} \otimes \bm{I}_p) \\
    \bm{\Theta}_{Z} &\sim  \mathcal{W}(\nu_{Z}, \bm{I}_N) \\
    \mathrm{vec}(\Z) | \bm{\Theta}_{Z} &\sim \mathcal{N}(\bm{0}, \bm{\Theta}_{Z}^{-1} \otimes \bm{I}_q) \:.
\end{align*}
With this at hand, the posteriors for $\bm{\Theta}_X$ and $\bm{\Theta}_Z$ can be derived in closed form: 
\begin{align*}
    \bm{\Theta}_{X} | \X &\sim  \mathcal{W}(\nu_{X}+p, \left(\bm{I}_N + \X \X^\top\right)^{-1}) \\
    \bm{\Theta}_{Z} | \Z &\sim  \mathcal{W}(\nu_{Z} + q, \left(\bm{I}_N + \Z \Z^\top\right)^{-1}) \:.
\end{align*}

Keeping terms of $-\mathbb{E}_{\bm{\Theta}_{X}}[\log \mathbb{P}(\bm{\Theta}_{Z} = \bm{\Theta}_{X}| \Z )| \X]$ that depends on $\Z$, one has the optimization problem:
\begin{align*}
    \min_{\Z \in \mathbb{R}^{n \times q}} \quad \frac{\nu_{X}+p}{2}\operatorname{tr}\left(\Z^\top(\bm{I}_N +  \X\X^\top)^{-1}\Z\right) - \frac{\nu_{Z}+q}{2}\log |\bm{I}_N +  \Z\Z^\top|
\end{align*}
Consider the eigendecomposition of the sample covariance matrices: $\X\X^\top = \bm{V D} \bm{V}^\top$ and $\Z\Z^\top = \bm{U \Lambda} \bm{U}^\top$ where $\bm{D}=\operatorname{diag}(\bm{d})$ and $\bm{\Lambda}=\operatorname{diag}(\bm{\lambda})$ such that $d_1 \geq ... \geq d_N$ and $\lambda_1 \geq ... \geq \lambda_N$. Denoting $\gamma = (\nu_{X}+q)/(\nu_{Z}+p)$, we consider the following problem:
\begin{align}
   \min_{\bm{U} \in \mathcal{O}(N), \bm{\Lambda}} \quad & \operatorname{tr}\left(\bm{U} \bm{\Lambda} \bm{U}^\top \bm{V} (\bm{I}_N + \bm{D})^{-1} \bm{V}^\top\right) - \gamma \log |\bm{I}_N + \bm{\Lambda}| \label{eq:optim_eigenvalues_eigenvectors} \\
    \textrm{s.t.} \quad & \bm{\Lambda} \succcurlyeq \bm{0} \label{eq:positive_definite_constraint}\\
    & \operatorname{rank}(\bm{\Lambda}) \leq q \label{eq:rank_constraint}
\end{align}
Note that the above problem is non-convex because of the rank constraint (\ref{eq:rank_constraint}). 

First, we focus on finding the optimal eigenvectors. To that extent, let us denote, $\bm{R} = \bm{U}^\top\bm{V}$. Only the left term in (\ref{eq:optim_eigenvalues_eigenvectors}) depends on $\bm{R}$. The optimization problem for eigenvectors writes:
\begin{align}
   \min_{\bm{R} \in \mathcal{O}(N)} \quad & \operatorname{tr}\left(\bm{R}^\top \bm{\Lambda} \bm{R} (\bm{I}_N + \bm{D})^{-1} \right) \label{eq:optim_eigenvalues_eigenvectors}
\end{align}
The objective (\ref{eq:optim_eigenvalues_eigenvectors}) can be expressed as: $\sum_{(i,j) \in \integ{N}^2} \lambda_i (1 + d_j)^{-1} R_{ij}^2$. Now one can notice that since $\bm{R}$ is orthogonal, $\bm{R} \odot \bm{R}$ is doubly stochastic (\textit{i.e.}\ sum of coefficients on each row and column is equal to one). Therefore thanks to the Birkhoffâ€“von Neumann theorem, there exists $\theta_1, ..., \theta_L \geq 0$, $\sum_{\ell \in \integ{L}} \theta_\ell = 1$ and permutation matrices $\bm{P}_1, ..., \bm{P}_L$ such that:
$$\bm{R} \odot \bm{R} = \sum_{\ell \in \integ{L}} \theta_\ell \bm{P}_\ell$$
where for all $\ell \in \integ{L}$, there exists a permutation $\sigma_\ell$ of $\integ{N}$ such that $P_{\ell,ij} = \ind_{\sigma_{\ell}(i) = j}$ for $(i,j) \in \integ{N}^2$. 

With this at hand, objective (\ref{eq:optim_eigenvalues_eigenvectors}) writes: $\sum_{\ell \in \integ{L}} \theta_\ell \sum_{i \in \integ{N}} \lambda_i (1 + d_{\sigma_\ell(i)})^{-1}$. There exists a permutation $\sigma^\star$ such that the quantity $\sum_{i \in \integ{N}} \lambda_i (1 + d_{\sigma_\ell(i)})^{-1}$ is minimal. Note that the identity permutation \textit{i.e.}\ for $i \in \integ{N}$, $ \sigma(i) = i$ is optimal in this case as the $(\lambda_i)_{i \in \integ{N}}$ and the $(d_i)_{i \in \integ{N}}$ are in decreasing order. Then choosing for $\ell \in \integ{L}$, $\theta_\ell = \ind_{\sigma_\ell = \sigma^\star}$ minimizes the latter quantity. Therefore the solution of (\ref{eq:optim_eigenvalues_eigenvectors}) $\bm{R}^{\star}$ is such that for $(i,j) \in \integ{N}^2$, $R^\star_{ij} = \pm \ind_{\sigma^\star(i)=j}$. Thus an optimum in $\bm{U}$ of $\ref{eq:optim_eigenvalues_eigenvectors}$ is such that $\bm{U}^\star = \bm{V} \bm{R}^\star$. 

Hence $\bm{U} = \bm{V}$, in particular, is optimal. We will choose this $\bm{U}$ in what follows as the sign of the axes do not influence the characterization of the final result in $\Z$ as a PCA embedding. Such a choice gives $\Z \Z^\top = \bm{V} \bm{\Lambda} \bm{V}^\top$. 

Now it remains to find the optimal eigenvalues $(\lambda_i)_{i \in \integ{N}}$. The rank constraint (\ref{eq:rank_constraint}) can be easily dealt with: since the eigenvalues are sorted in decreasing order, the constraint implies that for $i \geq q$, $\lambda_i=0$.  Thus the eigenvalue problem can be formulated in $\mathbb{R}^q$:
\begin{align}
    \min_{\bm{\lambda} \in \mathbb{R}^q} \quad & \bm{\lambda}^\top (\bm{1} + \bm{d})^{-1} - \gamma \bm{1}^\top \log (\bm{1} + \bm{\lambda}) \label{eq:objective_lambda}\\
    \textrm{s.t.} \quad & \forall i \in [q], \quad  \lambda_i \geq 0 , \quad \lambda_1 \geq ... \geq \lambda_q \label{eq:feasibility_lambda}
\end{align}
where (\ref{eq:feasibility_lambda}) accounts for (\ref{eq:positive_definite_constraint}). The above is convex. (\ref{eq:objective_lambda}) is minimized for $\bm{\lambda} = \gamma (\bm{1} + \bm{d}) - \bm{1}$. Taking the feasibility constraint (\ref{eq:feasibility_lambda}) into account one has a solution $\bm{\lambda}^*$ such that:
$$\forall i \in \integ{N}, \quad 
\lambda_i^* = \left\{
    \begin{array}{ll}
        \max(0, \gamma(1 + d_i) - 1) \quad &\mbox{if} \quad i \leq q \\
        0 \quad &\mbox{otherwise} \:.
    \end{array}
\right. $$

Note that this solution is not unique if there are repeated eigenvalues. Notice also that one has the freedom to choose the Wishart prior parameters such that $\gamma=1$. Doing so, the solution satisfies $\Z^\star \Z^{\star \:T} = \bm{V}_{[:,q]} \bm{D}_{[q,q]} \bm{V}^\top_{[q,:]}$. Therefore there exists $\bm{R}$ an orthogonal matrix of size $q$ such that $\Z^{\star} = \bm{V}_{[:,q]}\bm{D}_{[q,q]}^{\frac{1}{2}}\bm{R}$. The latter is the output of a PCA model of $\X$ with $q$ components, which is defined up to a rotation.
\end{proof}

\subsection{Proof of \Cref{prop:integrability_pairwise_MRF}} \label{proof:lambda_perp_integrability}

\integrabilitypairwiseMRF*

\begin{proof}
$\W \in \mathcal{S}_W$ is the weight matrix of a graph with $R$ connected components $\{C_1, ..., C_R\}$ partitioning $\integ{N}$. Since $k$ is upper bounded by a constant, there exists $M_+ > 1$ that upper bounds $k$. Let $\bm{\mathcal{T}}$ be the adjacency matrix of a spanning forest of $\W$, since each edge of $\W$ is bounded by $N$, one has:
\begin{align}
    \int f_{k}(\X, \W) \lambda_{\mathcal{S}_{C}}(d\X) &= \int \prod_{(i,j) \in \integ{N}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{W_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) \nonumber \\
    &\leq M_+^{N^{3}} \int \prod_{(i,j) \in \integ{N}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{\mathcal{T}_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) \nonumber \\
    &\leq M_+^{N^{3}} \prod_{r \in \integ{R}} \int \prod_{(i,j) \in C_{r}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{\mathcal{T}_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) \:. \label{bound_MRF_product_CC}
\end{align}
Let $r \in \integ{R}$. The spanning tree corresponding to the $r^{th}$ connected component called $\bm{\mathcal{T}}^r$ has exactly $n_r-1$ edges, where $n_r$ is the number of nodes in this connected component. There exists a leaf node $\ell \in \integ{N}$ of $\bm{\mathcal{T}}^r$ and let $\tilde{\ell}$ be the node linked to it. Consider a bijective map $\sigma \colon C_r \backslash \{\ell\} \to \integ{n_r - 1}$ such that $\sigma(\tilde{\ell}) = 1$ and for $(i,j) \in (C_r \backslash \{\ell\})^2$, $\sigma(i) \leq \sigma(j)$ implies that node $i$ has a shorter path on $\overline{\bm{\mathcal{T}}^r}$\footnote{Symmetrized version \textit{i.e.}\ $\overline{\bm{\mathcal{T}}^r} = \bm{\mathcal{T}}^r + \bm{(\mathcal{T}}^r)^\top$.} to $\ell$ than node $j$. There exists a bijective map $e \colon \integ{2:n_r - 1} \to \integ{n_r - 2}$ such that for $i \in \integ{2:n_R-1}$, $\overline{\bm{\mathcal{T}}^r}_{\sigma^{-1}(i), \sigma^{-1}(e(i))} > 0$ and node $\sigma^{-1}(e(i))$ has a shorter path on $\overline{\bm{\mathcal{T}}^r}$ to node $\ell$ than node $\sigma^{-1}(i)$.

Recall that since $\X \in \mathcal{S}_{C}$ one has: $\sum_{i \in C_r} \mathbf{x}_i = 0$ hence $\mathbf{x}_{\ell} = - \sum_{i \neq \ell} \mathbf{x}_i$. Let us now consider the linear map $\phi^r$ such that:
\begin{align*}
\forall i \in [n_r - 1], \quad \phi^r(\mathbf{x}_i) = \left\{
    \begin{array}{ll}
        \mathbf{x}_{\sigma^{-1}(i)} + \sum_{j \in \integ{n_r - 1}} \mathbf{x}_{\sigma^{-1}(j)} & \mbox{if i = 1}\\
        \mathbf{x}_{\sigma^{-1}(i)} - \mathbf{x}_{\sigma^{-1}(e(i))} & \mbox{otherwise} \:.
    \end{array}
\right.
\end{align*}

We now show that the change of variable $\phi^r$ is a $\mathcal{C}^1$ diffeomorphism by proving that its Jacobian has full rank. Ordering the columns with the map $\sigma$, the latter takes the form:
\[
    \mathbf{J}_{\phi^r} = \left(
    \begin{array}{ccccc}
    2 & 1 & 1 & \dots & 1 \\
      & 1 & 0 & \dots & 0 \\
      &   & \ddots &  \ddots & \vdots \\
      & \Ab &   & \ddots & 0 \\
      &               &   &   & 1
    \end{array}
    \right)
\]
where $\Ab$ is a strictly lower triangular matrix such that for all $i \in \integ{2:n_r-1}$, $A_{ie(i)} = -1$ and for all $t \neq e(i)$, $A_{it}=0$. The above can be factorized as:
\[
\mathbf{J}_{\phi^r} = 
\left(
    \begin{array}{ccccc}
    \alpha_{n_r-1} & \alpha_{n_r-2} & \dots & \alpha_2 & \alpha_1 \\
    0  & 1 & 0 & \dots & 0 \\
    \vdots & \ddots & \ddots & \ddots & \vdots \\
    \vdots & & \ddots & \ddots & 0 \\
    0 & \dots & \dots & 0 & 1
    \end{array}
    \right)^{-1}
\left(
    \begin{array}{ccccc}
    1 & 0 & \dots & \dots & 0 \\
      & 1 & \ddots & & \vdots\\
      & & \ddots & \ddots & \vdots \\
      & \Ab & & \ddots & 0 \\
      & & & & 1
    \end{array}
\right)
\]
where $\alpha_{1}=-1$ and for $\ell > 1$, $\alpha_\ell = \sum_{j < l} \alpha_j\ind_{e(n_r - j)=n_r -\ell} - 1$. With this in place, for $i \in \integ{n_r -1}$, $\alpha_i \neq 0$ in particular $\alpha_{n_r-1} \neq 0$ therefore $|\mathbf{J}_{\phi^r}| \neq 0 $ and $\phi^r$ is a $\mathcal{C}^1$ diffeomorphism. This change of variable yields:
\begin{align*}
\int \prod_{(i,j) \in C_{r}^2} k(\mathbf{x}_{i} - \mathbf{x}_{j})^{\mathcal{T}_{ij}} \lambda_{\mathcal{S}_{C}}(d\X) 
&= \int \bigotimes_{i \in \integ{n_r - 1}} k(\mathbf{y}_i) |\mathbf{J}_{\phi^r}(\mathbf{Y})|^{-1} \lambda_{\mathbb{R}^p}(d\mathbf{Y}) \\
&= |\mathbf{J}_{\phi^r}|^{-1} \prod_{i \in \integ{n_r - 1}} \int k(\mathbf{y}_i) \lambda_{\mathbb{R}^p}(d\mathbf{y}_i)
\end{align*}
using the Fubini Tonelli theorem. The result follows from $\lambda_{\mathbb{R}^p}$-integrability of $k$ and upper bound \ref{bound_MRF_product_CC}.
\end{proof}


\subsubsection{Proof of \Cref{prop:posterior_W}}
\label{proof:posterior_limit}

\posteriorW

\begin{proof}
Let $\mathcal{P} \in \{B, D, E\}$, $k$ be a valid kernel (assumptions of \cref{prop:integrability_pairwise_MRF}) with $\K_{X} = (k(\mathbf{x}_{i} - \mathbf{x}_{j}))_{(i,j) \in \integ{N}^2}$ and $\bm{\pi} \in \mathbb{R}_+^{n \times n}$. Let $\W \sim \mathbb{P}_{\mathcal{P},k}^{\varepsilon}(\cdot \: ; \bm{\pi},1)$. Inversion of conditional with Bayes rule gives:
\begin{align}
    \forall \mW \in \mathcal{S}_{W}, \quad \mathbb{P}(\W | \bm{X}) \propto
    \mathcal{C}_{k}^{\varepsilon}(\W)^{-1} f^{\varepsilon}(\X, \W) f_{k}(\X, \W) \mathbb{P}^{\varepsilon}_{\mathcal{P},k}(\W; \bm{\pi}, 1) \label{inversion_Conditional}
\end{align}
where the prior reads:
\begin{align}
    \mathbb{P}_{\mathcal{P},k}^{\varepsilon}(\mW; \bm{\pi}, 1) \propto \mathcal{C}^{\varepsilon}_k(\W) \Omega_{\mathcal{P}}(\W) \prod_{(i,j) \in \integ{N}^2} \pi_{ij}^{W_{ij}} \:.
\end{align}
Hence the joint normalizing constant simplifies such that:
\begin{align}
    \forall \mW \in \mathcal{S}_{W}, \quad \mathbb{P}(\W | \bm{X}) &\propto
    f^{\varepsilon}(\X, \W) \Omega_{\mathcal{P}}(\W) \prod_{(i,j) \in \integ{N}^2} \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \\
    &\xrightarrow[\varepsilon \to 0]{} \Omega_{\mathcal{P}}(\W) \prod_{(i,j) \in \integ{N}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}}
\end{align}
which ends the proof. As a complement, we now explicit the simple forms taken by the posterior limit graph in each case.

\paragraph{$B$-Prior.}
Recall that in this case the prior reads:
\begin{align*}
    \mathbb{P}_{B}^{\varepsilon}(\mW; \bm{\pi},1) &\propto \mathcal{C}_{k}^{\varepsilon}(\mW) \prod_{(i,j) \in \integ{N}^2} \pi_{ij}^{W_{ij}} \ind_{W_{ij} \leq 1} \:.
\end{align*}
Therefore the posterior limit graph has the distribution:
\begin{align*}
    \mathbb{P}_{B}(\W ;\bm{\pi} \odot \mK_{X})
    &= \frac{\prod_{(i,j) \in \integ{N}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{ij} \leq 1}}{\sum_{\W \in \mathcal{S}_{W}} \prod_{(i,j) \in \integ{N}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{ij} \leq 1}} \\
    &= \prod_{(i,j) \in \integ{N}^2}  \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{1+\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}\right)^{W_{ij}} \left(\frac{1}{1+\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}\right)^{1-W_{ij}} \ind_{W_{ij} \leq 1} \:.
\end{align*}

This distribution amounts to: $\forall (i,j) \in \integ{N}^2, \quad \W_{ij} \stackrel{\perp\!\!\!\!\perp}{\sim} \mathcal{B}\left( \frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{1+\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)} \right)$.

\paragraph{$D$-Prior.} The prior writes:
\begin{align*}
    \mathbb{P}_{D}^{\varepsilon}(\mW; \bm{\pi}, 1) &\propto \mathcal{C}_{k}^{\varepsilon}(\mW) \prod_{(i,j) \in \integ{N}^2} \pi_{ij}^{W_{ij}} \ind_{W_{i+} = 1} \:.
\end{align*}
The distribution of the posterior limit then becomes:
\begin{align*}
    \mathbb{P}_{D}(\W ;\bm{\pi} \odot \mK_{X}) &= \frac{\prod_{(i,j) \in \integ{N}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{i+} = 1}}{\sum_{\W \in \mathcal{S}_{W}} \prod_{(i,j) \in \integ{N}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{i+} = 1}} \\
    &= \frac{\prod_{(i,j) \in \integ{N}^2}  \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{i+} = 1}}{\prod_{i \in \integ{N}} \sum_{\ell \in \integ{N}} \pi_{i\ell} k(\mathbf{x}_i - \mathbf{x}_{\ell})} \\
    &= \prod_{(i,j) \in \integ{N}^2} \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{\ell \in \integ{N}} \pi_{i\ell} k(\mathbf{x}_i - \mathbf{x}_{\ell})}\right)^{W_{ij}} \ind_{W_{i+} = 1} \:.
\end{align*}

This distribution amounts to: $\forall i \in \integ{N}, \quad \W_{i} \stackrel{\perp\!\!\!\!\perp}{\sim} \mathcal{M}\left(1, \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{\ell \in \integ{N}} \pi_{i\ell} k(\mathbf{x}_i - \mathbf{x}_{\ell})}\right)_{j \in \integ{N}}\right)$.

\paragraph{$E$-Prior.}
In this case the prior reads:
\begin{align*}
    \mathbb{P}_{E}^{\varepsilon}(\mW; \bm{\pi}, 1) &\propto \mathcal{C}_{k}^{\varepsilon}(\mW) \prod_{(i,j) \in \integ{N}^2} \frac{\pi_{ij}^{W_{ij}}}{W_{ij}!} \ind_{W_{++} = n} \:.
\end{align*}
Finally, deriving the distribution of the posterior graph limit:
\begin{align*}
    \mathbb{P}_{E}(\W ;\bm{\pi} \odot \mK_{X}) &= \frac{\prod_{(i,j) \in \integ{N}^2}  (W_{ij}!)^{-1}\left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{++} = n}}{\sum_{\W \in \mathcal{S}_{W}} \prod_{(i,j) \in \integ{N}^2} (W_{ij}!)^{-1} \left(\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)\right)^{W_{ij}} \ind_{W_{++} = n}} \\
    &= N! \prod_{(i,j) \in \integ{N}^2} (W_{ij})^{-1} \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{(\ell,t) \in \integ{N}^2} \pi_{\ell t} k(\mathbf{x}_{\ell} - \mathbf{x}_t)}\right)^{W_{ij}} \ind_{W_{++} = n} \:.
\end{align*}

This distribution amounts to: $\W \sim \mathcal{M}\left(N, \left(\frac{\pi_{ij} k(\mathbf{x}_i - \mathbf{x}_j)}{\sum_{(\ell,t) \in \integ{N}^2} \pi_{\ell t} k(\mathbf{x}_{\ell} - \mathbf{x}_t)}\right)_{(i,j) \in \integ{N}^2}\right)$.
\end{proof}

\subsubsection{Proof of \Cref{corollary_ccPCA}}

With the presented hierarchical model (\cref{fig:graphical_model_hierarchical}), the coupling problem is the following:
\begin{align}\label{eq:optim_corollary}
    \min_{\Z \in \mathcal{S}^q_{\scaleto{M}{3pt}}} \quad \operatorname{tr}\left(\bm{U}_{\scaleto{[:R]}{5pt}}\Z^\top(\bm{I}_{\scaleto{R}{4pt}} +  \varepsilon \bm{U}_{\scaleto{[R]}{5pt}}^\top\X\X^\top\bm{U}_{\scaleto{[:R]}{5pt}})^{-1}\bm{U}_{\scaleto{[R]}{5pt}}^\top\Z\right) - \log |\bm{I}_{\scaleto{R}{4pt}}  +  \varepsilon \bm{U}_{\scaleto{[R]}{5pt}}^\top\Z\Z^\top\bm{U}_{\scaleto{[:R]}{5pt}}| 
\end{align}
where $\bm{U}_{\scaleto{[:R]}{5pt}}$ are the eigenvectors associated to the Laplacian null-space of $\overline{\W}_{\scaleto{X}{4pt}}$.

Let us denote $\bar{\Z} = \bm{U}_{\scaleto{[R]}{5pt}}^\top\Z \in \mathbb{R}^{R \times q}$ and $\bar{\X} = \bm{U}_{\scaleto{[R]}{5pt}}^\top\X \in \mathbb{R}^{R \times p}$. Note that $\Z \to \bm{U}_{\scaleto{[R]}{5pt}}^\top\Z$ is a bijective linear map from $\mathcal{S}^q_{\scaleto{M}{4pt}}$ to $\mathbb{R}^{R \times q}$ with inverse $\bar{\Z} \to \bm{U}_{\scaleto{[R]}{5pt}}\bar{\Z}$ (and equivalently for $\mathbb{R}^{R \times p}$). Hence (\ref{eq:optim_corollary}) is equivalent to:
\begin{align}\label{eq:small_dim_optim_corollary}
    \min_{\bar{\Z} \in \mathbb{R}^{R \times q}} \quad \operatorname{tr}\left(\bar{\Z}^\top(\bm{I}_{\scaleto{R}{4pt}} +  \varepsilon \bar{\X}\bar{\X}^\top)^{-1}\bar{\Z}\right) - \log |\bm{I}_{\scaleto{R}{4pt}}  +  \varepsilon \bar{\Z}\bar{\Z}^\top| 
\end{align}

According to \cref{PCA_graph_coupling}, the solution of problem (\ref{eq:small_dim_optim_corollary}) is such that there exists $\bm{R}$ orthogonal, $\bar{\Z}^\star = \bm{V}_{[:,q]} \bm{S}_{[q,q]} \bm{R}$ where $\bar{\X}\bar{\X}^\top = \bm{V} \bm{S}^2 \bm{V}^\top$ is the eigendecomposition in an orthogonal basis of the among-row covariance matrix of $\bar{\X}$. Note that the solution does not depend on $\varepsilon$.

Therefore (\ref{eq:optim_corollary}) is solved for $\Z^\star = \bm{U}_{\scaleto{[:R]}{5pt}}\bm{V}_{[:,q]} \bm{S}_{[q,q]} \bm{R}$. One can notice that since the singular value decomposition (\textit{i.e.}\ SVD) of $\bm{U}_{\scaleto{[R]}{5pt}}^\top\X$ takes the form $\bm{V}\bm{S}\bm{B}$ where $\bm{B}$ is an semi-orthogonal matrix of size $p$, then $\bm{U}_{\scaleto{[:R]}{5pt}}\bm{U}_{\scaleto{[R]}{5pt}}^\top\X = \bm{U}_{\scaleto{[:R]}{5pt}}\bm{V}\bm{S}\bm{B}$. Noticing that $\bm{V}' = \bm{U}_{\scaleto{[:R]}{5pt}}\bm{V}$ is orthogonal, one has that $\bm{V}' \bm{S}\bm{B}$ is a compact SVD of $\bm{U}_{\scaleto{[:R]}{5pt}}\bm{U}_{\scaleto{[R]}{5pt}}^\top\X$. Therefore, since $\Z^\star = \bm{V}' \bm{S}$, $\Z^\star$ is a PCA embedding of $\bm{U}_{\scaleto{[:R]}{5pt}}\bm{U}_{\scaleto{[R]}{5pt}}^\top\X$.


\subsection{Experiments with \textit{ccPCA}}\label{sec:cc_pca_exps}

\subsubsection{Experimental setup}\label{sec:setup_ccpca}

\paragraph{Implementation of existing methods.} For t-SNE, we rely on the openTSNE implementation \citep{polivcar2019opentsne} for both computing the kernel $\mK_{\scaleto{X}{4pt}}$ with appropriate bandwidths and running the tSNE algorithm. We keep the training default parameters and $1000$ iterations of gradient descent. For all experiments, the default perplexity of $30$ was used to set the kernel bandwidths. For UMAP, we use the default Python implementation of \citep{mcinnes2018umap} with default parameters. For PCA and Laplacian eigenmaps, the scikit-learn implementation is used \citep{pedregosa2011scikit} with default parameters as well. 

\paragraph{\textit{ccPCA}.} The pseudo code of the algorithm is given in \cref{algo:ccPCA}. CCs' memberships (\textit{i.e.}\ eigenvectors $\bm{U}_{\scaleto{[R]}{5pt}}$) are computed using igraph \citep{csardi2006igraph}. Regarded the time complexity of ccPCA, one can sample the posterior graph with constant time if $\mathcal{P}_{\scaleto{X}{4pt}} = E$, linear time if $\mathcal{P}_{\scaleto{X}{4pt}} = D$ and quadratic time if $\mathcal{P}_{\scaleto{X}{4pt}} = B$. Moreover, computing $\bm{U}_{\scaleto{[R]}{5pt}}$ can be done with linear complexity \textit{w.r.t.}\ the number of nodes. Hence the time complexity is $O(N \times n)$ for $E$ and $D$ priors and $O(N \times n^2)$ for the $B$ prior, where $N$ is the number of Monte Carlo samples. In practice we found that $N\approx 100$ Monte Carlo samples produce a consistent \textit{ccPCA} embedding for $n \approx 10000$. Note that the time complexity of PCA is $O(\min(p^3,n^3))$ where $p$ is the dimensionality (\textit{i.e.}\ number of columns) of $\X$. Hence in most common applications involving images or biological sequencing data (where $p$ is very large), the additional time complexity brought by \textit{ccPCA} compared to PCA is negligible.

\vspace{0.5cm}

\begin{algorithm}[H]
  \caption{\textit{ccPCA}}
  \label{alg:3CPCA}
\begin{algorithmic}
  \STATE {\bfseries Input:} $\mK_{\scaleto{X}{4pt}}$, $\mathcal{P}_{\scaleto{X}{4pt}}$, N
  \FOR{$\ell=1$ {\bfseries to} $N$}
  \STATE Sample $\W^{\ell} \sim  \mathbb{P}_{\scaleto{\mathcal{P}_{\scaleto{X}{4pt}}}{5pt}}(\cdot ; \mK_{\scaleto{X}{4pt}})$
  \STATE Compute CCs' memberships $\bm{U}^{\ell}_{\scaleto{[R]}{5pt}}$ of $\W^{\ell}$
  \ENDFOR
  \STATE {\bfseries Output:} PCA of $\left(N^{-1}\sum_{\ell \in \integ{N}} \bm{U}^{\ell}_{\scaleto{[R]}{5pt}} \bm{U}_{\scaleto{[R]}{5pt}}^{\ell\:T} \right) \X$
\end{algorithmic}
\label{algo:ccPCA}
\end{algorithm}

All experiments are performed on a
machine with four Intel Core i5 processors and 16 GB memory.

\subsubsection{\textit{ccPCA} with Varying Perplexity Values}\label{sec:other_perp}

Recall that the \textit{ccPCA} algorithm retrieves the same latent graph as neighbor embedding methods. As shown in \cref{sec:retrieving_DR_methods}, these graphs' distributions depend on the type of prior considered, and take simple forms as follows, when $\bm{\pi}_{\scaleto{X}{4pt}} = \bm{1}$ :
\begin{itemize}
    \item if $\mathcal{P} = B$, $\forall (i,j) \in \integ{N}^2, \: W_{ij} \stackrel{\perp\!\!\!\!\perp}{\sim} \mathcal{B}\left(K_{\scaleto{X}{4pt},ij}/(1 + K_{\scaleto{X}{4pt},ij}) \right)$
    \item if $\mathcal{P} = D$, $\forall i \in \integ{N}, \: \W_{i} \stackrel{\perp\!\!\!\!\perp}{\sim} \mathcal{M}\left(1, \mK_{\scaleto{X}{4pt},i}/K_{\scaleto{X}{4pt},i+} \right)$
    \item if $\mathcal{P} = E$, $\W \sim \mathcal{M}\left(n, \mK_{\scaleto{X}{4pt}}/ K_{\scaleto{X}{4pt},++} \right)$
\end{itemize}
and $\mK_{\scaleto{X}{4pt}}$ is the kernel matrix evaluated on the data such that:
$$\forall (i,j) \in \integ{N}^2, \quad \mK_{\scaleto{X}{4pt},ij} = k((\X_{i} - \X_{j})/\tau_{i})$$
where $\bm{\tau} \in \mathbb{R}^n$ is set using an heuristic depending on the method considered \citep{maaten2008tSNE, mcinnes2018umap, tang2016visualizing}. In \cref{fig:ccPCA_perp}, we focus on the effect of the kernel bandwidths on \textit{ccPCA}, choosing the example of t-SNE.

\begin{figure*}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/GraphCoupling/multi_perp.png}}
\caption{\textit{ccPCA} launched for different values of the perplexity parameter. The latter determines the kernel bandwidths and can be interpreted as the number of effective neighbors of each point \citep{maaten2008tSNE}. As the perplexity grows, the probability of connecting different clusters of digit by sampling through the graph posterior $\mathbb{P}_{\scaleto{\mathcal{P}_X}{4pt}}(\cdot;\mK_{\scaleto{X}{4pt}})$ increases. Therefore clusters are less and less identifiable as the perplexity increases.}
\label{fig:ccPCA_perp}
\end{center}
\end{figure*}

From \cref{fig:ccPCA_perp}, one can notably notice that using a high perplexity leads to a more connected graph and therefore a PCA-like embedding with less degeneracy and no clustering effect. Recall that \textit{ccPCA} computes the same clusters as t-SNE through the CCs of the latent MRF and manage to position t-SNE clusters by focusing on their relative positions (that are filtered by t-SNE). In the case of a connected graph (high perplexity), \textit{ccPCA} will show little advantage over classical PCA since there will not be any cluster to position. Note that this discussion can be extended to other neighbor embedding methods equivalently. Therefore, our probabilistic framework allows us to indentify which part of information is filtered by the posterior graphs with given kernel bandwidth.

\subsubsection{Quantitative Evaluation of \textit{ccPCA}}\label{sec:quantitative_evaluation}

For quantitative assessment of \textit{ccPCA}, we focused on t-SNE \citep{maaten2008tSNE} and UMAP \citep{mcinnes2018umap} which are the most popular neighbor embedding methods. Note that for these algorithms the initialization is crucial for the global structure of the embeddings as shown in \citep{kobak2021initialization}. In addition to MNIST \citep{deng2012mnist}, we considered the datasets cifar-10, cifar-100 \citep{krizhevsky2009cifar}, fashion-MNIST \citep{xiao2017fashion} as well as the CD8+ T lymphocytes single cell RNA-seq dataset from \citep{kurd2020early}.

We used the quantitative criterion of \citep{lee2015multi} to assess the quality of the embeddings. As mentionned in this paper, the use of this criterion appears as the general consensus in dimension reduction, a field in which building meaningful criteria is tedious. The criterion measures the rescaled average agreement between the K-ary neighbourhoods in the input and output spaces. It is constructed as follows.

We first define the following quantities for $(i,j) \in \integ{N}^2$,
$\rho_{ij} = \mid \{ k: ||\X_{i} - \X_k ||^2_2 < ||\X_{i} - \X_j ||^2_2 \} \mid, r_{ij} = \mid \{ k: ||\Z_{i} - \Z_k ||^2_2 < ||\Z_{i} - \Z_j ||^2_2 \} \mid, \nu_i^K = \{ j : 1 \leq \rho_{ij} \leq K \} \text{ and } \gamma_i^K = \{ j : 1 \leq r_{ij} \leq K \}$. The average K-ary neighbourhood preservation is rescaled to indicate the improvement over a random embedding such that:
\begin{align}\label{def_R}
R_{N}(K) = \frac{(N-1) Q_{N}(K) - K }{N-1-K}
\end{align}
where $Q_{N}(K) = \frac{1}{KN} \sum_{i=1}^N \mid \nu_i^K \cap \gamma_i^K \mid$, $N$ is the number of data points and $K$ is the hyperparameter that adjusts the scale at which we are looking.

To focus on large-scale structure, K was chosen as either N/4 or N/2. As summarized by \citep{kobak2021initialization}, current practice consists in using PCA or Laplacian eigenmaps as initialization for these algorithms, thus we compare to these strategies. Results are displayed in \cref{tSNE_quantitative_results} and \cref{UMAP_quantitative_results}, each entry being an average over 5 random seeds, with standard deviation displayed below each entry. Note that when not specified, tSNE and UMAP are initialized with an isotropic Gaussian variable. 

These results show that using \textit{ccPCA} is a reliable alternative to PCA and Laplacian eigenmaps for reproducing large-scale neighborhoods.

\vspace{2cm}

\begin{table}[h]
\caption{$100 \times R_{N}(K)$ (\ref{def_R}) for embeddings produced using t-SNE with various initializations.}
\begin{footnotesize}
\begin{center}
\begin{tabular}{lcccccccc}
\toprule
& \multicolumn{2}{c}{tSNE} & \multicolumn{2}{c}{PCA + tSNE} & \multicolumn{2}{c}{LE + tSNE} & \multicolumn{2}{c}{ccPCA + tSNE} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
K of K-ary & {N/4} & {N/2}  & {N/4} & {N/2} & {N/4} & {N/2} & {N/4} & {N/2} \\
\midrule
MNIST & 18.7 & 7.4 & 28.4 & 21.9 & 26.7 & 18.5 & $\bm{31.3}$ & $\bm{28.5}$ \\
& $\pm 2.2$ & $\pm 5.1$ & $\pm 0.3$ & $\pm 0.2$ & $\pm 0.7$ & $\pm 0.4$ & $\pm 0.4$ & $\pm 1.2$ 
\vspace{0.1cm}
\\
cifar-10 & 20.3 & 16.4 & $\bm{36.9}$ & 41.9 & 25.8 & 24.1 & 36.4 & $\bm{43.4}$ \\
& $\pm 3.2$ & $\pm 4.8$ & $\pm 0.6$ & $\pm 1.1$ & $\pm 0.6$ & $\pm 1.5$ & $\pm 0.4$ & $\pm 1.6$
\vspace{0.1cm}
\\
cifar-100 & 21.6 & 18.2 & 38.1 & $\bm{47.5}$ & 23.3 & 26.5 & $\bm{39.6}$ & 43.6 \\
& $\pm 3.6$ & $\pm 5.5$ & $\pm 0.4$ & $\pm 0.4$ & $\pm 1.5$ & $\pm 1.8$ & $\pm 0.7$ & $\pm 1.1$
\vspace{0.1cm}
\\
fashion-MNIST & 27.2 & 12.3 & 36.9 & 28.5 & 32.0 & 25.1 & $\bm{41.6}$ & $\bm{35.7}$ \\
& $\pm 4.3$ & $\pm 7.8$ & $\pm 0.1$ & $\pm 0.2$ & $\pm 0.8$ & $\pm 2.2$ & $\pm 0.9$ & $\pm 1.5$
\vspace{0.1cm}
\\
Single Cell data & 25.7 & 22.4 & 37.7 & 29.0 & 28.1 & 31.5 & $\bm{40.1}$ & $\bm{34.6}$ \\
& $\pm 4.8$ & $\pm 10.6$ & $\pm 2.7$ & $\pm 4.7$ & $\pm 1.5$ & $\pm 1.4$ & $\pm 1.7$ & $\pm 2.6$
\\
\bottomrule
\end{tabular}
\end{center}
\end{footnotesize}
\label{tSNE_quantitative_results}
\end{table}

\begin{table}[h]
\caption{$100 \times R_{N}(K)$ (\ref{def_R}) for embeddings produced using UMAP with various initializations.}
\begin{footnotesize}
\begin{center}
\begin{tabular}{lcccccccc}
\toprule
& \multicolumn{2}{c}{UMAP} & \multicolumn{2}{c}{PCA + UMAP} & \multicolumn{2}{c}{LE + UMAP} & \multicolumn{2}{c}{ccPCA + UMAP} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
K of K-ary & {N/4} & {N/2}  & {N/4} & {N/2} & {N/4} & {N/2} & {N/4} & {N/2} \\
\midrule
MNIST & 29.5 & 22.7 & $\bm{36.6}$ & 31.1 & 34.6 & 24.9 & 33.4 & $\bm{32.3}$ \\
& $\pm 1.4$ & $\pm 2.2$ & $\pm 0.2$ & $\pm 0.5$ & $\pm 0.2$ & $\pm 0.7$ & $\pm 0.3$ & $\pm 0.5$ 
\vspace{0.1cm}
\\
cifar-10 & 39.2 & 47.6 & 44.3 & $\bm{53.4}$ & 44.2 & 52.6 & $\bm{44.6}$ & 53.2\\
& $\pm 2.6$ & $\pm 1.1$ & $\pm 0.2$ & $\pm 0.1$ & $\pm 0.1$ & $\pm 0.2$ & $\pm 0.2$ & $\pm 0.2$ 
\vspace{0.1cm}
\\
cifar-100 & 41.6 & 42.2 & 45.4 & 45.2 & 44.2 & 43.4 & $\bm{49.9}$ & $\bm{52.9}$\\
& $\pm 1.8$ & $\pm 0.9$ & $\pm 0.2$ & $\pm 0.1$ & $\pm 0.3$ & $\pm 0.1$ & $\pm 0.4$ & $\pm 0.6$ 
\vspace{0.1cm}
\\
fashion-MNIST & 48.7 & 33.6 & 56.2 & 54.3 & 58.1 & 53.4 & $\bm{58.9}$ & $\bm{55.8}$ \\
& $\pm 2.6$ & $\pm 9.5$ & $\pm 0.5$ & $\pm 0.6$ & $\pm 0.5$ & $\pm 0.6$ & $\pm 0.5$ & $\pm 0.3$ 
\vspace{0.1cm}
\\
Single Cell data & 39.5 & 34.3 & 52.3 & 47.2 & $\bm{55.9}$ & 45.7 & 53.6 & $\bm{53.9}$\\
& $\pm 1.4$ & $\pm 6.1$ & $\pm 0.8$ & $\pm 6.9$ & $\pm 0.3$ & $\pm 0.9$ & $\pm 0.3$ & $\pm 1.3$ 
\\
\bottomrule
\end{tabular}
\end{center}
\end{footnotesize}
\label{UMAP_quantitative_results}
\end{table}

